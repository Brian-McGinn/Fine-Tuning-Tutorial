{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d24652-75aa-40b2-bbcb-cbb2e8f41c10",
   "metadata": {},
   "source": [
    "Togeher AI setup for a low cost open source way to validate GenAI prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d701c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Collecting together\n",
      "  Downloading together-1.5.21-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in ./venv/lib/python3.12/site-packages (from together) (3.12.13)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in ./venv/lib/python3.12/site-packages (from together) (8.2.1)\n",
      "Collecting eval-type-backport<0.3.0,>=0.1.3 (from together)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in ./venv/lib/python3.12/site-packages (from together) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from together) (2.2.6)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in ./venv/lib/python3.12/site-packages (from together) (11.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in ./venv/lib/python3.12/site-packages (from together) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in ./venv/lib/python3.12/site-packages (from together) (2.32.4)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.8.1 in ./venv/lib/python3.12/site-packages (from together) (14.0.0)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from together)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in ./venv/lib/python3.12/site-packages (from together) (4.67.1)\n",
      "Collecting typer<0.16,>=0.9 (from together)\n",
      "  Using cached typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->together) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich<15.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich<15.0.0,>=13.8.1->together) (2.19.2)\n",
      "Collecting click<9.0.0,>=8.1.7 (from together)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.12/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.8.1->together) (0.1.2)\n",
      "Downloading together-1.5.21-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: tabulate, eval-type-backport, click, typer, together\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.2.1\n",
      "    Uninstalling click-8.2.1:\n",
      "      Successfully uninstalled click-8.2.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.16.0\n",
      "    Uninstalling typer-0.16.0:\n",
      "      Successfully uninstalled typer-0.16.0\n",
      "Successfully installed click-8.1.8 eval-type-backport-0.2.2 tabulate-0.9.0 together-1.5.21 typer-0.15.4\n"
     ]
    }
   ],
   "source": [
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50225cc3-83d3-4259-8eb3-f2c88d16f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "def send_prompt(messages: []):\n",
    "    client = Together(api_key=\"<API_KEY>\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def create_prompt(prompt: str):\n",
    "    return [{\"role\": \"user\", \"content\": prompt}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f7c07-d923-41e6-84e3-805829638fbf",
   "metadata": {},
   "source": [
    "Zero-Shot Learning: This involves giving the AI a task without any prior examples. You describe what you want in detail, assuming the AI has no prior knowledge of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4e5c0e-f18c-412e-9905-0229c962bee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language at a large scale. It's a computer program that uses complex algorithms and massive amounts of data to learn patterns, relationships, and structures within language.\n",
      "\n",
      "LLMs are typically trained on vast amounts of text data, which can include books, articles, research papers, websites, and even social media posts. This training data allows the model to learn about language syntax, semantics, and pragmatics, enabling it to generate human-like text, answer questions, and even engage in conversation.\n",
      "\n",
      "Some key characteristics of large language models include:\n",
      "\n",
      "1. **Scalability**: LLMs are designed to handle massive amounts of data and can be trained on billions of parameters, making them highly scalable.\n",
      "2. **Deep learning**: LLMs use deep learning techniques, such as neural networks, to learn complex patterns in language.\n",
      "3. **Self-supervised learning**: LLMs can learn from raw text data without explicit human supervision, allowing them to discover patterns and relationships on their own.\n",
      "4. **Generative capabilities**: LLMs can generate text, including articles, stories, and even entire books, based on a given prompt or topic.\n",
      "5. **Conversational abilities**: LLMs can engage in conversation, answering questions, and responding to user input in a way that simulates human-like dialogue.\n",
      "\n",
      "Large language models have many potential applications, including:\n",
      "\n",
      "1. **Language translation**: LLMs can be used to translate text from one language to another.\n",
      "2. **Text summarization**: LLMs can summarize long documents or articles into concise, readable versions.\n",
      "3. **Chatbots and virtual assistants**: LLMs can power chatbots and virtual assistants, enabling them to understand and respond to user queries.\n",
      "4. **Content generation**: LLMs can generate high-quality content, such as articles, blog posts, and social media posts.\n",
      "5. **Research and analysis**: LLMs can be used to analyze large datasets, identify patterns, and provide insights in fields like social science, humanities, and natural language processing.\n",
      "\n",
      "Examples of large language models include transformer-based models like BERT, RoBERTa, and XLNet, as well as other models like language generators and conversational AI systems.\n",
      "\n",
      "I hope this helps! Do you have any specific questions about large language models or their applications?\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot\n",
    "zero_shot_prompt = \"\"\"\n",
    "    Explain what a large language model is.\"\"\"\n",
    "\n",
    "prompt = create_prompt(zero_shot_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fea36-7c6b-4158-932a-75297aa72d70",
   "metadata": {},
   "source": [
    "One-Shot Learning: You provide one example along with your prompt. This helps the AI understand the context or format you’re expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfc8ccf-cc18-4d97-af11-d266fc893457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of Foundation Models, BERT (Bidirectional Encoder Representations from Transformers) is a type of large language model that is similar to GPT-3. While GPT-3 is a generative model that can produce human-like text, BERT is a pre-trained language model that is primarily designed for natural language understanding (NLU) tasks.\n",
      "\n",
      "BERT is a transformer-based model that was developed by Google in 2018. It is trained on a massive dataset of text, such as the entire Wikipedia corpus, and is designed to learn the patterns and relationships of language. The key innovation of BERT is its ability to capture contextual relationships between words in a sentence, allowing it to better understand the nuances of language.\n",
      "\n",
      "Like GPT-3, BERT is a foundation model that can be fine-tuned for a wide range of NLU tasks, such as:\n",
      "\n",
      "1. Question answering: BERT can be used to answer questions based on a given text.\n",
      "2. Sentiment analysis: BERT can be used to determine the sentiment or emotional tone of a piece of text.\n",
      "3. Text classification: BERT can be used to classify text into categories, such as spam vs. non-spam emails.\n",
      "4. Named entity recognition: BERT can be used to identify and extract specific entities, such as names, locations, and organizations, from text.\n",
      "\n",
      "The main difference between BERT and GPT-3 is their primary function. While GPT-3 is designed to generate text, BERT is designed to understand and analyze text. However, both models are foundation models that can be adapted to various tasks, and they have both achieved state-of-the-art results in their respective areas.\n",
      "\n",
      "Some key features of BERT include:\n",
      "\n",
      "1. **Pre-training**: BERT is pre-trained on a large dataset of text, which allows it to learn the patterns and relationships of language.\n",
      "2. **Transformer architecture**: BERT uses a transformer-based architecture, which allows it to capture contextual relationships between words in a sentence.\n",
      "3. **Fine-tuning**: BERT can be fine-tuned for specific tasks, which allows it to adapt to new tasks and datasets.\n",
      "4. **Contextualized embeddings**: BERT produces contextualized embeddings, which capture the nuances of language and allow it to better understand the relationships between words.\n",
      "\n",
      "Overall, BERT is a powerful foundation model that has achieved state-of-the-art results in a wide range of NLU tasks, and it continues to be a widely used and influential model in the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = \"\"\"\n",
    "    A Foundation Model in AI refers to a model like GPT-3,\n",
    "    which is trained on a large dataset and can be adapted to various tasks.\n",
    "    Explain what BERT is in this context.\"\"\"\n",
    "\n",
    "prompt = create_prompt(one_shot_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e371dde-0e1d-4b97-85ba-b39a6d3486da",
   "metadata": {},
   "source": [
    "Few-Shot Learning: This involves providing a few examples (usually 2–5) to help the AI understand the pattern or style of the response you’re looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b26ce1-5e8f-4cc6-b460-573414b35227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Models, such as those used in natural language processing (NLP) and computer vision, have been increasingly applied to the field of robotics to improve its various aspects. While models like GPT-3 and DALL-E are specifically designed for NLP and image generation, respectively, their underlying technologies and principles can be adapted or integrated into robotics applications. Here are some ways Foundation Models are used in robotics:\n",
      "\n",
      "1. **Robot Learning and Control**: Foundation Models can be fine-tuned for robot learning and control tasks, such as learning from demonstrations, imitation learning, or reinforcement learning. For example, a model like GPT-3 can be used to generate text-based instructions for a robot to follow, which can then be translated into actions.\n",
      "2. **Scene Understanding and Perception**: Computer vision models, similar to DALL-E, can be used in robotics for scene understanding, object recognition, and perception. These models can help robots to better understand their environment, detect objects, and navigate through spaces.\n",
      "3. **Human-Robot Interaction (HRI)**: Foundation Models can be used to improve HRI by enabling robots to understand and respond to human language, gestures, and emotions. For instance, a robot can use a NLP model to comprehend voice commands or generate human-like responses to user queries.\n",
      "4. **Robot Vision and Manipulation**: Models like DALL-E can be used to generate images of objects or scenes, which can help robots to learn about object manipulation, grasping, and placement. This can be particularly useful in tasks like robotic assembly, packaging, or pick-and-place operations.\n",
      "5. **Transfer Learning and Adaptation**: Foundation Models can be used as a starting point for transfer learning in robotics, allowing robots to adapt to new tasks, environments, or objects with minimal additional training data. This can significantly reduce the time and effort required to train robots for new tasks.\n",
      "6. **Robotics Simulation and Planning**: Foundation Models can be used to generate simulated environments, objects, or scenarios, which can help robots to plan and practice tasks in a virtual setting before executing them in the real world.\n",
      "7. **Explainability and Transparency**: Foundation Models can be used to provide insights into robot decision-making processes, making them more explainable and transparent. This can be particularly important in applications where robots are used in critical tasks, such as healthcare or transportation.\n",
      "\n",
      "To apply Foundation Models in robotics, researchers and developers often use techniques like:\n",
      "\n",
      "* **Multimodal learning**: combining different modalities, such as vision, language, and sensor data, to enable robots to learn from multiple sources.\n",
      "* **Domain adaptation**: adapting Foundation Models to specific robotics domains, such as robotics manipulation or autonomous driving.\n",
      "* **Few-shot learning**: using Foundation Models to learn from limited data, which is often the case in robotics applications.\n",
      "\n",
      "While the application of Foundation Models in robotics is still an emerging area, it has the potential to significantly improve the capabilities and performance of robots in various tasks and domains.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "    Foundation Models such as GPT-3 are used for natural language\n",
    "    processing, while models like DALL-E are used for image generation.\n",
    "    How are Foundation Models used in the field of robotics?\"\"\"\n",
    "\n",
    "prompt = create_prompt(few_shot_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8119b36",
   "metadata": {},
   "source": [
    "Instruction Prompting: Embed explicit task steps within the prompt for the AI to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c5dc59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. \n",
      "LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n"
     ]
    }
   ],
   "source": [
    "Instruction_prompt = \"\"\"\n",
    "    Read the following wikipedia article. remove the [1], [2], [3] from the article.\n",
    "\n",
    "    The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. \n",
    "    LLMs can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] \n",
    "    inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]?\"\"\"\n",
    "\n",
    "prompt = create_prompt(Instruction_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca98db-a30a-420c-bc6a-f98debc8f132",
   "metadata": {},
   "source": [
    "Chain-of-Thought Prompting: Here, you ask the AI to detail its thought process step-by-step. This is particularly useful for complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8512f005-eea3-4cae-8603-92b17d0c9bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing a Foundation Model in AI involves a series of steps that transform raw data into a powerful, general-purpose model capable of performing a wide range of tasks. Here's an overview of the process, from data collection to model training:\n",
      "\n",
      "**Data Collection (Data Sourcing and Preparation)**\n",
      "\n",
      "1. **Data Sourcing**: Identify and collect large amounts of diverse, high-quality data from various sources, such as:\n",
      "\t* Web pages\n",
      "\t* Books\n",
      "\t* Articles\n",
      "\t* User-generated content\n",
      "\t* Databases\n",
      "2. **Data Cleaning**: Preprocess the collected data by:\n",
      "\t* Removing duplicates and irrelevant information\n",
      "\t* Handling missing values\n",
      "\t* Normalizing data formats\n",
      "\t* Tokenizing text data (e.g., splitting into individual words or subwords)\n",
      "3. **Data Augmentation**: Optionally, apply techniques to increase the size and diversity of the dataset, such as:\n",
      "\t* Text augmentation (e.g., paraphrasing, text noising)\n",
      "\t* Data synthesis (e.g., generating new data samples using existing ones)\n",
      "\n",
      "**Data Preprocessing and Tokenization**\n",
      "\n",
      "1. **Tokenization**: Split the preprocessed data into individual tokens, such as:\n",
      "\t* Words\n",
      "\t* Subwords (e.g., WordPiece, BPE)\n",
      "\t* Characters\n",
      "2. **Vocabulary Creation**: Create a vocabulary of unique tokens, which will be used to represent the input data.\n",
      "3. **Data Encoding**: Convert the tokenized data into numerical representations, such as:\n",
      "\t* One-hot encoding\n",
      "\t* Embeddings (e.g., Word2Vec, GloVe)\n",
      "\n",
      "**Model Architecture and Configuration**\n",
      "\n",
      "1. **Model Selection**: Choose a suitable foundation model architecture, such as:\n",
      "\t* Transformer-based models (e.g., BERT, RoBERTa)\n",
      "\t* Recurrent neural networks (RNNs)\n",
      "\t* Convolutional neural networks (CNNs)\n",
      "2. **Model Configuration**: Define the model's hyperparameters, including:\n",
      "\t* Number of layers\n",
      "\t* Hidden size\n",
      "\t* Number of attention heads\n",
      "\t* Activation functions\n",
      "\t* Optimizer and learning rate schedule\n",
      "\n",
      "**Model Training**\n",
      "\n",
      "1. **Masked Language Modeling**: Train the model using a masked language modeling objective, where:\n",
      "\t* A portion of the input tokens are randomly replaced with a [MASK] token\n",
      "\t* The model predicts the original token\n",
      "2. **Next Sentence Prediction**: Optionally, train the model using a next sentence prediction objective, where:\n",
      "\t* The model predicts whether two adjacent sentences are semantically similar\n",
      "3. **Optimization**: Use an optimizer (e.g., Adam, SGD) to update the model's parameters, minimizing the loss function (e.g., cross-entropy, mean squared error)\n",
      "4. **Regularization Techniques**: Apply regularization techniques to prevent overfitting, such as:\n",
      "\t* Dropout\n",
      "\t* Weight decay\n",
      "\t* Gradient clipping\n",
      "\n",
      "**Model Evaluation and Fine-Tuning**\n",
      "\n",
      "1. **Evaluation Metrics**: Monitor the model's performance on a validation set using metrics such as:\n",
      "\t* Perplexity\n",
      "\t* Accuracy\n",
      "\t* F1-score\n",
      "2. **Fine-Tuning**: Optionally, fine-tune the pre-trained model on a specific downstream task, such as:\n",
      "\t* Sentiment analysis\n",
      "\t* Question answering\n",
      "\t* Text classification\n",
      "3. **Hyperparameter Tuning**: Perform hyperparameter tuning to optimize the model's performance on the downstream task.\n",
      "\n",
      "By following these steps, you can develop a powerful Foundation Model in AI, capable of performing a wide range of natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought_prompt = \"\"\"\n",
    "    Describe the process of developing a Foundation Model in AI,\n",
    "    from data collection to model training.\"\"\"\n",
    "\n",
    "prompt = create_prompt(chain_of_thought_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192869f3-a0ee-4d12-ad4a-a5ccb66f32b2",
   "metadata": {},
   "source": [
    "Iterative Prompting: This is a process where you refine your prompt based on the outputs you get, slowly guiding the AI to the desired answer or style of answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31966f18-4146-4343-ad0c-f2597b727e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Models, also known as Large Language Models (LLMs) or Transformer Models, have been rapidly advancing in the field of Artificial Intelligence (AI). Here are some of the latest developments:\n",
      "\n",
      "1. **Scaling Up**: The trend of scaling up foundation models continues, with models like Google's PaLM (540 billion parameters), Meta's LLaMA (65 billion parameters), and Microsoft's Turing-NLG (17 billion parameters) pushing the boundaries of model size and complexity.\n",
      "2. **Improved Performance**: Larger models have led to significant improvements in performance on various natural language processing (NLP) tasks, such as language translation, question-answering, and text generation. For example, the recent model, Chinchilla, achieved state-of-the-art results on several benchmarks.\n",
      "3. **Multimodal Capabilities**: Foundation models are being extended to handle multiple modalities, including vision, speech, and text. This enables applications like visual question-answering, image-text retrieval, and speech-to-text translation.\n",
      "4. **Efficient Training Methods**: Researchers are exploring more efficient training methods, such as:\n",
      "\t* **Sparse training**: reducing the number of parameters and computations required during training.\n",
      "\t* **Knowledge distillation**: transferring knowledge from a large model to a smaller one, reducing the need for extensive training.\n",
      "\t* **Pruning**: removing redundant or unnecessary parameters to reduce model size and computational requirements.\n",
      "5. **Explainability and Interpretability**: As foundation models become more complex, there is a growing need to understand how they make decisions. Researchers are developing techniques to provide insights into model behavior, such as:\n",
      "\t* **Attention visualization**: visualizing the attention mechanisms used by the model to focus on specific input elements.\n",
      "\t* **Feature importance**: identifying the most important input features contributing to the model's predictions.\n",
      "6. **Specialized Models**: Foundation models are being adapted for specific domains, such as:\n",
      "\t* **Medical language models**: designed for medical text analysis and clinical decision support.\n",
      "\t* **Financial language models**: focused on financial text analysis and risk assessment.\n",
      "7. **Ethics and Fairness**: As foundation models are increasingly used in real-world applications, there is a growing concern about their potential biases and ethical implications. Researchers are working on:\n",
      "\t* **Bias detection and mitigation**: identifying and addressing biases in model training data and predictions.\n",
      "\t* **Fairness and transparency**: ensuring that models are fair, transparent, and accountable in their decision-making processes.\n",
      "8. **Open-Source Models**: The development of open-source foundation models, such as Hugging Face's Transformers and the Stanford Natural Language Processing Group's models, has democratized access to these powerful AI technologies, enabling a broader range of researchers and developers to contribute to and build upon these models.\n",
      "9. **Applications and Deployments**: Foundation models are being deployed in various applications, including:\n",
      "\t* **Virtual assistants**: powering conversational interfaces and chatbots.\n",
      "\t* **Content generation**: generating text, images, and videos for various industries, such as marketing, entertainment, and education.\n",
      "\t* **Language translation**: enabling more accurate and efficient language translation systems.\n",
      "\n",
      "These developments demonstrate the rapid progress being made in the field of foundation models, with significant advancements in model size, performance, and applications. However, they also highlight the need for ongoing research into explainability, ethics, and fairness to ensure that these powerful AI technologies are developed and deployed responsibly.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m first_response = send_prompt(messages)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(first_response)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m:\u001b[43mfirst_response\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     13\u001b[39m refined_prompt = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m    Can you provide more details about these improvements in multi-modal learning within Foundation Models?\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     16\u001b[39m messages += create_prompt(refined_prompt)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "# Store previous messages for better context\n",
    "messages = []\n",
    "iterative_prompt = \"\"\"\n",
    "    Tell me about the latest developments in Foundation Models in AI.\"\"\"\n",
    "\n",
    "messages += create_prompt(iterative_prompt)\n",
    "\n",
    "first_response = send_prompt(messages)\n",
    "print(first_response)\n",
    "\n",
    "messages.append({\"role\": \"assistant\",\"content\":first_response[\"message\"][\"content\"]})\n",
    "\n",
    "refined_prompt = \"\"\"\n",
    "    Can you provide more details about these improvements in multi-modal learning within Foundation Models?\"\"\"\n",
    "\n",
    "messages += create_prompt(refined_prompt)\n",
    "print(\"\\n\\n----------------------------------------------------------\\n\\n\")\n",
    "print(send_prompt(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977162e-2acc-467b-9046-210e765cd1cf",
   "metadata": {},
   "source": [
    "Negative Prompting: In this method, you tell the AI what not to do. For instance, you might specify that you don’t want a certain type of content in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f3c3dff-ec87-43e7-8d9b-3865fadfea03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Models refer to a class of artificial intelligence (AI) models that are trained on vast amounts of diverse data, allowing them to develop a broad range of skills and knowledge. These models are designed to be highly versatile and can be fine-tuned for a wide variety of tasks, making them a fundamental component of many AI systems.\n",
      "\n",
      "The key characteristics of Foundation Models include:\n",
      "\n",
      "1. **Large-scale training data**: Foundation Models are trained on enormous datasets that cover a wide range of topics, domains, and tasks. This enables them to learn general patterns, relationships, and representations that can be applied to various problems.\n",
      "2. **Generalizability**: Foundation Models are designed to be generalizable, meaning they can be adapted to new tasks, domains, or datasets with minimal additional training. This is achieved through their ability to learn abstract representations and features that are transferable across different contexts.\n",
      "3. **Transfer learning**: Foundation Models can leverage their pre-trained knowledge and skills to learn new tasks more efficiently. By fine-tuning a pre-trained Foundation Model on a specific task, developers can adapt the model to perform well on that task with less training data and computational resources.\n",
      "4. **Modularity**: Foundation Models can be used as a starting point for building more specialized models. By adding task-specific layers or modules on top of the Foundation Model, developers can create customized models that excel in specific areas.\n",
      "\n",
      "The benefits of Foundation Models include:\n",
      "\n",
      "1. **Improved performance**: Foundation Models can achieve state-of-the-art performance on a wide range of tasks, thanks to their broad knowledge and skills.\n",
      "2. **Increased efficiency**: By leveraging pre-trained Foundation Models, developers can reduce the amount of training data and computational resources required to build specialized models.\n",
      "3. **Faster development**: Foundation Models can accelerate the development of AI systems by providing a pre-trained foundation that can be adapted to various tasks and domains.\n",
      "\n",
      "Overall, Foundation Models have the potential to revolutionize the field of AI by providing a flexible and efficient way to build intelligent systems that can learn, adapt, and perform a wide range of tasks.\n"
     ]
    }
   ],
   "source": [
    "negative_prompt = \"\"\"\n",
    "    Explain the concept of Foundation Models in AI without mentioning natural language processing or NLP.\"\"\"\n",
    "\n",
    "prompt = create_prompt(negative_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db43ba-8586-4862-9d3a-38167f0c53d0",
   "metadata": {},
   "source": [
    "Hybrid Prompting: Combining different methods, like few-shot with chain-of-thought, to get more precise or creative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3195b917-48e2-414d-beba-cb6adf29ac2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Models, like GPT-3, have revolutionized the field of natural language processing (NLP). Similarly, foundation models are being applied in other domains of AI, including computer vision, to achieve state-of-the-art results. Here's how foundation models are being used in computer vision and other domains:\n",
      "\n",
      "**Computer Vision:**\n",
      "\n",
      "1. **Image Classification:** Foundation models like Vision Transformers (ViT) and ConvNeXt are being used for image classification tasks, achieving high accuracy on benchmark datasets like ImageNet.\n",
      "2. **Object Detection:** Models like DETR (DEtection TRansformer) and YOLO (You Only Look Once) are using foundation models to detect objects in images and videos, with improved accuracy and efficiency.\n",
      "3. **Image Generation:** Foundation models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are being used for image generation tasks, such as generating realistic images, videos, and 3D models.\n",
      "4. **Image Segmentation:** Models like U-Net and SegNet are using foundation models to segment images, identifying specific regions of interest, such as objects, scenes, or medical images.\n",
      "\n",
      "**Other Domains:**\n",
      "\n",
      "1. **Speech Recognition:** Foundation models like Wav2Vec and HuBERT are being used for speech recognition tasks, achieving high accuracy on benchmark datasets like LibriSpeech.\n",
      "2. **Reinforcement Learning:** Foundation models like MuZero and DreamerV2 are being used for reinforcement learning tasks, such as playing games, controlling robots, and optimizing complex systems.\n",
      "3. **Time Series Forecasting:** Models like Prophet and DeepAR are using foundation models to forecast time series data, such as stock prices, weather, and traffic patterns.\n",
      "4. **Graph Neural Networks:** Foundation models like GraphSAGE and Graph Attention Networks (GATs) are being used for graph-structured data, such as social networks, molecular structures, and traffic networks.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "Foundation models in these domains share similar characteristics, including:\n",
      "\n",
      "1. **Large-scale pre-training:** Foundation models are pre-trained on large datasets, often using self-supervised or unsupervised learning methods.\n",
      "2. **Transfer learning:** Foundation models are fine-tuned for specific downstream tasks, leveraging the knowledge and features learned during pre-training.\n",
      "3. **Modular architecture:** Foundation models often have modular architectures, allowing for easy adaptation to different tasks and domains.\n",
      "4. **Scalability:** Foundation models are designed to scale to large datasets and complex tasks, making them suitable for real-world applications.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "The application of foundation models in various AI domains offers several benefits, including:\n",
      "\n",
      "1. **Improved accuracy:** Foundation models often achieve state-of-the-art results on benchmark datasets.\n",
      "2. **Increased efficiency:** Foundation models can reduce the need for labeled data and improve training efficiency.\n",
      "3. **Flexibility:** Foundation models can be adapted to various tasks and domains, making them a versatile tool for AI research and applications.\n",
      "4. **Reducing the need for domain expertise:** Foundation models can be used by researchers and practitioners without extensive domain expertise, making AI more accessible and widely applicable.\n",
      "\n",
      "In summary, foundation models are being applied in various AI domains, including computer vision, speech recognition, reinforcement learning, time series forecasting, and graph neural networks. These models offer improved accuracy, efficiency, flexibility, and reduced need for domain expertise, making them a powerful tool for AI research and applications.\n"
     ]
    }
   ],
   "source": [
    "hybrid_prompt = \"\"\"\n",
    "    Like GPT-3, which is a versatile model used in various language tasks, \n",
    "    explain how Foundation Models are applied in other domains of AI, such as computer vision.\"\"\"\n",
    "\n",
    "prompt = create_prompt(hybrid_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de2f0d-5112-4fc8-9124-7915820a58e9",
   "metadata": {},
   "source": [
    " Prompt Chaining: Breaking down a complex task into smaller prompts and then chaining the outputs together to form a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9c1d162-5b7a-475e-939d-9fb04217e5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation models are a class of artificial intelligence (AI) models that are trained on large amounts of data and can be fine-tuned for a wide range of tasks. Here are some examples of foundation models in AI:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a pre-trained language model that can be fine-tuned for tasks such as question answering, sentiment analysis, and text classification.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook AI, RoBERTa is a variant of BERT that has been trained on a larger dataset and has achieved state-of-the-art results on several natural language processing (NLP) tasks.\n",
      "3. **Transformers**: Developed by researchers at Google, Transformers are a type of neural network architecture that can be used for a wide range of NLP tasks, including language translation, text summarization, and text generation.\n",
      "4. **DALL-E (Differentiable Augmentation of Latent Language and Images)**: Developed by OpenAI, DALL-E is a foundation model that can generate images from text prompts, and can be fine-tuned for tasks such as image generation, image-to-image translation, and image editing.\n",
      "5. **CLIP (Contrastive Language-Image Pre-training)**: Developed by OpenAI, CLIP is a foundation model that can be used for tasks such as image classification, object detection, and image generation, and has achieved state-of-the-art results on several computer vision benchmarks.\n",
      "6. **Vision Transformers (ViT)**: Developed by researchers at Google, ViT is a foundation model that can be used for tasks such as image classification, object detection, and image segmentation, and has achieved state-of-the-art results on several computer vision benchmarks.\n",
      "7. **Longformer**: Developed by researchers at Google, Longformer is a foundation model that can be used for tasks such as text classification, sentiment analysis, and question answering, and is designed to handle long-range dependencies in text data.\n",
      "8. **XLNet**: Developed by researchers at Google and Carnegie Mellon University, XLNet is a foundation model that can be used for tasks such as language modeling, text classification, and question answering, and has achieved state-of-the-art results on several NLP benchmarks.\n",
      "\n",
      "These are just a few examples of foundation models in AI, and there are many other models being developed and researched in this area. Foundation models have the potential to revolutionize the field of AI by enabling the development of more accurate, efficient, and generalizable models that can be applied to a wide range of tasks and domains.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m messages += create_prompt(chaining_prompt)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(send_prompt(messages))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m:\u001b[43mfirst_response\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     11\u001b[39m chaining_content_prompt = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33m    Choose one of these models and explain its foundational role in AI development.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     14\u001b[39m messages += create_prompt(chaining_content_prompt)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "chaining_prompt  = \"\"\"\n",
    "    List some examples of Foundation Models in AI.\"\"\"\n",
    "\n",
    "messages += create_prompt(chaining_prompt)\n",
    "\n",
    "print(send_prompt(messages))\n",
    "\n",
    "messages.append({\"role\": \"assistant\",\"content\":first_response[\"message\"][\"content\"]})\n",
    "\n",
    "chaining_content_prompt = \"\"\"\n",
    "    Choose one of these models and explain its foundational role in AI development.\"\"\"\n",
    "\n",
    "messages += create_prompt(chaining_content_prompt)\n",
    "print(\"\\n\\n----------------------------------------------------------\\n\\n\")\n",
    "print(send_prompt(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19db2e",
   "metadata": {},
   "source": [
    "Tree of thought / Self-Consistency- breakdown the task into steps and ask multiple sources. choose the best source and then continue the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f20b851-7a95-49d5-b70a-6099e65960af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's introduce our three experts: Mathematician Max, Logical Laura, and Analyst Alex. They will each write down one step of their thinking and share it with the group.\n",
      "\n",
      "**Step 1:**\n",
      "- Mathematician Max: First, calculate the number of passengers who get off and on at the first stop. 8 passengers get off.\n",
      "- Logical Laura: Determine the initial number of passengers and the changes at the first stop. Initially, there are 48 people on the bus, and 8 get off.\n",
      "- Analyst Alex: Identify the key events at the first stop: 8 passengers exit, and an unknown number enter based on the number who exited.\n",
      "\n",
      "All experts share their initial thoughts and proceed to the next step.\n",
      "\n",
      "**Step 2:**\n",
      "- Mathematician Max: Calculate the number of passengers who get on at the first stop. Since 5 times as many people as got off get on, it's 5 * 8 = 40 passengers.\n",
      "- Logical Laura: After 8 passengers get off, there are 48 - 8 = 40 passengers left. Then, 5 times the number who got off (5*8 = 40) get on.\n",
      "- Analyst Alex: The number of passengers who get on is 5 times the number who got off, which means 5 * 8 = 40 new passengers.\n",
      "\n",
      "The experts share their calculations and move on to the next step.\n",
      "\n",
      "**Step 3:**\n",
      "- Mathematician Max: Calculate the total number of passengers after the first stop. Initially, there were 48, then 8 got off, leaving 48 - 8 = 40. After that, 40 more got on, making it 40 + 40 = 80 passengers.\n",
      "- Logical Laura: Confirm the total after the first stop. After 8 got off, there were 40 passengers, and then 40 more got on, resulting in 40 + 40 = 80 passengers.\n",
      "- Analyst Alex: Summarize the situation after the first stop: 48 (initial) - 8 (got off) + 40 (got on) = 80 passengers.\n",
      "\n",
      "All experts agree on the total after the first stop and proceed.\n",
      "\n",
      "**Step 4:**\n",
      "- Mathematician Max: Calculate the number of passengers who get off at the second stop. 21 passengers get off.\n",
      "- Logical Laura: Note the number of passengers getting off at the second stop. 21 passengers exit.\n",
      "- Analyst Alex: Identify the number of passengers leaving at the second stop: 21 passengers.\n",
      "\n",
      "The experts share their thoughts and continue.\n",
      "\n",
      "**Step 5:**\n",
      "- Mathematician Max: Calculate the number of passengers who get on at the second stop. It's 3 times fewer than the number who got off at the second stop, which means (1/3) * 21 = 7 passengers get on.\n",
      "- Logical Laura: Determine the number of passengers boarding at the second stop. Since it's 3 times fewer than those who got off, we calculate it as 21 / 3 = 7 passengers.\n",
      "- Analyst Alex: Calculate the number of passengers getting on at the second stop. 3 times fewer than 21 means 21 / 3 = 7 passengers.\n",
      "\n",
      "The experts share their calculations for the second stop.\n",
      "\n",
      "**Step 6:**\n",
      "- Mathematician Max: Calculate the final number of passengers after the second stop. After the first stop, there were 80 passengers. At the second stop, 21 got off, leaving 80 - 21 = 59. Then, 7 got on, making it 59 + 7 = 66 passengers.\n",
      "- Logical Laura: Calculate the total after the second stop. After the first stop, there were 80 passengers. 21 got off, leaving 80 - 21 = 59. Then, 7 more got on, resulting in 59 + 7 = 66 passengers.\n",
      "- Analyst Alex: Summarize the final count after the second stop: 80 (after the first stop) - 21 (got off at the second stop) + 7 (got on at the second stop) = 66 passengers.\n",
      "\n",
      "All experts agree on the final number of passengers after the second stop. None of the experts realized they were wrong at any point, so none leave. The consensus is that there are 66 passengers riding the bus after the second stop.\n"
     ]
    }
   ],
   "source": [
    "tree_of_thought_prompt = \"\"\"\n",
    "    Imagine three different experts are answering this question. \n",
    "    All experts will write down 1 step of their thinking, then share it with the group. \n",
    "    Then all experts will go on to the next step, etc. \n",
    "    If any expert realises they're wrong at any point then they leave. \n",
    "    The question is: \n",
    "    48 people are riding a bus. On the first stop, 8 passengers get off, and 5 times as many people as the number who got off from the bus get into the bus. On the second stop 21, passengers get off and 3 times fewer passengers get on. How many passengers are riding the bus after the second stop?\n",
    "    \"\"\"\n",
    "\n",
    "prompt = create_prompt(tree_of_thought_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a88d62b",
   "metadata": {},
   "source": [
    "Directional stimulus prompt - give the AI a hint about what you want to help guide the AI to the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7972487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given equations, 5+5=10, 8+2=10, and 9+1=10, all equal 10, suggesting a pattern where different combinations of numbers can result in the same answer. Following this pattern, it can be inferred that 7+3 will also equal 10. Therefore, the answer to 7+3 is 10, consistent with the pattern established by the previous equations.\n"
     ]
    }
   ],
   "source": [
    "directional_stimulus_prompt = \"\"\"\n",
    "    if 5+5=10, 8+2=10, and 9+1=10 what does 7+3=?\n",
    "    Summarize the above in 2-3 sentences based on the hint. \n",
    "    Hint: The answer is 10.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = create_prompt(directional_stimulus_prompt)\n",
    "\n",
    "print(send_prompt(prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358bf0b",
   "metadata": {},
   "source": [
    "Chain of density - take all the text and create a summary. use the prompt to check the summary and make sure everything was covered. continue to do this until we are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0890f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of dictionaries with the missing entities and denser summaries:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"Missing_Entities\": \"LLM. GPTs\",\n",
      "    \"Denser_Summary\": \"This article discusses language models, specifically large language models, and their applications, including generative chatbots, with LLM and GPTs being notable examples, utilizing self-supervised machine learning.\"\n",
      "  },\n",
      "  {\n",
      "    \"Missing_Entities\": \"ChatGPT. Gemini\",\n",
      "    \"Denser_Summary\": \"Large language models, such as LLM and GPTs, are used in generative chatbots like ChatGPT and Gemini, applying self-supervised machine learning for natural language processing tasks.\"\n",
      "  },\n",
      "  {\n",
      "    \"Missing_Entities\": \"Claude. ontologies\",\n",
      "    \"Denser_Summary\": \"LLMs, including GPTs, power chatbots like ChatGPT, Gemini, and Claude, acquiring syntax, semantics, and ontologies knowledge through self-supervised machine learning for natural language processing.\"\n",
      "  },\n",
      "  {\n",
      "    \"Missing_Entities\": \"transformers. corpora\",\n",
      "    \"Denser_Summary\": \"GPTs, a type of LLM, and transformers, drive chatbots like ChatGPT, Gemini, and Claude, learning from human language corpora to understand syntax, semantics, and ontologies.\"\n",
      "  },\n",
      "  {\n",
      "    \"Missing_Entities\": \"biases. tasks\",\n",
      "    \"Denser_Summary\": \"GPTs, LLMs, and transformers, used in ChatGPT, Gemini, and Claude, learn from corpora, acquiring syntax, semantics, and ontologies knowledge, but also inherit biases, and can be fine-tuned for specific tasks.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "ARTICLE = \"\"\"\n",
    "A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. \n",
    "    The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or \n",
    "    guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present \n",
    "    in the data they are trained in.[3]\n",
    "    \"\"\"\n",
    "chain_of_density_prompt = f\"\"\"\n",
    "    Article: {ARTICLE}\n",
    "    You will generate increasingly concise, entity-dense summaries of the above Article.\n",
    "    Repeat the following 2 steps 5 times.\n",
    "    Step 1. Identify 1-3 informative Entities (\". \" delimited) from the Article which are missing from the previously generated summary.\n",
    "    Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n",
    "    A Missing Entity is:\n",
    "    - Relevant: to the main story.\n",
    "    - Specific: descriptive yet concise (5 words or fewer).\n",
    "    - Novel: not in the previous summary.\n",
    "    - Faithful: present in the Article.\n",
    "    - Anywhere: located anywhere in the Article.\n",
    "    Guidelines:\n",
    "    - The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n",
    "    - Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "    - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n",
    "    - Missing entities can appear anywhere in the new summary.\n",
    "    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "\n",
    "    Remember, use the exact same number of words for each summary.\n",
    "\n",
    "    Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "    \"\"\"\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": chain_of_density_prompt}]\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
