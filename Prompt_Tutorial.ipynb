{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d24652-75aa-40b2-bbcb-cbb2e8f41c10",
   "metadata": {},
   "source": [
    "Togeher AI setup for a low cost open source way to validate GenAI prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50225cc3-83d3-4259-8eb3-f2c88d16f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "def send_prompt(messages: []):\n",
    "    client = Together(api_key=\"<API_KEY>\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def create_prompt(prompt: str):\n",
    "    return [{\"role\": \"user\", \"content\": prompt}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f7c07-d923-41e6-84e3-805829638fbf",
   "metadata": {},
   "source": [
    "Zero-Shot Learning: This involves giving the AI a task without any prior examples. You describe what you want in detail, assuming the AI has no prior knowledge of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e5c0e-f18c-412e-9905-0229c962bee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Zero-Shot\n",
    "zero_shot_prompt = \"\"\"\n",
    "    Explain what a large language model is.\"\"\"\n",
    "\n",
    "prompt = create_prompt(zero_shot_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fea36-7c6b-4158-932a-75297aa72d70",
   "metadata": {},
   "source": [
    "One-Shot Learning: You provide one example along with your prompt. This helps the AI understand the context or format you’re expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc8ccf-cc18-4d97-af11-d266fc893457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_shot_prompt = \"\"\"\n",
    "    A Foundation Model in AI refers to a model like GPT-3,\n",
    "    which is trained on a large dataset and can be adapted to various tasks.\n",
    "    Explain what BERT is in this context.\"\"\"\n",
    "\n",
    "prompt = create_prompt(one_shot_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e371dde-0e1d-4b97-85ba-b39a6d3486da",
   "metadata": {},
   "source": [
    "Few-Shot Learning: This involves providing a few examples (usually 2–5) to help the AI understand the pattern or style of the response you’re looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b26ce1-5e8f-4cc6-b460-573414b35227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "    Foundation Models such as GPT-3 are used for natural language\n",
    "    processing, while models like DALL-E are used for image generation.\n",
    "    How are Foundation Models used in the field of robotics?\"\"\"\n",
    "\n",
    "prompt = create_prompt(few_shot_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8119b36",
   "metadata": {},
   "source": [
    "Instruction Prompting: Embed explicit task steps within the prompt for the AI to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5dc59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Instruction_prompt = \"\"\"\n",
    "    Read the following wikipedia article. remove the [1], [2], [3] from the article.\n",
    "\n",
    "    The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. \n",
    "    LLMs can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] \n",
    "    inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]?\"\"\"\n",
    "\n",
    "prompt = create_prompt(Instruction_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca98db-a30a-420c-bc6a-f98debc8f132",
   "metadata": {},
   "source": [
    "Chain-of-Thought Prompting: Here, you ask the AI to detail its thought process step-by-step. This is particularly useful for complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512f005-eea3-4cae-8603-92b17d0c9bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain_of_thought_prompt = \"\"\"\n",
    "    Describe the process of developing a Foundation Model in AI,\n",
    "    from data collection to model training.\"\"\"\n",
    "\n",
    "prompt = create_prompt(chain_of_thought_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192869f3-a0ee-4d12-ad4a-a5ccb66f32b2",
   "metadata": {},
   "source": [
    "Iterative Prompting: This is a process where you refine your prompt based on the outputs you get, slowly guiding the AI to the desired answer or style of answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31966f18-4146-4343-ad0c-f2597b727e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store previous messages for better context\n",
    "messages = []\n",
    "iterative_prompt = \"\"\"\n",
    "    Tell me about the latest developments in Foundation Models in AI.\"\"\"\n",
    "\n",
    "messages += create_prompt(iterative_prompt)\n",
    "\n",
    "first_response = send_prompt(messages)\n",
    "print(first_response)\n",
    "\n",
    "messages.append({\"role\": \"assistant\",\"content\":first_response[\"message\"][\"content\"]})\n",
    "\n",
    "refined_prompt = \"\"\"\n",
    "    Can you provide more details about these improvements in multi-modal learning within Foundation Models?\"\"\"\n",
    "\n",
    "messages += create_prompt(refined_prompt)\n",
    "print(\"\\n\\n----------------------------------------------------------\\n\\n\")\n",
    "print(send_prompt(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977162e-2acc-467b-9046-210e765cd1cf",
   "metadata": {},
   "source": [
    "Negative Prompting: In this method, you tell the AI what not to do. For instance, you might specify that you don’t want a certain type of content in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c3dff-ec87-43e7-8d9b-3865fadfea03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\"\n",
    "    Explain the concept of Foundation Models in AI without mentioning natural language processing or NLP.\"\"\"\n",
    "\n",
    "prompt = create_prompt(negative_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db43ba-8586-4862-9d3a-38167f0c53d0",
   "metadata": {},
   "source": [
    "Hybrid Prompting: Combining different methods, like few-shot with chain-of-thought, to get more precise or creative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195b917-48e2-414d-beba-cb6adf29ac2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hybrid_prompt = \"\"\"\n",
    "    Like GPT-3, which is a versatile model used in various language tasks, \n",
    "    explain how Foundation Models are applied in other domains of AI, such as computer vision.\"\"\"\n",
    "\n",
    "prompt = create_prompt(hybrid_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de2f0d-5112-4fc8-9124-7915820a58e9",
   "metadata": {},
   "source": [
    " Prompt Chaining: Breaking down a complex task into smaller prompts and then chaining the outputs together to form a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1d162-5b7a-475e-939d-9fb04217e5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = []\n",
    "chaining_prompt  = \"\"\"\n",
    "    List some examples of Foundation Models in AI.\"\"\"\n",
    "\n",
    "messages += create_prompt(chaining_prompt)\n",
    "\n",
    "print(send_prompt(messages))\n",
    "\n",
    "messages.append({\"role\": \"assistant\",\"content\":first_response[\"message\"][\"content\"]})\n",
    "\n",
    "chaining_content_prompt = \"\"\"\n",
    "    Choose one of these models and explain its foundational role in AI development.\"\"\"\n",
    "\n",
    "messages += create_prompt(chaining_content_prompt)\n",
    "print(\"\\n\\n----------------------------------------------------------\\n\\n\")\n",
    "print(send_prompt(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19db2e",
   "metadata": {},
   "source": [
    "Tree of thought / Self-Consistency- breakdown the task into steps and ask multiple sources. choose the best source and then continue the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20b851-7a95-49d5-b70a-6099e65960af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_of_thought_prompt = \"\"\"\n",
    "    Imagine three different experts are answering this question. \n",
    "    All experts will write down 1 step of their thinking, then share it with the group. \n",
    "    Then all experts will go on to the next step, etc. \n",
    "    If any expert realises they're wrong at any point then they leave. \n",
    "    The question is: \n",
    "    48 people are riding a bus. On the first stop, 8 passengers get off, and 5 times as many people as the number who got off from the bus get into the bus. On the second stop 21, passengers get off and 3 times fewer passengers get on. How many passengers are riding the bus after the second stop?\n",
    "    \"\"\"\n",
    "\n",
    "prompt = create_prompt(tree_of_thought_prompt)\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a88d62b",
   "metadata": {},
   "source": [
    "Directional stimulus prompt - give the AI a hint about what you want to help guide the AI to the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_stimulus_prompt = \"\"\"\n",
    "    if 5+5=10, 8+2=10, and 9+1=10 what does 7+3=?\n",
    "    Summarize the above in 2-3 sentences based on the hint. \n",
    "    Hint: The answer is 10.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = create_prompt(directional_stimulus_prompt)\n",
    "\n",
    "print(send_prompt(prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358bf0b",
   "metadata": {},
   "source": [
    "Chain of density - take all the text and create a summary. use the prompt to check the summary and make sure everything was covered. continue to do this until we are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0890f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\"\n",
    "A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. \n",
    "    The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or \n",
    "    guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present \n",
    "    in the data they are trained in.[3]\n",
    "    \"\"\"\n",
    "chain_of_density_prompt = f\"\"\"\n",
    "    Article: {ARTICLE}\n",
    "    You will generate increasingly concise, entity-dense summaries of the above Article.\n",
    "    Repeat the following 2 steps 5 times.\n",
    "    Step 1. Identify 1-3 informative Entities (\". \" delimited) from the Article which are missing from the previously generated summary.\n",
    "    Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n",
    "    A Missing Entity is:\n",
    "    - Relevant: to the main story.\n",
    "    - Specific: descriptive yet concise (5 words or fewer).\n",
    "    - Novel: not in the previous summary.\n",
    "    - Faithful: present in the Article.\n",
    "    - Anywhere: located anywhere in the Article.\n",
    "    Guidelines:\n",
    "    - The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n",
    "    - Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "    - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n",
    "    - Missing entities can appear anywhere in the new summary.\n",
    "    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "\n",
    "    Remember, use the exact same number of words for each summary.\n",
    "\n",
    "    Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "    \"\"\"\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": chain_of_density_prompt}]\n",
    "\n",
    "print(send_prompt(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
