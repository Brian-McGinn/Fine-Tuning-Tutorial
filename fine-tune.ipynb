{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36078ad3-348c-4164-8483-955966a4f020",
   "metadata": {},
   "source": [
    "Initial query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8361ecb9-8a5c-446b-990d-9f70ebdc04ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1', 'created_at': '2025-07-05T18:12:06.121178506Z', 'message': {'role': 'assistant', 'content': 'I couldn\\'t find any information on \"unsloth\". It\\'s possible that it\\'s a made-up word or not a widely recognized term.\\n\\nHowever, I\\'m assuming you may be thinking of sloth, which is a slow-moving mammal that lives in the tropical rainforests of Central and South America. If that\\'s the case, here\\'s 100 words about sloths: Sloths are arboreal mammals that spend most of their time hanging upside down from trees. They have a low metabolism, which means they don\\'t need to eat much or move around much, allowing them to conserve energy in their dense rainforest habitats.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 1458366621, 'load_duration': 19558549, 'prompt_eval_count': 22, 'prompt_eval_duration': 116783676, 'eval_count': 128, 'eval_duration': 1321612281}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"},}],\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, data=json.dumps(payload))\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600a5a9-705c-42a0-9141-40c1e7039346",
   "metadata": {},
   "source": [
    "Now let's do it in python. \n",
    "\n",
    "First we need to install ollama with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3c5ce-6258-4930-9b3a-0956b39620f1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbce5ce-0222-4cd8-a28a-9e63ac989e98",
   "metadata": {},
   "source": [
    "Next let's create a simple python query using the ollama library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7550214-582d-47bf-8f6d-5b6ab26cbb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a large language model, I don't have any information about \"Unsloth.ai\" as it doesn't seem to be a real or well-known entity. It's possible that you may be thinking of Sloth AI, which is a hypothetical concept rather than an actual AI system.\n",
      "\n",
      "However, I can provide some information on sloths and their biology if you'd like!\n",
      "\n",
      "If you meant something else entirely, please let me know what Unsloth.ai is supposed to be, or give me any additional context about it.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48630fa6-a7ed-4137-8e42-442237ab81e2",
   "metadata": {},
   "source": [
    "As you can see unsloth is an unown term to the default deepseek model and the results can be quite humorous. We will want to fix this by fine tuning the model and give it some information about the unsloth library. \n",
    "The first step in doing this is to convert the provided unsloth_documentation.pdf into a dataset for training.\n",
    "For this task we can use Docling and LiteLLM.\n",
    "To help visualize different outputs we'll also install colorama to color code out terminal outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362af26",
   "metadata": {},
   "source": [
    "Most Likely you will want to run this on GPU so you'll need to install pytorch as well. I used the latest version with CUDA 12.8 since I have a 5000 series GPU which is not supported by earlier CUDA versions. Please check version [compatability](https://developer.nvidia.com/cuda-gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1207d",
   "metadata": {},
   "source": [
    "You may also might need to update huggingface cache persmissions and pre-download the docling-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo chown -R $(whoami) ~/.cache/huggingface\n",
    "huggingface-cli download ds4sd/docling-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc837f",
   "metadata": {},
   "source": [
    "Now We can being getting out data chunks from the pdf and save them to a list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b69cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (991 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "converter = DocumentConverter()\n",
    "doc = converter.convert(\"unsloth_documentation.pdf\").document\n",
    "chunker = HybridChunker()\n",
    "chunks = chunker.chunk(dl_doc=doc)\n",
    "\n",
    "contextualized_chunks = []\n",
    "for i, chunk in enumerate(chunks): \n",
    "    # print( f\"Raw Text:\\n{chunk.text[:300]}…\" )\n",
    "    contextualized_chunks.append(chunker.contextualize(chunk=chunk))\n",
    "    # print(f\"Contextualized Text:\\n{contextualized_chunks[i][:300]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32283d35",
   "metadata": {},
   "source": [
    "Now that we have contextualize chunks we can use ollama to generate data for our fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4b259d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What are the models that can be fine-tuned with the provided data?', 'answer': 'The data supports fine-tuning of Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral'}\n",
      "{'question': 'How will these models benefit from using this dataset?', 'answer': 'These models can be finetuned with the provided data 2x faster and require 80% less VRAM.'}\n",
      "{'question': 'What are the key differences between Gemma 3n and Qwen3 in terms of performance?', 'answer': 'Gemma 3n (4B) has a 1.5x faster performance compared to Qwen3, while Qwen3 (14B) has a 2x faster performance.'}\n",
      "{'question': 'Which model uses the least amount of memory among all the options provided?', 'answer': 'Qwen3 (4B): GRPO and Gemma 3 (4B) use 50% and 60% less memory, respectively, but Qwen3 (14B): GRPO uses 80% less memory.'}\n",
      "{'question': 'Are there any free notebooks available for the models mentioned in this data chunk?', 'answer': 'Yes, most of the models mentioned offer free notebooks, including Gemma 3n, Qwen3, and Llama 3.2 (3B).'}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'Can you describe the notebooks featured in this dataset, including those for Kaggle and GRPO?', 'answer': 'The notebooks provided cover various topics, such as Kaggle and GRPO projects.'}\n",
      "{'question': 'Where can one find detailed documentation for Unsloth?', 'answer': 'Detailed documentation for Unsloth is available here.'}\n",
      "{'question': 'What types of models and notebooks are available in this dataset?', 'answer': 'This dataset features various models and notebooks, including those related to TTS (Text-to-Speech) and Vision tasks.'}\n",
      "{'question': 'Can you provide more information about the different projects supported by Finetune for Free?', 'answer': 'Finetune for Free supports a range of projects, including Kaggle, GRPO, TTS & Vision.'}\n",
      "{'question': 'What is the latest update regarding model support in Unsloth.ai News?', 'answer': 'Everything is now supported, including models like BERT, diffusion, Cohere, and Mamba.'}\n",
      "{'question': 'How can I enable FFT in my model fine-tuning process using Unsloth.ai News?', 'answer': 'You can enable FFT by setting full_finetuning = True.'}\n",
      "{'question': 'What is the significance of MultiGPU support in Unsloth.ai News, and when can we expect it?', 'answer': 'MultiGPU support is coming soon to Unsloth.ai News.'}\n",
      "{'question': 'Can you provide a brief overview of DeepSeek-R1 and how to run or fine-tune models with its guide?', 'answer': 'You can find the guide for running or fine-tuning DeepSeek-R1 on our website, along with other model uploads.'}\n",
      "{'question': 'What is Long-context Reasoning (GRPO) in Unsloth.ai News, and what are the system requirements to train a reasoning model?', 'answer': 'Long-context Reasoning (GRPO) is now available in Unsloth, requiring only 5GB VRAM to train your own reasoning model.'}\n",
      "{'question': 'What are the newly supported vision models in Unsloth.ai News?', 'answer': 'Unsloth.ai News now supports Llama 3.2 Vision (11B), Qwen 2.5 VL (7B) and Pixtral (12B) vision models.'}\n",
      "{'question': \"How does the addition of Cut Cross Entropy with Apple improve Unsloth's capabilities?\", 'answer': \"The inclusion of Cut Cross Entropy enables Unsloth to support 89K context for Meta's Llama 3.3 (70B) on an 80GB GPU, providing a significant improvement over previous models.\"}\n",
      "{'question': 'What is the key benefit of using Unsloth for fine-tuning language models?', 'answer': 'Unsloth enables longer context windows and improved memory efficiency, making it an attractive option for developers working with large-scale language models like Llama 3.1 (8B).'}\n",
      "{'question': 'What issue was identified and resolved by the Unsloth team in collaboration with users?', 'answer': 'A gradient accumulation bug was found and fixed by the Unsloth team, which was addressed through a software update for both Unsloth and transformers.'}\n",
      "{'question': 'How has memory usage been optimized in the latest version of Unsloth.ai News?', 'answer': 'The recent optimization reduced memory usage by an additional 30% and allowed for even longer context windows compared to previous versions.'}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'What are some key resources mentioned in this text that can be helpful to users?', 'answer': 'The provided links include documentation, Reddit pages, and Twitter handles that offer additional information and community engagement opportunities.'}\n",
      "{'question': 'How can one install the required model or package from these resources?', 'answer': \"Users can find installation instructions through 'Pip install' or by visiting the website 'Unsloth Releases'.\"}\n",
      "{'question': 'What kind of content is typically found on the blog and wiki sections linked here?', 'answer': \"The blog contains articles about the project, while the wiki page likely hosts detailed documentation related to the project's development.\"}\n",
      "{'question': 'Where can one join a community discussion or ask questions related to this project?', 'answer': 'Users can participate in discussions by joining the provided Reddit page and engaging with other users through comments.'}\n",
      "{'question': 'What is the command to install Unsloth?', 'answer': \"The command to install Unsloth is 'Install Unsloth'.\"}\n",
      "{'question': 'Where can I find detailed installation and updating instructions for Unsloth?', 'answer': 'You can find detailed installation and updating instructions for Unsloth in our documentation.'}\n",
      "{'question': \"What is the purpose of using 'unsloth' package in Python?\", 'answer': \"The 'unsloth' package is installed to facilitate data transformation and create diverse Q&A pairs for fine-tuning a language model.\"}\n",
      "{'question': \"How can you install the 'unsloth' library on Linux-based systems?\", 'answer': \"You can install 'unsloth' using pip, which involves running the command 'pip install unsloth' in your terminal.\"}\n",
      "{'question': \"What is the recommended method for installing packages like 'unsloth' on Linux devices?\", 'answer': 'The recommended method for installing such packages is through pip, ensuring compatibility and ease of use.'}\n",
      "{'question': \"Can you describe a scenario where one might need to install the 'unsloth' library?\", 'answer': \"One would typically need to install 'unsloth' when working on projects requiring data transformation and preparation for fine-tuning language models.\"}\n",
      "{'question': \"What are the potential benefits of using 'unsloth' in a machine learning workflow?\", 'answer': \"Using 'unsloth' can streamline data curation, enhance model performance by providing high-quality training data, and support efficient project development.\"}\n",
      "{'question': 'What are the necessary software installations for using Unsloth on Windows?', 'answer': 'You need to install NVIDIA Video Driver, Visual Studio C++, and CUDA Toolkit, along with PyTorch.'}\n",
      "{'question': 'Which version of Python is compatible with Unsloth?', 'answer': 'Unsloth supports Python 3.10, 3.11, and 3.12, but not Python 3.13.'}\n",
      "{'question': 'How do you install the necessary dependencies for using Unsloth on Windows?', 'answer': 'You can follow the instructions provided in this document to download and install NVIDIA Video Driver, Visual Studio C++, CUDA Toolkit, and PyTorch accordingly.'}\n",
      "{'question': 'What is the purpose of installing Visual Studio C++ when setting up Unsloth?', 'answer': 'Visual Studio C++ is required for compiling code, which is essential for using Unsloth on Windows.'}\n",
      "{'question': 'Which SDK should you select while installing Visual Studio to ensure compatibility with Unsloth?', 'answer': 'You need to choose the options for Windows 10/11 SDK during installation.'}\n",
      "{'question': 'How do I install the Unsloth library?', 'answer': 'You can install Unsloth using pip: `pip install unsloth`.'}\n",
      "{'question': 'What additional steps are required for installing Unsloth on Windows, considering any potential issues with dependencies or compatibility?', 'answer': 'For running Unsloth directly on Windows, you need to install Triton from a specific fork and follow the provided instructions. Note that this requires PyTorch >= 2.4 and CUDA 12. Additionally, set dataset_num_proc=1 in the SFTTrainer to avoid any crashing issues.'}\n",
      "{'question': 'What are some advanced installation instructions or troubleshooting steps for Unsloth?', 'answer': 'For advanced installation instructions or resolving weird errors during installations, refer to the provided notes and links. This includes specific guidance on handling potential issues related to dependencies or compatibility.'}\n",
      "{'question': 'How do I run Unsloth with a specified number of processing units to prevent crashing?', 'answer': 'You can set dataset_num_proc=1 in the SFTTrainer to run Unsloth safely and avoid any crashing issues during installation.'}\n",
      "{'question': 'Can you provide an overview of the necessary steps for running Unsloth on Windows, considering its specific requirements and potential pitfalls?', 'answer': 'To run Unsloth directly on Windows, install Triton from a specific fork, follow the provided instructions, ensure PyTorch >= 2.4 and CUDA 12 are installed, and set dataset_num_proc=1 in the SFTTrainer to prevent any crashes.'}\n",
      "{'question': 'What is the first step in installing Unsloth?', 'answer': 'The first step in installing Unsloth is to install torch and triton by going to https://pytorch.org.'}\n",
      "{'question': 'How do I confirm if CUDA is installed correctly for Unsloth installation?', 'answer': 'You can try running nvcc to confirm if CUDA is installed correctly. If it fails, you need to install cudatoolkit or CUDA drivers.'}\n",
      "{'question': 'What are the necessary versions of Python and other libraries required for Unsloth installation?', 'answer': 'To ensure compatibility, check the PyTorch Compatibility Matrix for the required versions of Python, CUDA, CUDNN, torch , triton , and xformers.'}\n",
      "{'question': 'How can I troubleshoot issues during the installation process of xformers in Unsloth?', 'answer': 'If vllm fails to install, try installing flash-attn for Ampere GPUs as an alternative solution. Check if xformers succeeded by running python -m xformers.info.'}\n",
      "{'question': 'What is the final step in installing Unsloth after confirming library versions?', 'answer': 'The final step is to install bitsandbytes and verify it with python -m bitsandbytes.'}\n",
      "{'question': 'What are the system requirements for installing the necessary packages?', 'answer': 'The user must have either Conda installed or be able to install it, and select between pytorch-cuda=11.8,12.1 for CUDA 11.8 or CUDA 12.1.'}\n",
      "{'question': 'How do you create a new environment with the required packages using Conda?', 'answer': \"The user should run 'conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y'.\"}\n",
      "{'question': 'What if the user does not have Conda? What alternative installation method is suggested?', 'answer': \"The user can use Pip to install the required packages, including 'pytorch-cuda=12.1' for CUDA 12.1.\"}\n",
      "{'question': 'How do you activate the newly created environment in a Linux-based system?', 'answer': \"Activate the new environment using 'conda activate unsloth_env'.\"}\n",
      "{'question': 'What command is used to install the required packages when using Pip?', 'answer': \"The user should run 'pip install unsloth'.\"}\n",
      "{'question': 'How do you download and install Miniconda on a Linux system?', 'answer': 'You can download and install Miniconda using the command wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda, then run bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3.'}\n",
      "{'question': 'What are the specific commands used to uninstall Miniconda and remove its configuration files?', 'answer': 'The command to uninstall Miniconda is rm -rf ~/miniconda3/miniconda.sh ~/miniconda3/bin/conda init bash ~/miniconda3/bin/conda init zsh.'}\n",
      "{'question': \"Can you explain the purpose of running 'bash ~/miniconda3/miniconda.sh' in the installation process?\", 'answer': \"Running this command activates Miniconda's base environment and sets up the PATH variable for conda executables.\"}\n",
      "{'question': 'How does the script handle different shell types during initialization?', 'answer': \"The script uses 'bash ~/miniconda3/bin/conda init bash' to configure Bash environments, while using 'bash ~/miniconda3/bin/conda init zsh' handles ZSH configurations.\"}\n",
      "{'question': 'What are the general steps for a complete Miniconda installation and setup on Linux?', 'answer': 'To install and set up Miniconda, download it, then run the bash script to activate and initialize it. Finally, add its bin directory to your system PATH variable.'}\n",
      "{'question': 'What versions of PyTorch are supported by the unsloth library?', 'answer': 'The unsloth library supports torch 2.1, 2.1.2, 2.2, 2.3, 2.4 and 2.5.'}\n",
      "{'question': 'What CUDA versions are compatible with the unsloth library?', 'answer': 'The library is supported on CUDA versions cu118 and cu121 and cu124.'}\n",
      "{'question': 'How do I install the unsloth library for a specific PyTorch and CUDA version combination?', 'answer': \"You can install it using pip, for example: `pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'`\"}\n",
      "{'question': 'Are there any special installation instructions for Ampere devices?', 'answer': 'Yes, for Ampere devices (A100, H100, RTX3090) and above, use cu118-ampere or cu121-ampere or cu124-ampere.'}\n",
      "{'question': 'What is the general process to install the unsloth library on my system?', 'answer': \"First, upgrade pip using `pip install --upgrade pip`, then install the unsloth library with a specific version combination, for example: `pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'`\"}\n",
      "{'question': 'What are the different versions of the unsloth library available for installation?', 'answer': 'The different versions of the unsloth library include cu118-ampere-torch240, cu121-torch240, cu118-torch240, cu121-torch230, and cu121-ampere-torch230.'}\n",
      "{'question': 'How can you install the unsloth library with specific CUDA and PyTorch versions?', 'answer': \"You can install the unsloth library by specifying the required CUDA and PyTorch versions using a command like pip install 'unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git'.\"}\n",
      "{'question': 'What is the general format for installing the unsloth library with custom configurations?', 'answer': \"The general format for installing the unsloth library with custom configurations involves using pip install followed by the required version specification, like 'unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git'.\"}\n",
      "{'question': 'Can you provide an example of installing a specific version of the unsloth library from GitHub?', 'answer': \"An example of installing a specific version of the unsloth library from GitHub is pip install 'unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git'.\"}\n",
      "{'question': 'What are some potential CUDA and PyTorch versions that can be used for installing the unsloth library?', 'answer': 'Some potential CUDA and PyTorch versions that can be used for installing the unsloth library include cu118-ampere-torch240, cu121-torch240, cu118-torch240, cu121-torch230, and cu121-ampere-torch230.'}\n",
      "{'question': 'What is the optimal pip installation command for unsloth?', 'answer': 'The optimal pip installation command can be obtained by running `wget -qOhttps://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py` in a terminal.'}\n",
      "{'question': 'How to install the required packages and torch version for unsloth?', 'answer': \"To install the required packages, run `pip install 'unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git'` or use the auto-install script.\"}\n",
      "{'question': 'What is the minimum torch version and CUDA version required for unsloth to work?', 'answer': 'The minimum torch version and CUDA version required are 12.4 and Ampere, respectively.'}\n",
      "{'question': 'How can I check if my system has the necessary requirements for running unsloth?', 'answer': 'You can check by trying to import torch in a Python REPL; if it fails, install torch via `pip install torch`.'}\n",
      "{'question': 'What happens if the CUDA version is not 12.1, 11.8, or 12.4?', 'answer': 'If the CUDA version is not 12.1, 11.8, or 12.4, an error will be raised suggesting installation via `pip install torch`.'}\n",
      "{'question': 'What causes a RuntimeError in Torch when CUDA version is less than 2.1.0?', 'answer': 'A RuntimeError occurs in Torch when the CUDA version is less than 2.1.0 due to compatibility issues.'}\n",
      "{'question': 'How does Torch handle different CUDA versions, and what are the corresponding library names for each version?', 'answer': \"Torch handles different CUDA versions by assigning specific library names based on the CUDA version: 'cu{}{}-torch211' for v <= 2.1.1, 'cu{}{}-torch212' for v <= 2.1.2, 'cu{}{}-torch220' for v < 2.3.0, 'cu{}{}-torch230' for v < 2.4.0, 'cu{}{}-torch240' for v < 2.5.0, and 'cu{}{}-torch250' for v < 2.6.0.\"}\n",
      "{'question': 'What happens if the CUDA version is greater than or equal to 2.6.0?', 'answer': 'If the CUDA version is greater than or equal to 2.6.0, a RuntimeError occurs due to Torch being too new for compatibility.'}\n",
      "{'question': 'Where can you find additional information on saving to GGUF, checkpointing, evaluation, and more?', 'answer': 'Our official Documentation provides further details on these topics.'}\n",
      "{'question': \"What are some supported training models in Hugging Face's ecosystem that can be used with unsloth_cli.py?\", 'answer': \"Huggingface's TRL, Trainer, Seq2SeqTrainer, and Pytorch code are all supported by unsloth_cli.py.\"}\n",
      "{'question': 'How do you enable downloading models from the ModelScope community using unsloth_cli.py?', 'answer': 'Set the environment variable UNSLOTH_USE_MODELSCOPE=1 to download models and install the modelscope library via pip install modelscope -U.'}\n",
      "{'question': 'What libraries or frameworks does unsloth_cli.py support for model and dataset integration?', 'answer': \"unsloth_cli.py supports Huggingface's Trainer, Seq2SeqTrainer, TRL, Pytorch code, as well as the modelscope library from ModelScope community.\"}\n",
      "{'question': 'Where can you find detailed documentation on downloading models and datasets using UNSLOTH_USE_MODELSCOPE=1 in unsloth_cli.py?', 'answer': \"Check out Hugging Face's official docs for SFT and DPO documents, which include this information.\"}\n",
      "{'question': 'What is the purpose of the provided data chunk?', 'answer': 'The data chunk serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'Which dataset was used to generate this data chunk?', 'answer': 'This data chunk is from the LAION OIG unified_chip2.jsonl dataset.'}\n",
      "{'question': 'What are some of the characteristics of the models supported by this code?', 'answer': 'The code supports 4bit pre-quantized models, such as Meta-Llama-3.1-8B-bnb-4bit and Meta-Llama-3.1-70B-bnb-4bit.'}\n",
      "{'question': 'What is the maximum sequence length supported by this model?', 'answer': 'The maximum sequence length supported by this model is 2048.'}\n",
      "{'question': 'How can you download and fine-tune a language model using this code?', 'answer': 'You can download and fine-tune a language model by specifying the desired model from the list of fourbit_models and utilizing the SFTTrainer class with its associated configuration.'}\n",
      "{'question': 'What are the names of the models mentioned in this dataset?', 'answer': 'The models mentioned are Meta-Llama-3.1-405B-bnb-4bit, Mistral-Small-Instruct-2409, mistral-7b-instruct-v0.3-bnb-4bit, Phi-3.5-mini-instruct, Phi-3-medium-4k-instruct, gemma-2-9b-bnb-4bit, and Llama-3.2-1B-bnb-4bit.'}\n",
      "{'question': 'Which models are claimed to be 2x faster than their respective counterparts?', 'answer': 'The Phi-3.5-mini-instruct and Gemma models are mentioned as being 2x faster.'}\n",
      "{'question': 'What is the size of some of the mentioned models (e.g., Mistral-Small-Instruct-2409, Llama-3.2-1B-bnb-4bit)?', 'answer': 'Mistral-Small-Instruct-2409 and Llama-3.2-1B-bnb-4bit are 22b and 1B models respectively.'}\n",
      "{'question': 'Are there any specific naming conventions or prefixes used for these model names?', 'answer': \"Yes, some of the model names have prefixes such as 'unsloth/', 'Meta-Llama-', 'Mistral-', 'Phi-3.', and 'Llama-' indicating their origin and type.\"}\n",
      "{'question': 'Are there any new or updated models mentioned in this dataset?', 'answer': 'Yes, Llama 3.2 models are mentioned as NEW!'}\n",
      "{'question': 'What are the model names listed in the provided data chunk?', 'answer': 'The model names are unsloth/Llama-3.2-3B-Instruct-bnb-4bit and unsloth/Llama-3.3-70B-Instruct-bnb-4bit.'}\n",
      "{'question': 'Can you provide more information about the updates in Llama 3.3 compared to Llama 3.2?', 'answer': 'Llama 3.3 introduces significant improvements, including a larger model size of 70B parameters.'}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'Can you describe the main topics discussed in the provided text?', 'answer': 'The topics include reinforcement learning, specifically DPO, GRPO, PPO, reward modeling, and online DPO, all of which work with Unsloth. Additionally, there is mention of custom chat templates.'}\n",
      "{'question': \"How can the Unsloth model be used in conjunction with Hugging Face's official documentation?\", 'answer': \"The Unsloth model is featured in Hugging Face's official documentation for reinforcement learning, specifically GRPO and DPO.\"}\n",
      "{'question': 'What types of reinforcement learning techniques are supported by the Unsloth model?', 'answer': 'The Unsloth model supports various reinforcement learning techniques, including DPO, GRPO, PPO, reward modeling, and online DPO.'}\n",
      "{'question': 'Are there any available notebooks for experimenting with different reinforcement learning algorithms on the Unsloth model?', 'answer': 'Yes, there are several notebooks listed for advanced Qwen3 GRPO, DPO Zephyr, ORPO, KTO, and SimPO. Clicking the link will provide access to these resources.'}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'How does the `DPOTrainer` class relate to the overall model architecture?', 'answer': 'The `DPOTrainer` class is part of the DPO (Distillation-based Prompt Optimization) framework, which enables efficient and effective training of large language models.'}\n",
      "{'question': 'What is the significance of using a FastLanguageModel in this project?', 'answer': 'FastLanguageModels are designed to provide improved inference speed while maintaining model performance, making them ideal for deployment in resource-constrained environments.'}\n",
      "{'question': 'How does the `from_pretrained` method initialize the model and tokenizer?', 'answer': 'The `from_pretrained` method loads a pre-trained FastLanguageModel and its corresponding tokenizer from a specified checkpoint file.'}\n",
      "{'question': 'What is the role of LoRA weights in the provided code snippet?', 'answer': 'LoRA (Low-Rank Adaptation) weights are used to efficiently adapt a pre-trained model to new tasks or domains, allowing for fast and effective fine-tuning.'}\n",
      "{'question': \"What is the purpose of using 'unsloth' in this code?\", 'answer': \"The 'unsloth' setting optimizes the data for use with gradient checkpointing, allowing for larger batch sizes.\"}\n",
      "{'question': \"How does the 'use_gradient_checkpointing' parameter affect model performance?\", 'answer': \"Setting 'use_gradient_checkpointing' to 'unsloth' enables the model to process longer contexts and optimize VRAM usage.\"}\n",
      "{'question': \"What is the significance of setting 'random_state' to 3407 in this code snippet?\", 'answer': \"The 'random_state' parameter ensures reproducibility of results by fixing the random seed for this specific run.\"}\n",
      "{'question': 'What are the batch size and gradient accumulation steps set to in this configuration?', 'answer': 'The model is configured with a per-device train batch size of 4 and gradient accumulation steps set to 8, allowing for efficient processing of large datasets.'}\n",
      "{'question': \"How does the 'DPOConfig' affect the overall training process?\", 'answer': \"The 'DPOConfig' controls various aspects of the training process, including learning rate, optimizer, and output directory, ensuring a smooth and effective training experience.\"}\n",
      "{'question': 'What are the context length benchmarks for Llama 3.1 (8B)?', 'answer': 'The context length benchmarks tested for Llama 3.1 (8B) are 2,972 with 8 GB of GPU VRAM, 40,724 with 16 GB of GPU VRAM, and 342,733 with 80 GB of GPU VRAM.'}\n",
      "{'question': 'What were the results when using a batch size of 1 and padding all sequences to a certain maximum sequence length?', 'answer': 'The model achieved an Unsloth context length of 932 with 12 GB of GPU VRAM, 2,551 with 16 GB of GPU VRAM, and 28,454 with 80 GB of GPU VRAM.'}\n",
      "{'question': 'What were the results when testing Llama 3.1 (8B) Instruct and doing 4bit QLoRA on all linear layers?', 'answer': 'The model achieved an Unsloth context length of OOM with 8 GB of GPU VRAM, 932 with 12 GB of GPU VRAM, and 28,454 with 80 GB of GPU VRAM.'}\n",
      "{'question': 'What were the maximum sequence lengths used in the testing process?', 'answer': 'The maximum sequence lengths used were 21,848 with 12 GB of GPU VRAM, 40,724 with 16 GB of GPU VRAM, and 342,733 with 80 GB of GPU VRAM.'}\n",
      "{'question': 'What type of workloads were the context length benchmarks mimicking?', 'answer': 'The context length benchmarks were mimicking long context fine-tuning workloads.'}\n",
      "{'question': 'Who are the developers that have significantly contributed to Unsloth?', 'answer': 'The developers include The llama.cpp library, Erik, and Etherl.'}\n",
      "{'question': 'What libraries or technologies has Unsloth incorporated for model support?', 'answer': \"Unsloth supports models from Hugging Face's TRL library, as well as Apple's ML Cross Entropy via Erik's contribution.\"}\n",
      "{'question': \"Which users have played a crucial role in Unsloth's development through their contributions or use of the platform?\", 'answer': 'The list includes every single person who has contributed to Unsloth or used it.'}\n",
      "{'question': 'What is unique about Unsloth, considering its ability to save models with specific formats?', 'answer': 'Unsloth can save models with the Unsloth format, thanks to the llama.cpp library.'}\n",
      "{'question': 'To whom does Unsloth express gratitude for their help and contributions in its development?', 'answer': 'Unsloth expresses gratitude to various individuals including Erik, The Hugging Face team, Etherl, and many more contributors.'}\n",
      "Done writing unsloth_data.json to system\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List \n",
    "from pydantic import BaseModel\n",
    "from litellm import completion\n",
    "\n",
    "def prompt_template(data: str, num_records: int = 5):\n",
    "\n",
    "    return f\"\"\"You are an expert data curator assisting a machine learning engineer in creating a high-quality instruction tuning dataset. Your task is to transform \n",
    "    the provided data chunk into diverse question and answer (Q&A) pairs that will be used to fine-tune a language model. \n",
    "\n",
    "    For each of the {num_records} entries, generate one or two well-structured questions that reflect different aspects of the information in the chunk. \n",
    "    Ensure a mix of longer and shorter questions, with shorter ones typically containing 1-2 sentences and longer ones spanning up to 3-4 sentences. Each \n",
    "    Q&A pair should be concise yet informative, capturing key insights from the data.\n",
    "\n",
    "    Structure your output in JSON format, where each object contains 'question' and 'answer' fields. The JSON structure should look like this:\n",
    "\n",
    "        \"question\": \"Your question here...\",\n",
    "        \"answer\": \"Your answer here...\"\n",
    "\n",
    "    Focus on creating clear, relevant, and varied questions that encourage the model to learn from diverse perspectives. Avoid any sensitive or biased \n",
    "    content, ensuring answers are accurate and neutral.\n",
    "\n",
    "    Example:\n",
    "    \n",
    "        \"question\": \"What is the primary purpose of this dataset?\",\n",
    "        \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
    "    \n",
    "\n",
    "    By following these guidelines, you'll contribute to a robust and effective dataset that enhances the model's performance.\"\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Explanation:**\n",
    "\n",
    "    - **Clarity and Specificity:** The revised prompt clearly defines the role of the assistant and the importance of the task, ensuring alignment with the \n",
    "    project goals.\n",
    "    - **Quality Standards:** It emphasizes the need for well-formulated Q&A pairs, specifying the structure and content of each question and answer.\n",
    "    - **Output Format:** An example JSON structure is provided to guide the format accurately.\n",
    "    - **Constraints and Biases:** A note on avoiding sensitive or biased content ensures ethical considerations are met.\n",
    "    - **Step-by-Step Guidance:** The prompt breaks down the task into manageable steps, making it easier for the assistant to follow.\n",
    "\n",
    "    This approach ensures that the generated data is both high-quality and meets the specific requirements of the machine learning project.\n",
    "    \n",
    "    Data\n",
    "    {data}\n",
    "    \"\"\"\n",
    "\n",
    "class Record(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class Response(BaseModel):\n",
    "    generated: List[Record]\n",
    "\n",
    "def llm_call(data: str, num_records: int = 5) -> dict:\n",
    "    stream = completion(\n",
    "        model=\"ollama/llama3.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_template(data, num_records),\n",
    "            }\n",
    "        ],\n",
    "        stream=True,\n",
    "        options={\"num_predict\": 2000},\n",
    "        format=Response.model_json_schema(),\n",
    "    )\n",
    "    data = \"\"\n",
    "    for x in stream: \n",
    "        delta = x['choices'][0][\"delta\"][\"content\"]\n",
    "        if delta is not None: \n",
    "            # print(delta, end=\"\") \n",
    "            data += delta \n",
    "    return json.loads(data)\n",
    "\n",
    "dataset = []\n",
    "for i, chunk in enumerate(contextualized_chunks):\n",
    "    data = llm_call(chunk)\n",
    "    for pair in data['generated']:\n",
    "        print(pair)\n",
    "        dataset.append({\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer']\n",
    "            })\n",
    "tuning_data = 'unsloth_data.json'\n",
    "with open(tuning_data,'w') as f: \n",
    "    json.dump(dataset, f) \n",
    "\n",
    "print(f\"Done writing {tuning_data} to system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb0d03-f066-4850-b250-ba4bb5983635",
   "metadata": {},
   "source": [
    "Now that we have a set of training data we are ready to train!\n",
    "\n",
    "Before doing however we need to install unsloth to help speedup the performance the reduce the RAM required for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14f23ca-942c-4515-9616-f3e4d66f9e3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: unsloth in ./venv/lib/python3.12/site-packages (2025.6.12)\n",
      "Collecting vllm\n",
      "  Downloading vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.6.8 in ./venv/lib/python3.12/site-packages (from unsloth) (2025.6.8)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in ./venv/lib/python3.12/site-packages (from unsloth) (0.0.30)\n",
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.12/site-packages (from unsloth) (0.46.1)\n",
      "Requirement already satisfied: triton>=3.0.0 in ./venv/lib/python3.12/site-packages (from unsloth) (3.3.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: tyro in ./venv/lib/python3.12/site-packages (from unsloth) (0.9.26)\n",
      "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 in ./venv/lib/python3.12/site-packages (from unsloth) (4.53.0)\n",
      "Requirement already satisfied: datasets>=3.4.1 in ./venv/lib/python3.12/site-packages (from unsloth) (3.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in ./venv/lib/python3.12/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.12/site-packages (from unsloth) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in ./venv/lib/python3.12/site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from unsloth) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in ./venv/lib/python3.12/site-packages (from unsloth) (1.8.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in ./venv/lib/python3.12/site-packages (from unsloth) (0.19.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in ./venv/lib/python3.12/site-packages (from unsloth) (0.16.0)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.12/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.12/site-packages (from unsloth) (0.33.2)\n",
      "Requirement already satisfied: hf_transfer in ./venv/lib/python3.12/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in ./venv/lib/python3.12/site-packages (from unsloth) (0.34.0)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (from unsloth) (0.22.1+cu128)\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.12/site-packages (from vllm) (2024.11.6)\n",
      "Collecting cachetools (from vllm)\n",
      "  Downloading cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.12/site-packages (from vllm) (2.32.4)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in ./venv/lib/python3.12/site-packages (from vllm) (0.21.2)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.12/site-packages (from vllm) (3.12.13)\n",
      "Requirement already satisfied: openai>=1.52.0 in ./venv/lib/python3.12/site-packages (from vllm) (1.93.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in ./venv/lib/python3.12/site-packages (from vllm) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in ./venv/lib/python3.12/site-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.12/site-packages (from vllm) (11.0.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in ./venv/lib/python3.12/site-packages (from vllm) (0.9.0)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
      "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines==0.1.11 (from vllm)\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.19 (from vllm)\n",
      "  Downloading xgrammar-0.1.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in ./venv/lib/python3.12/site-packages (from vllm) (4.12.2)\n",
      "Collecting filelock>=3.16.1 (from vllm)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in ./venv/lib/python3.12/site-packages (from vllm) (27.0.0)\n",
      "Requirement already satisfied: msgspec in ./venv/lib/python3.12/site-packages (from vllm) (0.19.0)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
      "  Downloading mistral_common-1.6.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in ./venv/lib/python3.12/site-packages (from vllm) (4.11.0.86)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.12/site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: six>=1.16.0 in ./venv/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Collecting setuptools<80,>=77.0.3 (from vllm)\n",
      "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.10.1 (from vllm)\n",
      "  Downloading compressed_tensors-0.10.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.18.0 (from vllm)\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in ./venv/lib/python3.12/site-packages (from vllm) (3.3.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.12/site-packages (from vllm) (1.16.0)\n",
      "Requirement already satisfied: ninja in ./venv/lib/python3.12/site-packages (from vllm) (1.11.1.4)\n",
      "Collecting opentelemetry-sdk>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-api>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n",
      "  Downloading opentelemetry_semantic_conventions_ai-0.4.10-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading ray-2.47.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting torchaudio==2.7.0 (from vllm)\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in ./venv/lib/python3.12/site-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting interegular (from outlines==0.1.11->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (3.1.4)\n",
      "Requirement already satisfied: nest_asyncio in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: referencing in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (4.24.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
      "  Downloading airportsdata-20250622-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
      "  Downloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (3.3)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./venv/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./venv/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (2.26.2)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.12/site-packages (from datasets>=3.4.1->unsloth) (20.0.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from datasets>=3.4.1->unsloth) (2.3.0)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.12/site-packages (from datasets>=3.4.1->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.12/site-packages (from datasets>=3.4.1->unsloth) (0.70.16)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Collecting jinja2 (from outlines==0.1.11->vllm)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.12/site-packages (from huggingface_hub->unsloth) (1.1.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-api>=1.26.0->vllm) (8.7.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.63.2 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (0.4.1)\n",
      "Requirement already satisfied: click>=7.0 in ./venv/lib/python3.12/site-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.2.1)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading msgpack-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2025.6.15)\n",
      "INFO: pip is looking at multiple versions of unsloth-zoo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ray[cgraph]!=2.44.*,>=2.43.0 (from vllm)\n",
      "  Downloading ray-2.47.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "  Downloading ray-2.46.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Downloading ray-2.45.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Downloading ray-2.43.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: aiosignal in ./venv/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist in ./venv/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.7.0)\n",
      "INFO: pip is looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of unsloth-zoo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting vllm\n",
      "  Downloading vllm-0.9.0.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Collecting compressed-tensors==0.9.4 (from vllm)\n",
      "  Downloading compressed_tensors-0.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cut_cross_entropy in ./venv/lib/python3.12/site-packages (from unsloth_zoo>=2025.6.8->unsloth) (25.1.1)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unsloth_zoo>=2025.6.8 (from unsloth)\n",
      "  Using cached unsloth_zoo-2025.6.8-py3-none-any.whl.metadata (8.1 kB)\n",
      "INFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting vllm\n",
      "  Downloading vllm-0.9.0-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
      "Collecting xgrammar==0.1.18 (from vllm)\n",
      "  Downloading xgrammar-0.1.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting setuptools>=74.1.1 (from vllm)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting compressed-tensors==0.9.3 (from vllm)\n",
      "  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchaudio==2.6.0 (from vllm)\n",
      "  Downloading torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy==1.13.1->torch<=2.7.0,>=2.4.0->unsloth) (1.3.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<1.27.0,>=1.26.0->vllm)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib_metadata (from vllm)\n",
      "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.12/site-packages (from importlib_metadata->vllm) (3.23.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (1.20.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in ./venv/lib/python3.12/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./venv/lib/python3.12/site-packages (from tyro->unsloth) (14.0.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./venv/lib/python3.12/site-packages (from tyro->unsloth) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./venv/lib/python3.12/site-packages (from tyro->unsloth) (4.4.4)\n",
      "Collecting typing_extensions>=4.10 (from vllm)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: typer>=0.12.3 in ./venv/lib/python3.12/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich_toolkit-0.14.8-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->vllm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.26.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.12/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Downloading vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl (326.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl (44.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.47.1-cp312-cp312-manylinux2014_x86_64.whl (68.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.6.3-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.10-py3-none-any.whl (5.6 kB)\n",
      "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading airportsdata-20250622-py3-none-any.whl (912 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl (105.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich_toolkit-0.14.8-py3-none-any.whl (24 kB)\n",
      "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, py-cpuinfo, nvidia-cusparselt-cu12, fastrlock, blake3, wrapt, websockets, uvloop, uvicorn, typing_extensions, sympy, setuptools, python-multipart, pycountry, partial-json-parser, opentelemetry-semantic-conventions-ai, opentelemetry-proto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgpack, llvmlite, llguidance, lark, jinja2, interegular, importlib_metadata, httptools, grpcio, googleapis-common-protos, gguf, filelock, einops, dnspython, diskcache, cupy-cuda12x, cloudpickle, cachetools, astor, airportsdata, opentelemetry-exporter-otlp-proto-common, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, email-validator, depyf, deprecated, watchfiles, starlette, rich-toolkit, opentelemetry-api, nvidia-cusolver-cu12, torch, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, lm-format-enforcer, fastapi, xgrammar, xformers, torchvision, torchaudio, ray, outlines_core, opentelemetry-sdk, mistral_common, fastapi-cli, compressed-tensors, outlines, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, vllm\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.1\n",
      "    Uninstalling triton-3.3.1:\n",
      "      Successfully uninstalled triton-3.3.1\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 70.2.0\n",
      "    Uninstalling setuptools-70.2.0:\n",
      "      Successfully uninstalled setuptools-70.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.55\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.55:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.55\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.61\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
      "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.55\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.55:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.55\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.41\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.41:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.41\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.61\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.3.14\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.3.14:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.3.14\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: importlib_metadata\n",
      "    Found existing installation: importlib_metadata 8.7.0\n",
      "    Uninstalling importlib_metadata-8.7.0:\n",
      "      Successfully uninstalled importlib_metadata-8.7.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.7.53\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.7.53:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.7.53\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.7.1.26\n",
      "    Uninstalling nvidia-cudnn-cu12-9.7.1.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.7.1.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.2.55\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.2.55:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.2.55\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1+cu128\n",
      "    Uninstalling torch-2.7.1+cu128:\n",
      "      Successfully uninstalled torch-2.7.1+cu128\n",
      "  Attempting uninstall: xformers\n",
      "    Found existing installation: xformers 0.0.30\n",
      "    Uninstalling xformers-0.0.30:\n",
      "      Successfully uninstalled xformers-0.0.30\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.1+cu128\n",
      "    Uninstalling torchvision-0.22.1+cu128:\n",
      "      Successfully uninstalled torchvision-0.22.1+cu128\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.7.1+cu128\n",
      "    Uninstalling torchaudio-2.7.1+cu128:\n",
      "      Successfully uninstalled torchaudio-2.7.1+cu128\n",
      "Successfully installed airportsdata-20250622 astor-0.8.1 blake3-1.0.5 cachetools-6.1.0 cloudpickle-3.1.1 compressed-tensors-0.9.3 cupy-cuda12x-13.4.1 deprecated-1.2.18 depyf-0.18.0 diskcache-5.6.3 dnspython-2.7.0 einops-0.8.1 email-validator-2.2.0 fastapi-0.115.14 fastapi-cli-0.0.7 fastrlock-0.8.3 filelock-3.18.0 gguf-0.17.1 googleapis-common-protos-1.70.0 grpcio-1.73.1 httptools-0.6.4 importlib_metadata-8.0.0 interegular-0.3.3 jinja2-3.1.6 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.6.3 msgpack-1.1.1 numba-0.61.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.10 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pycountry-24.6.1 python-multipart-0.0.20 ray-2.47.1 rich-toolkit-0.14.8 setuptools-80.9.0 starlette-0.46.2 sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0 typing_extensions-4.14.1 uvicorn-0.35.0 uvloop-0.21.0 vllm-0.8.5.post1 watchfiles-1.1.0 websockets-15.0.1 wrapt-1.17.2 xformers-0.0.29.post2 xgrammar-0.1.18\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d1d7e-0406-4510-a20d-e2e3718c3bd4",
   "metadata": {},
   "source": [
    "How let's setup our tokenizer using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d21330a-792e-422e-aea7-4de988f4110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.12: Fast Llama patching. Transformers: 4.53.1. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070. Num GPUs = 1. Max memory: 11.94 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m max_seq_length = \u001b[32m2048\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munsloth/Meta-Llama-3.1-8B-bnb-4bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m prompt_style = \u001b[33m\"\"\"\u001b[39m\u001b[33mBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\u001b[39m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[33m### Instruction:\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[33m### Response:\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     23\u001b[39m EOS_TOKEN = tokenizer.eos_token \u001b[38;5;66;03m# Must add EOS_TOKEN\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brian-intel/fine-tuning/venv/lib/python3.12/site-packages/unsloth/models/loader.py:393\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    417\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brian-intel/fine-tuning/venv/lib/python3.12/site-packages/unsloth/models/llama.py:1912\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, **kwargs)\u001b[39m\n\u001b[32m   1899\u001b[39m     model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m   1900\u001b[39m         model_name,\n\u001b[32m   1901\u001b[39m         device_map              = device_map,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1909\u001b[39m         **kwargs,\n\u001b[32m   1910\u001b[39m     )\n\u001b[32m   1911\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[32m-> \u001b[39m\u001b[32m1912\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meager\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1923\u001b[39m     model.fast_generate = model.generate\n\u001b[32m   1924\u001b[39m     model.fast_generate_batches = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brian-intel/fine-tuning/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brian-intel/fine-tuning/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brian-intel/fine-tuning/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4820\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4818\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   4819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4820\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4822\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   4823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brian-intel/fine-tuning/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1460\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1457\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1463\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brian-intel/fine-tuning/venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:117\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None\n",
    ")\n",
    "\n",
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "\"You are a helpful, honest and harmless assitant designed to help engineers. Think through each question logically and provide an answer. Don't make things up, if you're unable to answer a question advise the user that you're unable to answer as it is outside of your scope.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"question\"]\n",
    "    outputs      = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = prompt_style.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"data\", split='train')\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset[\"text\"][0])\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=128   , \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0, \n",
    "    bias=\"none\", \n",
    "   \n",
    "    use_gradient_checkpointing=\"unsloth\", \n",
    "    random_state=3407,\n",
    "    use_rslora=False, \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81e60b-f0df-4d5d-8cfa-f00f62413574",
   "metadata": {},
   "source": [
    "Now that we have the tokenizer we can setup the prompt for fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a582658-e27b-4d5e-bdd6-de21c7521f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df49a026d66345339826a7d4b5eea0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    num_train_epochs = 10,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 100,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a9d66-1c90-4f75-9ac9-e13a21ceef9a",
   "metadata": {},
   "source": [
    "The next step is to map our dataset to the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e82d836-9a6e-4d79-b242-624ebafe160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 117 | Num Epochs = 25 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 335,544,320 of 8,000,000,000 (4.19% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 09:05, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.472800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549.5646 seconds used for training.\n",
      "9.16 minutes used for training.\n",
      "Peak reserved memory = 10.236 GB.\n",
      "Peak reserved memory for training = 3.576 GB.\n",
      "Peak reserved memory % of max memory = 85.729 %.\n",
      "Peak reserved memory for training % of max memory = 29.95 %.\n"
     ]
    }
   ],
   "source": [
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024, 3) \n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# get performance\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da8dd3-9e96-4a33-9d24-6214408d82bc",
   "metadata": {},
   "source": [
    "Save the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7eeae5b-10f4-4174-9d5e-3f77bd0d6ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama-3.1-8B-unsloth/tokenizer_config.json',\n",
       " 'Llama-3.1-8B-unsloth/special_tokens_map.json',\n",
       " 'Llama-3.1-8B-unsloth/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_local = \"Llama-3.1-8B-unsloth\"\n",
    "model.save_pretrained(new_model_local) # Local saving\n",
    "tokenizer.save_pretrained(new_model_local) # Local saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b6601-021c-477e-a5c5-1442b9905746",
   "metadata": {},
   "source": [
    "host in local ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa08fff2-85f6-4ec5-9ae1-0184ceb6b9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 0% ⠋ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 5% ⠙ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 16% ⠹ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 27% ⠸ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 32% ⠼ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 42% ⠴ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 53% ⠦ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 58% ⠧ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 69% ⠇ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 79% ⠏ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 84% ⠋ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 94% ⠙ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter \u001b[K\n",
      "using existing layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 \u001b[K\n",
      "using existing layer sha256:948af2743fc78a328dcb3b0f5a31b3d75f415840fdb699e8b1235978392ecf85 \u001b[K\n",
      "using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177 \u001b[K\n",
      "creating new layer sha256:d7708cc5dde5e99feed15efe6f923f5737f34d9bff5d63c579e27e1e641437a3 \u001b[K\n",
      "using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama create unsloth-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf389d-2fb2-4603-b9b1-619e50a789e8",
   "metadata": {},
   "source": [
    "Call latest model from ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95f65071-df25-4968-8eb1-786c950363fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not aware of any recent news or updates from Unsloth.ai. As a conversational AI, I don't have direct access to the web and may not be up-to-date on the latest developments.\n",
      "\n",
      "However, I can suggest some possible ways to find recent news or updates from Unsloth.ai:\n",
      "\n",
      "1. Check their website: You can visit Unsloth.ai's website and check for any recent blog posts, announcements, or updates.\n",
      "2. Social media: Unsloth.ai may have a social media presence on platforms like Twitter, LinkedIn, or Facebook. You can try searching for them on these platforms to see if they've posted any recent updates.\n",
      "3. News articles: You can try searching online for news articles related to Unsloth.ai or their products.\n",
      "\n",
      "If you're looking for specific information, I'd be happy to help with that!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"unsloth-trained\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4e799-5b50-42b2-9cea-8f3d20e20134",
   "metadata": {},
   "source": [
    "Results inconsistent let's try rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c19c4f35-e9cf-4c7d-8a22-8f99364e2697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documentation, here are some key points and news related to Unsloth:\n",
      "\n",
      "**News and Updates**\n",
      "\n",
      "* Unsloth was tested using the Alpaca Dataset with a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down). The results showed that Unsloth achieves a significant VRAM reduction of >75% compared to Hugging Face + FA2.\n",
      "* Benchmarking of Unsloth was also conducted by Hugging Face. They tested Llama 3.1 (8B) and Llama 3.3 (70B) with QLoRA on all linear layers and reported that Unsloth achieves a longer context length than Hugging Face + FA2.\n",
      "\n",
      "**Recent Developments**\n",
      "\n",
      "* Unsloth has added support for 4-bit quantization, which reduces memory usage while maintaining accuracy.\n",
      "* The team has also implemented full finetuning capabilities, allowing users to fine-tune models from scratch.\n",
      "* A new LoRA module called \"unsloth\" has been introduced, which uses 30% less VRAM and fits 2x larger batch sizes.\n",
      "\n",
      "**Recent Contributions**\n",
      "\n",
      "* The llama.cpp library allows users to save models with Unsloth.\n",
      "* The Hugging Face team and their TRL library have contributed significantly to Unsloth's development.\n",
      "* Erik helped add Apple's ML Cross Entropy in Unsloth, while Etherl added support for TTS, diffusion, and BERT models.\n",
      "\n",
      "**Reinforcement Learning (RL) Support**\n",
      "\n",
      "* Unsloth supports various RL algorithms, including DPO, GRPO, PPO, Reward Modeling, and Online DPO.\n",
      "* The team has also published several notebooks on GitHub, showcasing the usage of these RL algorithms with Unsloth.\n",
      "\n",
      "**Performance Benchmarking**\n",
      "\n",
      "* Unsloth has been tested using the Alpaca Dataset, and the results showed a significant VRAM reduction compared to Hugging Face + FA2.\n",
      "* Context length benchmarks have been conducted using Llama 3.1 (8B) and Llama 3.3 (70B), demonstrating that Unsloth achieves longer context lengths than Hugging Face + FA2.\n",
      "\n",
      "**Citation**\n",
      "\n",
      "If you're planning to use or reference Unsloth in your research, please cite the repository as follows:\n",
      "\n",
      "@software{unsloth,\n",
      "author = {Daniel Han, Michael Han and Unsloth team},\n",
      "title = {Unsloth},\n",
      "url = {http://github.com/unslothai/unsloth},\n",
      "year = {2023}\n",
      "}\n",
      "\n",
      "Overall, Unsloth continues to evolve with new features and improvements, making it an exciting development in the NLP community.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "\n",
    "# Load and extract text from the PDF\n",
    "def load_pdf_content(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_content = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_content += page.extract_text() + \"\\n\"\n",
    "    return text_content\n",
    "\n",
    "# Load the unsloth documentation\n",
    "pdf_content = load_pdf_content(\"unsloth_documentation.pdf\")\n",
    "\n",
    "# Create the RAG-enhanced prompt\n",
    "rag_prompt = f\"\"\"Based on the following documentation about Unsloth.ai, please answer the user's question:\n",
    "\n",
    "Documentation:\n",
    "{pdf_content}\n",
    "\n",
    "User Question: Give me some Unsloth.ai News?\n",
    "\n",
    "Please provide a comprehensive answer based on the documentation provid d.\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
