{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36078ad3-348c-4164-8483-955966a4f020",
   "metadata": {},
   "source": [
    "Initial query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361ecb9-8a5c-446b-990d-9f70ebdc04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "payload = {\n",
    "    \"model\": \"deepseek-r1:1.5b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about QLoRA in 100 words or less.\"}],\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, data=json.dumps(payload))\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600a5a9-705c-42a0-9141-40c1e7039346",
   "metadata": {},
   "source": [
    "Now let's do it in python. \n",
    "\n",
    "First we need to install ollama with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3c5ce-6258-4930-9b3a-0956b39620f1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbce5ce-0222-4cd8-a28a-9e63ac989e98",
   "metadata": {},
   "source": [
    "Next let's create a simple python query using the ollama library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7550214-582d-47bf-8f6d-5b6ab26cbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Why is QLoRA such a popular method for fine tuning. Explain this in 100 words or less.\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48630fa6-a7ed-4137-8e42-442237ab81e2",
   "metadata": {},
   "source": [
    "As you can see unsloth is an unown term to the default deepseek model and the results can be quite humorous. We will want to fix this by fine tuning the model and give it some information about the unsloth library. \n",
    "The first step in doing this is to convert the provided unsloth_documentation.pdf into a dataset for training.\n",
    "For this task we can use Docling and LiteLLM.\n",
    "To help visualize different outputs we'll also install colorama to color code out terminal outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling litellm colorama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362af26",
   "metadata": {},
   "source": [
    "Most Likely you will want to run this on GPU so you'll need to install pytorch as well. I used the latest version with CUDA 12.8 since I have a 5000 series GPU which is not supported by earlier CUDA versions. Please check version [compatability](https://developer.nvidia.com/cuda-gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1207d",
   "metadata": {},
   "source": [
    "You may also might need to update huggingface cache persmissions and pre-download the docling-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo chown -R $(whoami) ~/.cache/huggingface\n",
    "huggingface-cli download ds4sd/docling-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc837f",
   "metadata": {},
   "source": [
    "Now We can being getting out data chunks from the pdf and save them to a list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b69cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "converter = DocumentConverter()\n",
    "doc = converter.convert(\"unsloth_documentation.pdf\").document\n",
    "chunker = HybridChunker()\n",
    "chunks = chunker.chunk(dl_doc=doc)\n",
    "\n",
    "contextualized_chunks = []\n",
    "for i, chunk in enumerate(chunks): \n",
    "    print( f\"Raw Text:\\n{chunk.text[:300]}…\" )\n",
    "    contextualized_chunks.append(chunker.contextualize(chunk=chunk))\n",
    "    print(f\"Contextualized Text:\\n{contextualized_chunks[i][:300]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32283d35",
   "metadata": {},
   "source": [
    "Now that we have contextualize chunks we can use ollama to generate data for our fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What are the benefits of using specific language models for fine-tuning?\",\n",
      "      \"answer\": \"Using Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral results in a speed boost and significant VRAM reduction.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Can you elaborate on how these specialized models improve performance?\",\n",
      "      \"answer\": \"They enable fine-tuning up to 2x faster with 80% less VRAM requirements compared to the original setup.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are the implications of using these optimized language models for large-scale applications?\",\n",
      "      \"answer\": \"By leveraging Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral, developers can achieve better performance and efficiency in their projects.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How do the specifications of these language models compare to traditional models?\",\n",
      "      \"answer\": \"These specialized models provide a unique combination of speed and memory efficiency not typically seen in standard language models.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are some potential use cases where these optimized models would be particularly beneficial?\",\n",
      "      \"answer\": \"Applications requiring extensive fine-tuning, such as large-scale NLP tasks or complex text generation projects, can greatly benefit from using Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the purpose of this prompt?\",\n",
      "      \"answer\": \"This prompt serves as a guide for creating a high-quality instruction tuning dataset.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How can I fine-tune a language model using this dataset?\",\n",
      "      \"answer\": \"You can add your dataset, click 'Run All', and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Are notebooks beginner-friendly for working with this dataset?\",\n",
      "      \"answer\": \"Yes, notebooks are beginner-friendly for working with this dataset. A guide is available for reference.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are the benefits of using this finetuning approach?\",\n",
      "      \"answer\": \"The benefits include fine-tuning a language model using your own dataset and exporting it to various platforms.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What are the key features of Gemma 3n (4B) compared to other models?\",\n",
      "            \"answer\": \"Gemma 3n (4B) offers 1.5x faster performance and 50% less memory use.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Can you compare the performance of Qwen3 (14B) with Gemma 3n (4B)?\",\n",
      "            \"answer\": \"Qwen3 (14B) has a performance boost of 2x compared to Gemma 3n (4B).\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are some notable differences in memory usage between Qwen3 (14B) and other models?\",\n",
      "            \"answer\": \"Qwen3 (14B) has a memory use reduction of 70% compared to other models.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does Gemma 3n (4B) compare to Llama 3.2 (3B) in terms of performance and memory usage?\",\n",
      "            \"answer\": \"Gemma 3n (4B) offers slightly faster performance than Llama 3.2 (3B), but similar memory reduction.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Can you summarize the key benefits of using Qwen3 (14B): GRPO compared to other models?\",\n",
      "            \"answer\": \"Qwen3 (14B): GRPO offers 2x faster performance and an 80% reduction in memory usage.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What are the key benefits of using Phi-4 (14B) model?\",\n",
      "            \"answer\": \"Phi-4 (14B) offers improved performance, reducing memory usage by 70%, and starting notebooks for free.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does Llama 3.2 Vision (11B) differ from other models in terms of notebook provision and resource efficiency?\",\n",
      "            \"answer\": \"Llama 3.2 Vision (11B) also starts notebooks for free, delivers a performance boost of 2x faster, and exhibits significant memory savings.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "   {\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What are the supported platforms for the data curation tool?\",\n",
      "      \"answer\": \"The data curation tool supports Kaggle, GRPO, TTS, and Vision.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Where can I find detailed documentation for the Unsloth model?\",\n",
      "      \"answer\": \"Detailed documentation for the Unsloth model is available at this link: [insert link].\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      " {\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the command to install the Unsloth package using pip on Linux?\",\n",
      "      \"answer\": \"The command to install the Unsloth package using pip on Linux is: 'pip install unsloth'.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How do I install Unsloth on a Windows device if the pip method doesn't work?\",\n",
      "      \"answer\": \"For Windows installation, please refer to the provided link for instructions.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the recommended method of installing the Unsloth package?\",\n",
      "      \"answer\": \"The recommended method of installing the Unsloth package is using pip on Linux devices: 'pip install unsloth'.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Where can I find alternative installation instructions for Unsloth on Windows if needed?\",\n",
      "      \"answer\": \"Alternative installation instructions for Unsloth on Windows are provided in a separate link.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the primary purpose of this dataset, and how will it be used for model fine-tuning?\",\n",
      "      \"answer\": \"This dataset serves as training data for fine-tuning a language model to generate accurate responses to technical questions.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What recent development has been announced for Gemma 3n by Google?\",\n",
      "            \"answer\": \"Gemma 3n by Google recently allowed users to upload GGUFs and 4-bit models.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Can you provide more information about Qwen3, a model that's now supported?\",\n",
      "            \"answer\": \"Qwen3 is a supported model that requires only 17.5GB VRAM for operation, specifically the variant Qwen3-30B-A3B.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are some new features added to Unsloth.ai News in terms of text-to-speech capabilities?\",\n",
      "            \"answer\": \"The platform now supports Text-to-Speech (TTS) with options including sesame/csm-1b and STT openai/whisper-large-v3.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How have the dynamic quantization techniques evolved on Unsloth.ai News?\",\n",
      "            \"answer\": \"Unsloth.ai News has introduced Dynamic 2.0 quants that set new benchmarks in both 5-shot MMLU and KL Divergence.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Can you detail the recent addition of Llama 4 by Meta, including Scout & Maverick to Unsloth.ai News?\",\n",
      "            \"answer\": \"Llama 4 by Meta, which includes models such as Scout & Maverick, is now supported on Unsloth.ai News.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What are the supported models for fine-tuning in Unsloth.ai News?\",\n",
      "            \"answer\": \"The supported models include BERT, diffusion, Cohere, Mamba, and others.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How can I enable FFT (Fast Fourier Transform) in my model training?\",\n",
      "            \"answer\": \"You can enable FFT by setting full_finetuning to True or using 8-bit with load_in_8bit set to True.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is DeepSeek-R1, and how can I use it?\",\n",
      "            \"answer\": \"DeepSeek-R1 is a model that you can run or fine-tune using Unsloth's guide. Model uploads are available here.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does Long-context Reasoning (GRPO) work in Unsloth.ai News?\",\n",
      "            \"answer\": \"You can train your own reasoning model with just 5GB VRAM, transforming models like Llama, Phi, or Mistral into reasoning LLMs.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the advantage of using Unsloth Dynamic 4-bit Quantization compared to BnB 4-bit?\",\n",
      "            \"answer\": \"Unsloth Dynamic 4-bit Quantization dynamically chooses not to quantize certain parameters, increasing accuracy while using <10% more VRAM than BnB 4-bit.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Can you provide an update on Phi-4 by Microsoft and its compatibility with Unsloth?\",\n",
      "            \"answer\": \"Yes, we fixed bugs in Phi-4 and uploaded GGUFs (Generalized Graph Unitary Factors) for 4-bit support.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What new features has Unsloth.ai added to its platform?\",\n",
      "            \"answer\": \"Unsloth.ai now supports vision models, including Llama 3.2 Vision (11B), Qwen 2.5 VL (7B), and Pixtral (12B).\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does Unsloth's support for context windows compare to other platforms?\",\n",
      "            \"answer\": \"Unsloth enables up to 4x longer context windows, surpassing its native support.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the significance of Cut Cross Entropy in Unsloth's platform?\",\n",
      "            \"answer\": \"Cut Cross Entropy was added by collaborating with Apple, allowing for 89K context support for Meta's Llama 3.3 (70B) on an 80GB GPU.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the result of the recent update to Unsloth and transformers?\",\n",
      "            \"answer\": \"The gradient accumulation bug has been fixed, enabling improved performance.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does Unsloth's memory usage compare after its latest optimization?\",\n",
      "            \"answer\": \"Memory usage has been reduced by a further 30%.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What are the primary categories of content presented in this data chunk?\",\n",
      "      \"answer\": \"The main categories include links, resources, documentation, wiki, and social media platforms.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How can one access the installation guide for a machine learning model from this dataset?\",\n",
      "      \"answer\": \"You can find the installation guide under the 'Installation' section.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the purpose of including links to external resources in this data chunk?\",\n",
      "      \"answer\": \"The links provide additional information and context related to the topics discussed.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Can you give an example of a social media platform mentioned in this dataset?\",\n",
      "      \"answer\": \"Twitter (also referred to as X) is one of the social media platforms listed.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are some benefits of following the official documentation and blog for updates related to the project?\",\n",
      "      \"answer\": \"Following the official documentation and blog allows users to stay updated with the latest information, releases, and news about the project.\"\n",
      "    }\n",
      "  ]\n",
      "}{\"generated\": []}\n",
      "        \t\t\t\t\t\t\t \t{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the task that needs to be performed in this context?\",\n",
      "      \"answer\": \"The task is to install Unsloth.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Where can one find more detailed installation and updating instructions for Unsloth?\",\n",
      "      \"answer\": \"One can find these instructions in the documentation provided by Unsloth.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What command is used to install a package using Python's package manager?\",\n",
      "      \"answer\": \"The command to install a package using Python's package manager is pip.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How can you ensure pip is installed on your Linux device for installing packages?\",\n",
      "      \"answer\": \"You can use the command 'pip --version' to verify if pip is installed, and then proceed with installation if it's not already present.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the primary method of package management mentioned in this data chunk?\",\n",
      "      \"answer\": \"The primary method of package management mentioned here is using Python's package manager, which involves installing packages via pip.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Is there a recommendation for manually verifying if unsloth is installed before attempting to install it?\",\n",
      "      \"answer\": \"No specific instructions are given in the data chunk for manually verifying if unsloth is installed. However, using pip for installation as recommended is mentioned.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What does the 'recommended' status imply for using pip on Linux devices?\",\n",
      "      \"answer\": \"The 'recommended' status indicates that using pip to install packages on Linux devices is preferred due to its simplicity and effectiveness in package management.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What are the necessary steps for setting up a development environment?\",\n",
      "            \"answer\": \"To set up a development environment, install NVIDIA Video Driver, Visual Studio C++, and CUDA Toolkit.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Which software should be installed in step 1 of the setup process?\",\n",
      "            \"answer\": \"In step 1, you should install the latest version of your GPUs driver from NVIDIA GPU Drive.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What should be selected during the installation of Visual Studio to ensure C++ is included?\",\n",
      "            \"answer\": \"When installing Visual Studio, select all C++ options and also choose the Windows 10/11 SDK for detailed instructions with options, see here.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Can I use Python 3.13 to install Unsloth due to its compatibility issues?\",\n",
      "            \"answer\": \"No, due to compatibility issues, you should use Python 3.12, 3.11, or 3.10 instead of Python 3.13.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the purpose of installing CUDA Toolkit in the setup process?\",\n",
      "            \"answer\": \"Installing CUDA Toolkit is crucial for setting up a development environment as it provides the necessary tools and libraries for the environment to function properly.\"\n",
      "        }\n",
      "    ]\n",
      "}{ \"generated\": [\n",
      "    {\n",
      "        \"question\": \"What is Unsloth, and how do I install it?\",\n",
      "        \"answer\": \"Unsloth can be installed using pip with the command 'pip install unsloth'.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"How do I run Unsloth directly on Windows if it requires specific configurations?\",\n",
      "        \"answer\": \"To run Unsloth directly on Windows, you need to install Triton from a specific fork that supports PyTorch >= 2.4 and CUDA 12. Follow the instructions provided for the Windows fork carefully.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What is the issue with running Unsloth in parallel on Windows?\",\n",
      "        \"answer\": \"Running Unsloth in parallel on Windows can cause a crashing issue due to dataset_num_proc settings. Set dataset_num_proc=1 in the SFTTrainer to avoid this problem.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"How do I resolve advanced installation issues or weird errors during installation?\",\n",
      "        \"answer\": \"For advanced installation instructions or troubleshooting, refer to the provided documentation for handling specific installation-related problems.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What are the minimum requirements for running Unsloth on Windows, including PyTorch and CUDA versions?\",\n",
      "        \"answer\": \"Unsloth requires PyTorch >= 2.4 and CUDA 12 for successful installation and operation on Windows.\"\n",
      "    }\n",
      "] }{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the step-by-step process to install Unsloth?\",\n",
      "            \"answer\": \"To install Unsloth, you need to follow these steps: (1) Install torch and triton by going to https://pytorch.org and running pip install torch torchvision torchaudio triton, (2) Confirm if CUDA is installed correctly by trying nvcc, (3) Install xformers manually, and (4) Double check that all software versions are compatible with each other.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How do you troubleshoot issues during the installation of Unsloth?\",\n",
      "            \"answer\": \"You can troubleshoot issues during the installation of Unsloth by checking if CUDA is installed correctly by trying nvcc, and if xformers succeeded with python -m xformers.info. Additionally, you can check the PyTorch Compatibility Matrix for compatible versions.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are the different options available to install xformers?\",\n",
      "            \"answer\": \"You have two options to install xformers: (1) Try installing vllm and see if vllm succeeds, or (2) Install flash-attn for Ampere GPUs.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How do you verify that Unsloth has been installed successfully?\",\n",
      "            \"answer\": \"You can verify that Unsloth has been installed successfully by running python -m xformers.info and python -m bitsandbytes. If both commands run without errors, then Unsloth is installed correctly.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are some common issues that may arise during the installation of Unsloth?\",\n",
      "            \"answer\": \"Common issues that may arise during the installation of Unsloth include CUDA not being installed correctly, or software versions not being compatible with each other. You can troubleshoot these issues by checking the PyTorch Compatibility Matrix and running diagnostic commands like nvcc and python -m xformers.info.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What are the supported CUDA versions for PyTorch?\",\n",
      "      \"answer\": \"The supported CUDA versions are 11.8 and 12.1.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How do I install PyTorch with CUDA support using Conda?\",\n",
      "      \"answer\": \"Use the command 'conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y' to install it.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the recommended way to activate the Conda environment?\",\n",
      "      \"answer\": \"Activate the Conda environment by running 'conda activate unsloth_env'.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Can I use Pip if I don't have Conda? How do I install PyTorch using Pip?\",\n",
      "      \"answer\": \"Yes, you can use Pip. Install PyTorch using the command 'pip install' without specifying a CUDA version.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How do I create a new Linux environment for installing Conda if it doesn't exist already?\",\n",
      "      \"answer\": \"Create a new directory for Miniconda and run the commands to install Conda in a Linux environment as described in the documentation link provided.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"How do you install Miniconda on a Linux system?\",\n",
      "            \"answer\": \"You can download the installation script using wget, then run it with bash to begin the installation process.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the purpose of the rm -rf command in this installation script?\",\n",
      "            \"answer\": \"The rm -rf command removes temporary files created during the Miniconda installation process, ensuring a clean environment.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How do you activate the conda environment after installing Miniconda?\",\n",
      "            \"answer\": \"You can activate the conda environment using the conda init bash/zsh command to ensure conda is properly integrated into your shell configuration.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are the steps involved in setting up a new conda environment for package management?\",\n",
      "            \"answer\": \"To set up a new conda environment, you can use the conda create --name [env_name] command to specify an environment name, then activate it with conda activate [env_name].\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How do you handle errors or issues during the Miniconda installation process?\",\n",
      "            \"answer\": \"If you encounter any issues during installation, refer to the error messages for troubleshooting guidance. Common problems can often be resolved by adjusting your system configuration or reinstalling Miniconda.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the purpose of using Pip instead of Conda?\",\n",
      "      \"answer\": \"Pip is used as it is a bit more complex due to dependency issues.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Why is Pip more suitable for this scenario than Conda?\",\n",
      "      \"answer\": \"Pip is more appropriate in this case because of its ability to handle dependency issues more effectively.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are the specific differences between using Pip with torch 2.2, 2.3, 2.4, or 2.5 and CUDA versions?\",\n",
      "      \"answer\": \"The pip command differs for torch 2.2, 2.3, 2.4, and 2.5, as well as for different CUDA versions.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are the specific dependency issues that Pip is designed to handle?\",\n",
      "      \"answer\": \"Pip handles dependency issues that arise due to its complexity.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Why should users be aware of the pip command variations based on torch and CUDA versions?\",\n",
      "      \"answer\": \"The variations in the pip command are necessary to accommodate different versions of torch and CUDA, ensuring proper installation.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What are the supported versions of PyTorch and CUDA for installing unsloth?\",\n",
      "      \"answer\": \"The supported versions include torch211, torch212, torch220, torch230, torch240, cu118, cu121, cu124, cu118-ampere, cu121-ampere, and cu124-ampere.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How do I install unsloth on my system with PyTorch 2.4 and CUDA 12.1?\",\n",
      "      \"answer\": \"You can use pip install --upgrade pip followed by pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the command to upgrade pip and then install unsloth for PyTorch 2.5 and CUDA 12.4?\",\n",
      "      \"answer\": \"The command is pip install --upgrade pip followed by pip install 'unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git'.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Can you provide an example of installing unsloth for Ampere devices with PyTorch 2.4 and CUDA 12.1?\",\n",
      "      \"answer\": \"You can use pip install 'unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git'.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are the general steps to install unsloth, regardless of the PyTorch and CUDA versions?\",\n",
      "      \"answer\": \"The general steps involve upgrading pip and then installing unsloth using a specific version string that combines your PyTorch and CUDA versions.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"How to install the unsloth package with specific CUDA and PyTorch versions?\",\n",
      "      \"answer\": \"You can install the unsloth package using pip, specifying the desired CUDA and PyTorch versions. For example: `pip install 'unsloth[cu118-ampere-torch240]' @ git+https://github.com/unslothai/unsloth.git`.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are the different installation options for unsloth package?\",\n",
      "      \"answer\": \"The unsloth package can be installed with various CUDA and PyTorch versions, including cu118-ampere-torch240, cu121-torch240, cu118-torch240, cu121-torch230, cu121-ampere-torch230, and cu121-torch250. You can install the desired version using pip.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What command can be used to install the 'unsloth' package with the 'cu124-ampere-torch250' extra?\",\n",
      "            \"answer\": \"'pip install unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git'\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How can you obtain the optimal pip installation command for 'unsloth' in a terminal?\",\n",
      "            \"answer\": \"Use the command: wget -qO https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the condition under which an ImportError will be raised when trying to import 'torch' in a Python REPL?\",\n",
      "            \"answer\": \"When CUDA version is not 12.1, 11.8, or 12.4 and torch.__version__ does not match the CUDA version.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are some common ways to install the required packages for running the code, including 'torch'?\",\n",
      "            \"answer\": \"You can either run a specific command like 'pip install unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git' or manually install 'torch' via 'pip install torch'.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What information is crucial to determine the CUDA version for a successful import of 'torch'?\",\n",
      "            \"answer\": \"The CUDA version and the version of 'torch' must match, which can be determined by checking 'torch.version.cuda' and comparing it with supported versions.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is causing the error in this code snippet?\",\n",
      "      \"answer\": \"The error is caused by a compatibility issue between CUDA and Torch versions.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Can you explain what's happening with the version checks in the RuntimeError?\",\n",
      "      \"answer\": \"The code checks various Torch versions and their corresponding CUDA version prefixes to determine if the installation is compatible.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are some possible solutions for resolving this compatibility issue?\",\n",
      "      \"answer\": \"Upgrading or downgrading either CUDA or Torch can resolve the issue, depending on the specific versions in use.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How does the error handling mechanism in this code snippet work?\",\n",
      "      \"answer\": \"The error handling mechanism raises a RuntimeError with a custom message that includes information about the incompatible version and its prefix.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Can you summarize the key differences between Torch versions v2.1.0 to v2.6.0, as mentioned in the code snippet?\",\n",
      "      \"answer\": \"The code snippet outlines the CUDA prefix changes for various Torch versions from v2.1.0 up to and excluding v2.6.0.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the purpose of installing the unsloth package?\",\n",
      "      \"answer\": \"The unsloth package is used for parallelizing and scaling up experiments using multiple GPUs.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How does the provided code upgrade pip and install the unsloth package with a specific version?\",\n",
      "      \"answer\": \"The code upgrades pip, then installs the unsloth package from a specific GitHub repository using git+https protocol.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the primary purpose of this data chunk?\",\n",
      "      \"answer\": \"This data chunk serves as a guide for fine-tuning a language model.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How can one save to GGUF and checkpoint their work according to the documentation provided?\",\n",
      "      \"answer\": \"To save to GGUF, follow the instructions in our official Documentation. Additionally, learn about checkpointing, evaluation, and more by referring to the same documentation.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Are there any specific trainers or libraries supported for model training, and what are they?\",\n",
      "      \"answer\": \"Yes, we support Hugging Face's TRL (Trainer), Trainer, Seq2SeqTrainer, as well as Pytorch code. These can be used to suit different needs and environments.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How do I access models and datasets from the ModelScope community?\",\n",
      "      \"answer\": \"To download models and datasets from the ModelScope community, set the environment variable UNSLOTH_USE_MODELSCOPE=1 and install the modelscope library by running pip install modelscope -U. You can then use the model and dataset id in the ModelScope community.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What additional resources are available for learning more about SFT and DPO documents?\",\n",
      "      \"answer\": \"For detailed information on SFT and DPO, please refer to Hugging Face's official docs. This will provide a comprehensive understanding of these topics.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the primary purpose of this dataset?\",\n",
      "            \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How was the LAION dataset obtained?\",\n",
      "            \"answer\": \"The LAION dataset was retrieved from the Hugging Face datasets repository, specifically from the 'laion/OIG' package.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the maximum sequence length supported by this model?\",\n",
      "            \"answer\": \"The maximum sequence length supported by this model is 2048.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Are there any specific models that support 4-bit quantization for faster downloading and reduced memory usage?\",\n",
      "            \"answer\": \"Yes, four-bit pre-quantized models are supported for faster downloading and no out-of-memory (OOM) errors. Some examples include 'unsloth/Meta-Llama-3.1-8B-bnb-4bit', 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit', and 'unsloth/Meta-Llama-3.1-70B-bnb-4bit'.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does the LAION dataset facilitate language model fine-tuning?\",\n",
      "            \"answer\": \"The LAION dataset serves as training data for fine-tuning a language model, allowing it to learn from diverse perspectives and generate accurate responses.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the name of this dataset?\",\n",
      "            \"answer\": \"This dataset is called Meta-Llama-3.1-405B-bnb-4bit.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does this model compare to previous versions in terms of performance?\",\n",
      "            \"answer\": \"Meta-Llama-3.1-405B-bnb-4bit has been optimized for improved efficiency, making it 2x faster than earlier models like Mistral-Small-Instruct-2409.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "   {\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the name of the new LLaMA model?\",\n",
      "            \"answer\": \"The new LLaMA model is named unsloth/Llama-3.3-70B-Instruct-bnb-4bit.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does this new model differ from its predecessor?\",\n",
      "            \"answer\": \"This new model, unsloth/Llama-3.3-70B-Instruct-bnb-4bit, has an increased scale of 70 billion parameters compared to the previous LLaMA model.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are some key features of the updated LLaMA model?\",\n",
      "            \"answer\": \"The unsloth/Llama-3.3-70B-Instruct-bnb-4bit model is an instruct-based variant, indicating it has been fine-tuned for specific instruction-following tasks.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does this model relate to the Instruct benchmark?\",\n",
      "            \"answer\": \"This updated LLaMA model has been fine-tuned on the Instruct benchmark, a key aspect of its development and training data.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are some potential applications or use cases for this new LLaMA model?\",\n",
      "            \"answer\": \"The unsloth/Llama-3.3-70B-Instruct-bnb-4bit model can be used in various applications requiring specific instruction-following capabilities, such as chatbots or task-oriented dialogue systems.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the primary purpose of this dataset?\",\n",
      "            \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does the model handle long context with max_seq_length set to 2048?\",\n",
      "            \"answer\": \"The model can handle long contexts due to its ability to process sequences up to 2048 tokens.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the impact of using 4-bit quantization on memory load in this setup?\",\n",
      "            \"answer\": \"Using 4-bit quantization reduces the memory load, making it suitable for this application.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Are there any specific modules that are targeted for LoRA weight adaptation in this configuration?\",\n",
      "            \"answer\": \"The q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj modules are targeted for LoRA weight adaptation.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does the full_finetuning option affect the training process of the model?\",\n",
      "            \"answer\": \"Full finetuning is now supported, allowing for more flexible training options.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the purpose of the SFTTrainer class?\",\n",
      "            \"answer\": \"The SFTTrainer class is used to train a model on a dataset.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are the key hyperparameters for training the model?\",\n",
      "            \"answer\": \"The key hyperparameters include max_seq_length, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, and max_steps.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does the SFTTrainer handle random state during training?\",\n",
      "            \"answer\": \"Random state is handled using a seed value of 3407.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are the supported optimization algorithms for this model?\",\n",
      "            \"answer\": \"The supported optimization algorithm is AdamW with 8-bit precision.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does the SFTTrainer optimize memory usage during training?\",\n",
      "            \"answer\": \"It uses a technique called unsloth, which reduces VRAM usage by 30% and allows for larger batch sizes.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What are some advanced tips for fine-tuning a language model with Unsloth?\",\n",
      "      \"answer\": \"Tips like saving to GGUF, merging to 16bit for vLLM, continued training from a saved LoRA adapter, adding an evaluation loop / OOMs, and customized chat templates can be found on the Unsloth wiki.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What reinforcement learning algorithms work with Unsloth?\",\n",
      "      \"answer\": \"DPO, GRPO, PPO, Reward Modelling, and Online DPO are some of the reinforcement learning algorithms that work with Unsloth.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Where can I find more information about Reinforcement Learning with Unsloth?\",\n",
      "      \"answer\": \"You can find more information in Hugging Face's official docs, as well as on the GRPO and DPO docs pages.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What are some examples of reinforcement learning notebooks that work with Unsloth?\",\n",
      "      \"answer\": \"Advanced Qwen3 GRPO notebook, DPO Zephyr notebook, ORPO notebook, KTO notebook, and SimPO notebook are some examples of reinforcement learning notebooks that work with Unsloth.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How can I access the code for a specific reinforcement learning algorithm?\",\n",
      "      \"answer\": \"You can click on the link provided to access the code for DPO or other algorithms.\"\n",
      "    }\n",
      "  ]\n",
      "}{\"generated\": []}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the purpose of using 'unsloth' in gradient checkpointing?\",\n",
      "            \"answer\": \"'Unsloth' uses 30% less VRAM and allows for 2x larger batch sizes.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does setting use_gradient_checkpointing to 'unsloth' affect model performance?\",\n",
      "            \"answer\": \"Setting use_gradient_checkpointing to 'unsloth' improves model performance by optimizing memory usage and increasing batch size capabilities.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is the effect of changing random_state to 3407 in this code snippet?\",\n",
      "            \"answer\": \"Changing random_state to 3407 sets a specific seed for reproducibility purposes.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Why is it necessary to set max_seq_length when creating the DPOTrainer object?\",\n",
      "            \"answer\": \"Setting max_seq_length ensures that the model can process sequences of a certain maximum length, preventing errors during training.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What are the key advantages of using the 'adamw_8bit' optimizer in this code snippet?\",\n",
      "            \"answer\": \"'AdamW 8-bit' is an optimized version of AdamW that uses less memory and improves performance on large-scale tasks.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the primary purpose of this dataset?\",\n",
      "      \"answer\": \"This dataset serves as performance benchmarking data for various language models.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Which specific benchmarks were used to evaluate the performance of Llama 3.3?\",\n",
      "      \"answer\": \"The detailed benchmarks, including VRAM reduction and longer context capabilities, are mentioned in our Llama 3.3 Blog.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is QLoRA, and how was it applied to the linear layers of the language model?\",\n",
      "      \"answer\": \"QLoRA was applied to all linear layers (q, k, v, o, gate, up, down) of Llama 3.3.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Can you describe the key differences in performance between Llama 3.3 and Llama 3.1?\",\n",
      "      \"answer\": \"Llama 3.3 showed improvements over Llama 3.1 in VRAM reduction (>75% vs >70%) and longer context capabilities (13x longer vs 12x longer).\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What were the results of the Hugging Face + FA2 experiment with Llama 3.3?\",\n",
      "      \"answer\": \"The Hugging Face + FA2 experiment resulted in a performance equal to that of Llama 3.1, indicating no significant improvement.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the maximum context length supported by Llama 3.1 (8B)?\",\n",
      "            \"answer\": \"The maximum context length supported by Llama 3.1 (8B) is not explicitly mentioned, but we tested it with sequences up to a certain length.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What were the batch sizes used in testing Llama 3.1 (8B)?\",\n",
      "            \"answer\": \"We tested Llama 3.1 (8B) at a batch size of 1.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How did you modify the linear layers in Llama 3.1 (8B) during testing?\",\n",
      "            \"answer\": \"We applied QLoRA on all linear layers with rank = 32.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What was the outcome when testing Llama 3.1 (8B) on a GPU with 8 GB of VRAM?\",\n",
      "            \"answer\": \"We encountered an Out-of-Memory (OOM) error.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How did the maximum sequence length affect the performance of Llama 3.1 (8B)?\",\n",
      "            \"answer\": \"Padding all sequences to a certain maximum sequence length helped mimic long context fine-tuning workloads.\"\n",
      "        }\n",
      "    ]\n",
      "}{\n",
      "    \"generated\": [\n",
      "        {\n",
      "            \"question\": \"What is the maximum context length for Llama 3.3 (70B) in this experiment?\",\n",
      "            \"answer\": \"The maximum context length was set to mimic long context finetuning workloads.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How does padding sequences affect the performance of the model?\",\n",
      "            \"answer\": \"Padding all sequences to a certain maximum sequence length is done to mimic long context finetuning workloads.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "       {\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What is the primary purpose of this dataset?\",\n",
      "      \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Who are the authors behind Unsloth?\",\n",
      "      \"answer\": \"Unsloth is maintained by Daniel Han, Michael Han, and other contributors from the Unsloth team.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What repository can be used to access the full codebase of Unsloth?\",\n",
      "      \"answer\": \"The Unsloth codebase is available on GitHub at http://github.com/unslothai/unsloth.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How can I cite the Unsloth repo in my research paper?\",\n",
      "      \"answer\": \"You can cite the Unsloth repo using the following citation: @software{unsloth, author = {Daniel Han, Michael Han and Unsloth team}, title = {Unsloth}, url = {http://github.com/unslothai/unsloth}, year = {2023} }. \"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Are there any specific guidelines or standards for contributing to the Unsloth project?\",\n",
      "      \"answer\": \"No, there are no strict guidelines for contributions. You can contribute by submitting issues or pull requests on GitHub.\"\n",
      "    }\n",
      "  ]\n",
      "}{\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"question\": \"What libraries are utilized in the development of Unsloth?\",\n",
      "      \"answer\": \"The llama.cpp library, Hugging Face's TRL library, and Apple's ML Cross Entropy are employed.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Who have been instrumental in contributing to Unsloth, either directly or through their usage of the tool?\",\n",
      "      \"answer\": \"Several individuals have contributed to Unsloth, including Erik for his work on adding Apple's ML Cross Entropy and Etherl for incorporating support for TTS, diffusion, and BERT models.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the significance of the acknowledgement given to Hugging Face and their TRL library in relation to Unsloth?\",\n",
      "      \"answer\": \"The mention acknowledges the team's contribution to Unsloth through their library, which has facilitated certain functionalities within the tool.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How does Unsloth utilize its various features, such as model saving with Unsloth, TTS, diffusion, and BERT models?\",\n",
      "      \"answer\": \"These capabilities are made possible by the integration of different libraries, including llama.cpp and Hugging Face's TRL library.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Who is credited with adding support for model saving with Unsloth to the tool, according to the provided information?\",\n",
      "      \"answer\": \"The contribution of adding this feature is attributed to the llama.cpp library.\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List \n",
    "from pydantic import BaseModel\n",
    "from litellm import completion\n",
    "\n",
    "def prompt_template(data: str, num_records: int = 5):\n",
    "\n",
    "    return f\"\"\"You are an expert data curator assisting a machine learning engineer in creating a high-quality instruction tuning dataset. Your task is to transform \n",
    "    the provided data chunk into diverse question and answer (Q&A) pairs that will be used to fine-tune a language model. \n",
    "\n",
    "    For each of the {num_records} entries, generate one or two well-structured questions that reflect different aspects of the information in the chunk. \n",
    "    Ensure a mix of longer and shorter questions, with shorter ones typically containing 1-2 sentences and longer ones spanning up to 3-4 sentences. Each \n",
    "    Q&A pair should be concise yet informative, capturing key insights from the data.\n",
    "\n",
    "    Structure your output in JSON format, where each object contains 'question' and 'answer' fields. The JSON structure should look like this:\n",
    "\n",
    "        \"question\": \"Your question here...\",\n",
    "        \"answer\": \"Your answer here...\"\n",
    "\n",
    "    Focus on creating clear, relevant, and varied questions that encourage the model to learn from diverse perspectives. Avoid any sensitive or biased \n",
    "    content, ensuring answers are accurate and neutral.\n",
    "\n",
    "    Example:\n",
    "    \n",
    "        \"question\": \"What is the primary purpose of this dataset?\",\n",
    "        \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
    "    \n",
    "\n",
    "    By following these guidelines, you'll contribute to a robust and effective dataset that enhances the model's performance.\"\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Explanation:**\n",
    "\n",
    "    - **Clarity and Specificity:** The revised prompt clearly defines the role of the assistant and the importance of the task, ensuring alignment with the \n",
    "    project goals.\n",
    "    - **Quality Standards:** It emphasizes the need for well-formulated Q&A pairs, specifying the structure and content of each question and answer.\n",
    "    - **Output Format:** An example JSON structure is provided to guide the format accurately.\n",
    "    - **Constraints and Biases:** A note on avoiding sensitive or biased content ensures ethical considerations are met.\n",
    "    - **Step-by-Step Guidance:** The prompt breaks down the task into manageable steps, making it easier for the assistant to follow.\n",
    "\n",
    "    This approach ensures that the generated data is both high-quality and meets the specific requirements of the machine learning project.\n",
    "    \n",
    "    Data\n",
    "    {data}\n",
    "    \"\"\"\n",
    "\n",
    "class Record(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class Response(BaseModel):\n",
    "    generated: List[Record]\n",
    "\n",
    "def llm_call(data: str, num_records: int = 5) -> dict:\n",
    "    stream = completion(\n",
    "        model=\"ollama_chat/llama3.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_template(data, num_records),\n",
    "            }\n",
    "        ],\n",
    "        stream=True,\n",
    "        options={\"num_predict\": 2000},\n",
    "        format=Response.model_json_schema(),\n",
    "    )\n",
    "    data = \"\"\n",
    "    for x in stream: \n",
    "        delta = x['choices'][0][\"delta\"][\"content\"]\n",
    "        if delta is not None: \n",
    "            print(delta, end=\"\") \n",
    "            data += delta \n",
    "    return json.loads(data)\n",
    "\n",
    "dataset = {}\n",
    "for i, chunk in enumerate(contextualized_chunks):\n",
    "    data = llm_call(chunk)\n",
    "    dataset[i] = {\"generated\":data[\"generated\"], \"context\":chunk}\n",
    "\n",
    "with open('unsloth_data.json','w') as f: \n",
    "    json.dump(dataset, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d22cc0-5fb8-4284-be67-5a79c4d5d7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
