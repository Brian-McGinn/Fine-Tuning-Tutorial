{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4edda7-6dd1-40b3-9a5d-abb11941d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e038b7-66dd-4ce4-8779-281f8a69a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm\n",
    " # curl https://ollama.ai/install.sh | sh\n",
    " # ollama serve & \n",
    " # ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36078ad3-348c-4164-8483-955966a4f020",
   "metadata": {},
   "source": [
    "Initial query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8361ecb9-8a5c-446b-990d-9f70ebdc04ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1', 'created_at': '2025-07-06T23:54:24.498315152Z', 'message': {'role': 'assistant', 'content': 'There is no such thing as \"Unsloth.ai\" news. It appears to be a made-up or fictional term.\\n\\nIf you\\'re looking for information on AI, technology, or industry-related news, I\\'d be happy to provide updates and insights from reputable sources like:\\n\\n* Google\\'s AI research\\n* Meta AI (formerly Facebook AI)\\n* Microsoft AI\\n* OpenAI\\n* Research papers and publications in the field of artificial intelligence\\n\\nLet me know if there\\'s something specific you\\'re interested in, and I\\'ll do my best to help!'}, 'done_reason': 'stop', 'done': True, 'total_duration': 6440074865, 'load_duration': 5076463525, 'prompt_eval_count': 18, 'prompt_eval_duration': 208438417, 'eval_count': 112, 'eval_duration': 1153440250}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"}],\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, data=json.dumps(payload))\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600a5a9-705c-42a0-9141-40c1e7039346",
   "metadata": {},
   "source": [
    "Now let's try the same command with Python.\n",
    "\n",
    "First we need to install the Ollama library for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac3c5ce-6258-4930-9b3a-0956b39620f1",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Collecting ollama\n",
      "  Using cached ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in ./venv/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Using cached ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, ollama\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.5.1 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbce5ce-0222-4cd8-a28a-9e63ac989e98",
   "metadata": {},
   "source": [
    "Next let's create a simple python query using the Ollama library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7550214-582d-47bf-8f6d-5b6ab26cbb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find any information on \"Unsloth.ai\". It's possible that it's a fictional company, or it may be a very new and unknown entity. Can you provide more context or clarify what Unsloth.ai is? I'll do my best to help.\n",
      "\n",
      "However, if you're interested in news related to AI, machine learning, or technology, I'd be happy to share some general updates or trends with you!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48630fa6-a7ed-4137-8e42-442237ab81e2",
   "metadata": {},
   "source": [
    "As you can see unsloth.ai causes some hallucinations with llama3.1 and the results can be quite humorous. We will want to fix this by fine tuning the model and give it some information about the Unsloth project. \n",
    "\n",
    "The first step in doing this is to convert the provided unsloth_documentation.pdf into chunks using the PyPDFLoader library from LangChain. To use LangChain we will need to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c5161b-b737-49f2-8d30-51c0ac9b81cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pypdf\n",
      "  Using cached pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.4.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./venv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Using cached aiohttp-3.12.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Using cached httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting numpy>=1.26.2 (from langchain_community)\n",
      "  Using cached numpy-2.3.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Using cached langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "Using cached pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
      "Using cached aiohttp-3.12.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.4.4-py3-none-any.whl (367 kB)\n",
      "Using cached numpy-2.3.1-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Using cached sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Using cached greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Using cached orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Using cached zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: zstandard, tenacity, python-dotenv, pypdf, propcache, packaging, orjson, numpy, mypy-extensions, multidict, jsonpatch, httpx-sse, greenlet, frozenlist, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, marshmallow, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.4.0 dataclasses-json-0.6.7 frozenlist-1.7.0 greenlet-3.2.3 httpx-sse-0.4.1 jsonpatch-1.33 langchain-0.3.26 langchain-core-0.3.68 langchain-text-splitters-0.3.8 langchain_community-0.3.27 langsmith-0.4.4 marshmallow-3.26.1 multidict-6.6.3 mypy-extensions-1.1.0 numpy-2.3.1 orjson-3.10.18 packaging-24.2 propcache-0.3.2 pydantic-settings-2.10.1 pypdf-5.7.0 python-dotenv-1.1.1 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0 yarl-1.20.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community pypdf\n",
    "!wget https://raw.githubusercontent.com/Brian-McGinn/Fine-Tuning-Tutorial/main/unsloth_documentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f75d4-0d62-4c7d-9f9e-52a151830892",
   "metadata": {},
   "source": [
    "With everything installed we can split our PDF into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b69cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Finetune Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral 2x faster with\n",
      "80% less VRAM!\n",
      "Finetune for Free\n",
      "Notebooks are beginner friendly. Read our guide. Add your dataset, click “Run\n",
      "All”, and export your finetuned model to GGUF, Ollama, vLLM or Hugging\n",
      "Face.\n",
      "Unsloth supports Free Notebooks Performance Memory use\n",
      "Gemma 3n (4B) Start for free 1.5x faster 50% less\n",
      "Qwen3 (14B) Start for free 2x faster 70% less\n",
      "Qwen3 (4B):\n",
      "GRPO\n",
      "Start for free 2x faster 80% less\n",
      "Gemma 3 (4B) Start for free 1.6x faster 60% less\n",
      "Llama 3.2 (3B) Start for free 2x faster 70% less\n",
      "Phi-4 (14B) Start for free 2x faster 70% less\n",
      "Llama 3.2 Vision\n",
      "(11B)\n",
      "Start for free 2x faster 50% less\n",
      "Llama 3.1 (8B) Start for free 2x faster 70% less\n",
      "Mistral v0.3 (7B) Start for free 2.2x faster 75% less\n",
      "Orpheus-TTS\n",
      "(3B)\n",
      "Start for free 1.5x faster 50% less\n",
      "• See all our notebooks for: Kaggle, GRPO,TTS & Vision\n",
      "• See all our models and all our notebooks\n",
      "• See detailed documentation for Unsloth here\n",
      "Quickstart' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX via pandoc', 'creationdate': '2025-07-03T16:37:52-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-03T16:37:52-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023/Debian) kpathsea version 6.3.5', 'source': 'unsloth_documentation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"unsloth_documentation.pdf\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, \n",
    "            chunk_overlap=50\n",
    "        )\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(split_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32283d35",
   "metadata": {},
   "source": [
    "Now that we have the PDF chunks we can use ollama to generate data for our fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f4b259d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated': [{'question': \"What are the benefits of using Unsloth's Notebooks for fine-tuning language models?\", 'answer': \"Unsloth's Notebooks offer a faster and more memory-efficient way to finetune language models, with some models being 2x faster and requiring up to 80% less VRAM.\"}, {'question': 'Which Unsloth Notebooks support free usage for beginners?', 'answer': \"Unsloth's Notebooks are beginner-friendly, and users can start using them for free, allowing them to finetune their models without any additional cost.\"}, {'question': 'What is the difference in performance between Gemma 3n and Qwen3 when it comes to fine-tuning language models?', 'answer': 'Gemma 3n offers 1.5x faster performance compared to Qwen3, while also requiring 50% less VRAM.'}, {'question': \"Can I export my finetuned model to various platforms using Unsloth's Notebooks?\", 'answer': \"Yes, users can export their finetuned models to GGUF, Ollama, vLLM, or Hugging Face using Unsloth's Notebooks.\"}, {'question': 'Where can I find detailed documentation for Unsloth and its Notebooks?', 'answer': 'Detailed documentation for Unsloth is available here, providing users with a comprehensive guide to using the platform and its features.'}]}\n",
      "{'question': \"What are the benefits of using Unsloth's Notebooks for fine-tuning language models?\", 'answer': \"Unsloth's Notebooks offer a faster and more memory-efficient way to finetune language models, with some models being 2x faster and requiring up to 80% less VRAM.\"}\n",
      "{'question': 'Which Unsloth Notebooks support free usage for beginners?', 'answer': \"Unsloth's Notebooks are beginner-friendly, and users can start using them for free, allowing them to finetune their models without any additional cost.\"}\n",
      "{'question': 'What is the difference in performance between Gemma 3n and Qwen3 when it comes to fine-tuning language models?', 'answer': 'Gemma 3n offers 1.5x faster performance compared to Qwen3, while also requiring 50% less VRAM.'}\n",
      "{'question': \"Can I export my finetuned model to various platforms using Unsloth's Notebooks?\", 'answer': \"Yes, users can export their finetuned models to GGUF, Ollama, vLLM, or Hugging Face using Unsloth's Notebooks.\"}\n",
      "{'question': 'Where can I find detailed documentation for Unsloth and its Notebooks?', 'answer': 'Detailed documentation for Unsloth is available here, providing users with a comprehensive guide to using the platform and its features.'}\n",
      "{'generated': [{'question': 'How do I install Unsloth on a Linux device?', 'answer': 'You can install Unsloth on a Linux device using pip with the command `pip install unsloth`.'}, {'question': 'What are some of the new features supported by Unsloth, and which specific models are included?', 'answer': 'Unsloth now supports Text-to-Speech (TTS), including sesame/csm-1b and STT openai/whisper-large-v3. Additionally, Qwen3 is supported, with Qwen3-30B-A3B fitting on 17.5GB VRAM.'}, {'question': 'Can you provide more information about the Dynamic 2.0 quants in Unsloth and their impact?', 'answer': \"Unsloth's Dynamic 2.0 quants have set new benchmarks for 5-shot MMLU & KL Divergence, indicating improved performance and efficiency.\"}, {'question': 'What are the supported Llama models in Unsloth, and how do they contribute to its capabilities?', 'answer': 'Unsloth now supports Llama 4 by Meta, including Scout & Maverick, which further enhance its language understanding and generation abilities.'}, {'question': 'How does Unsloth handle updates and new features, ensuring users have access to the latest advancements?', 'answer': \"The provided installation instructions for Windows can be found via a link within the documentation. Regular updates and new feature introductions are available as part of Unsloth's development and improvement process.\"}]}\n",
      "{'question': 'How do I install Unsloth on a Linux device?', 'answer': 'You can install Unsloth on a Linux device using pip with the command `pip install unsloth`.'}\n",
      "{'question': 'What are some of the new features supported by Unsloth, and which specific models are included?', 'answer': 'Unsloth now supports Text-to-Speech (TTS), including sesame/csm-1b and STT openai/whisper-large-v3. Additionally, Qwen3 is supported, with Qwen3-30B-A3B fitting on 17.5GB VRAM.'}\n",
      "{'question': 'Can you provide more information about the Dynamic 2.0 quants in Unsloth and their impact?', 'answer': \"Unsloth's Dynamic 2.0 quants have set new benchmarks for 5-shot MMLU & KL Divergence, indicating improved performance and efficiency.\"}\n",
      "{'question': 'What are the supported Llama models in Unsloth, and how do they contribute to its capabilities?', 'answer': 'Unsloth now supports Llama 4 by Meta, including Scout & Maverick, which further enhance its language understanding and generation abilities.'}\n",
      "{'question': 'How does Unsloth handle updates and new features, ensuring users have access to the latest advancements?', 'answer': \"The provided installation instructions for Windows can be found via a link within the documentation. Regular updates and new feature introductions are available as part of Unsloth's development and improvement process.\"}\n",
      "{'generated': []}\n",
      "{'generated': [{'question': 'What is the primary purpose of Unsloth in relation to transformer-style models?', 'answer': 'Unsloth enables support for full-finetuning, pretraining, 4b-bit, 16-bit and 8-bit training, as well as supporting all transformer-style models including TTS, STT, multimodal, diffusion, BERT and more.'}, {'question': 'How does Unsloth improve memory usage in comparison to native transformer models?', 'answer': 'We cut memory usage by a further 30% with Unsloth, allowing for support of 4x longer context windows compared to its native counterpart.'}, {'question': 'What is the significance of the gradient accumulation bug fix in Unsloth and transformers?', 'answer': 'The gradient accumulation bug was found and fixed; users are advised to update Unsloth and transformers accordingly to ensure optimal performance.'}, {'question': \"How does Unsloth's context support compare to that of Llama 3.1 (8B) and other models?\", 'answer': \"Unsloth enables 342K context, surpassing the native 128K support of Llama 3.1 (8B), and is 13x longer than HF+FA2's context.\"}, {'question': \"What programming languages are used in Unsloth's kernel implementation?\", 'answer': 'All kernels written in OpenAI’s Triton language, which includes a manual backprop engine for added efficiency.'}]}\n",
      "{'question': 'What is the primary purpose of Unsloth in relation to transformer-style models?', 'answer': 'Unsloth enables support for full-finetuning, pretraining, 4b-bit, 16-bit and 8-bit training, as well as supporting all transformer-style models including TTS, STT, multimodal, diffusion, BERT and more.'}\n",
      "{'question': 'How does Unsloth improve memory usage in comparison to native transformer models?', 'answer': 'We cut memory usage by a further 30% with Unsloth, allowing for support of 4x longer context windows compared to its native counterpart.'}\n",
      "{'question': 'What is the significance of the gradient accumulation bug fix in Unsloth and transformers?', 'answer': 'The gradient accumulation bug was found and fixed; users are advised to update Unsloth and transformers accordingly to ensure optimal performance.'}\n",
      "{'question': \"How does Unsloth's context support compare to that of Llama 3.1 (8B) and other models?\", 'answer': \"Unsloth enables 342K context, surpassing the native 128K support of Llama 3.1 (8B), and is 13x longer than HF+FA2's context.\"}\n",
      "{'question': \"What programming languages are used in Unsloth's kernel implementation?\", 'answer': 'All kernels written in OpenAI’s Triton language, which includes a manual backprop engine for added efficiency.'}\n",
      "{'generated': [{'question': 'What are the system requirements for running Unsloth?', 'answer': 'Unsloth supports NVIDIA GPUs with CUDA Capability 7.0 or higher, and works on both Linux and Windows.'}, {'question': 'Is there a specific version of Python required to use Unsloth?', 'answer': 'No, but you should avoid using Python 3.13 as it is not supported by Unsloth. Instead, use versions like 3.12, 3.11, or 3.10.'}, {'question': 'How do I install the necessary drivers for my NVIDIA GPU?', 'answer': \"You should download and install the latest version of your GPU's driver from the NVIDIA website.\"}, {'question': 'What are some additional steps required for setting up Unsloth on Windows?', 'answer': 'On Windows, you need to install Visual Studio with C++ metadata in addition to installing the NVIDIA Video Driver.'}, {'question': 'How do I update Unsloth and its dependencies using pip?', 'answer': \"You can update Unsloth by running 'pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo'.\"}]}\n",
      "{'question': 'What are the system requirements for running Unsloth?', 'answer': 'Unsloth supports NVIDIA GPUs with CUDA Capability 7.0 or higher, and works on both Linux and Windows.'}\n",
      "{'question': 'Is there a specific version of Python required to use Unsloth?', 'answer': 'No, but you should avoid using Python 3.13 as it is not supported by Unsloth. Instead, use versions like 3.12, 3.11, or 3.10.'}\n",
      "{'question': 'How do I install the necessary drivers for my NVIDIA GPU?', 'answer': \"You should download and install the latest version of your GPU's driver from the NVIDIA website.\"}\n",
      "{'question': 'What are some additional steps required for setting up Unsloth on Windows?', 'answer': 'On Windows, you need to install Visual Studio with C++ metadata in addition to installing the NVIDIA Video Driver.'}\n",
      "{'question': 'How do I update Unsloth and its dependencies using pip?', 'answer': \"You can update Unsloth by running 'pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo'.\"}\n",
      "{'generated': [{'question': 'What C++ options should be selected during the installation process in Visual Studio?', 'answer': 'Select all of the C++ options and also select options for Windows 10/11 SDK.'}, {'question': 'How can I ensure that I have installed the correct version of PyTorch compatible with my CUDA drivers?', 'answer': 'Make sure to select the correct version of PyTorch that is compatible with your CUDA drivers.'}, {'question': 'What are the prerequisites for running Unsloth directly on Windows?', 'answer': 'Install Triton from a specific Windows fork and ensure you have PyTorch >= 2.4 and CUDA 12 installed.'}, {'question': 'How can I troubleshoot a crashing issue in SFTTrainer when using Unsloth?', 'answer': 'Set dataset_num_proc=1 to avoid the crashing issue by adding this parameter to the trainer initialization: trainer = SFTTrainer(dataset_num_proc=1,...'}, {'question': 'What are the necessary steps to install CUDA Toolkit according to the instructions provided?', 'answer': 'Follow the instructions to install CUDA Toolkit.'}]}\n",
      "{'question': 'What C++ options should be selected during the installation process in Visual Studio?', 'answer': 'Select all of the C++ options and also select options for Windows 10/11 SDK.'}\n",
      "{'question': 'How can I ensure that I have installed the correct version of PyTorch compatible with my CUDA drivers?', 'answer': 'Make sure to select the correct version of PyTorch that is compatible with your CUDA drivers.'}\n",
      "{'question': 'What are the prerequisites for running Unsloth directly on Windows?', 'answer': 'Install Triton from a specific Windows fork and ensure you have PyTorch >= 2.4 and CUDA 12 installed.'}\n",
      "{'question': 'How can I troubleshoot a crashing issue in SFTTrainer when using Unsloth?', 'answer': 'Set dataset_num_proc=1 to avoid the crashing issue by adding this parameter to the trainer initialization: trainer = SFTTrainer(dataset_num_proc=1,...'}\n",
      "{'question': 'What are the necessary steps to install CUDA Toolkit according to the instructions provided?', 'answer': 'Follow the instructions to install CUDA Toolkit.'}\n",
      "{'generated': [{'question': 'What are some common installation errors that may occur during the setup of this project?', 'answer': 'Some common installation errors include weird errors during installations, which can be resolved by following advanced installation instructions or troubleshooting steps.'}, {'question': 'How do I confirm if CUDA is installed correctly?', 'answer': 'You can try using nvcc to confirm if CUDA is installed correctly. If it fails, you may need to install cudatoolkit or CUDA drivers.'}, {'question': 'What are the steps for installing xformers manually?', 'answer': 'To install xformers manually, you can try installing vllm and see if it succeeds. Then, check if xformers succeeded with python -m xformers.info. Alternatively, you can use flash-attn for Ampere GPUs.'}, {'question': 'How do I ensure compatibility between different versions of Python, CUDA, CUDNN, torch, triton, and xformers?', 'answer': 'You can refer to the PyTorch Compatibility Matrix to check the compatibility between different versions. Double-check that your versions are compatible with each other.'}, {'question': 'What is an alternative installation method using Conda?', 'answer': \"If you have Conda installed, you can use it as an alternative to Pip for installation. However, if not, it's recommended to use Pip instead.\"}]}\n",
      "{'question': 'What are some common installation errors that may occur during the setup of this project?', 'answer': 'Some common installation errors include weird errors during installations, which can be resolved by following advanced installation instructions or troubleshooting steps.'}\n",
      "{'question': 'How do I confirm if CUDA is installed correctly?', 'answer': 'You can try using nvcc to confirm if CUDA is installed correctly. If it fails, you may need to install cudatoolkit or CUDA drivers.'}\n",
      "{'question': 'What are the steps for installing xformers manually?', 'answer': 'To install xformers manually, you can try installing vllm and see if it succeeds. Then, check if xformers succeeded with python -m xformers.info. Alternatively, you can use flash-attn for Ampere GPUs.'}\n",
      "{'question': 'How do I ensure compatibility between different versions of Python, CUDA, CUDNN, torch, triton, and xformers?', 'answer': 'You can refer to the PyTorch Compatibility Matrix to check the compatibility between different versions. Double-check that your versions are compatible with each other.'}\n",
      "{'question': 'What is an alternative installation method using Conda?', 'answer': \"If you have Conda installed, you can use it as an alternative to Pip for installation. However, if not, it's recommended to use Pip instead.\"}\n",
      "{'generated': [{'question': 'What are the supported versions of PyTorch and CUDA for this project?', 'answer': 'This project supports PyTorch versions 3.10, 3.11, and 3.12, as well as CUDA 11.8 and 12.1.'}, {'question': 'How can you create a new environment with the required packages using Conda?', 'answer': \"You can create a new environment named 'unsloth_env' using the command `conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y`.\"}, {'question': 'What is the alternative method for installing packages if you already have Conda installed?', 'answer': 'You can use pip, but be aware of potential dependency issues. The pip command for this project would be different than the one used for PyTorch 4.'}, {'question': 'How do you activate a Conda environment in Linux after installation?', 'answer': 'After installing Conda in Linux, you can activate it using the command `conda init bash` or `conda init zsh`, depending on your shell.'}, {'question': 'What is the recommended approach for installing packages if you have existing Conda environments?', 'answer': \"It's best not to use pip if you already have Conda installed. Instead, refer to the instructions for advanced pip installation, taking note of potential dependency issues.\"}]}\n",
      "{'question': 'What are the supported versions of PyTorch and CUDA for this project?', 'answer': 'This project supports PyTorch versions 3.10, 3.11, and 3.12, as well as CUDA 11.8 and 12.1.'}\n",
      "{'question': 'How can you create a new environment with the required packages using Conda?', 'answer': \"You can create a new environment named 'unsloth_env' using the command `conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y`.\"}\n",
      "{'question': 'What is the alternative method for installing packages if you already have Conda installed?', 'answer': 'You can use pip, but be aware of potential dependency issues. The pip command for this project would be different than the one used for PyTorch 4.'}\n",
      "{'question': 'How do you activate a Conda environment in Linux after installation?', 'answer': 'After installing Conda in Linux, you can activate it using the command `conda init bash` or `conda init zsh`, depending on your shell.'}\n",
      "{'question': 'What is the recommended approach for installing packages if you have existing Conda environments?', 'answer': \"It's best not to use pip if you already have Conda installed. Instead, refer to the instructions for advanced pip installation, taking note of potential dependency issues.\"}\n",
      "{'generated': [{'question': 'What versions of PyTorch are supported by this dataset?', 'answer': 'The dataset supports torch211, torch212, torch220, torch230, and torch240.'}, {'question': 'How do I install the unsloth library for a specific PyTorch and CUDA version?', 'answer': \"You can use pip to install the library with a command like `pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'`.\"}, {'question': 'What CUDA versions are supported for PyTorch 2.4?', 'answer': 'The supported CUDA versions for torch 2.4 are cu118 and cu121 and cu124.'}, {'question': 'How do I determine the correct installation command for unsloth based on my device type?', 'answer': 'For Ampere devices (A100, H100, RTX3090) and above, use cu118-ampere or cu121-ampere or cu124-ampere.'}, {'question': 'Can you provide an example installation command for unsloth with torch 2.5 and CUDA 12.4?', 'answer': \"An example installation command is `pip install --upgrade pip` followed by `pip install 'unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git'`.\"}]}\n",
      "{'question': 'What versions of PyTorch are supported by this dataset?', 'answer': 'The dataset supports torch211, torch212, torch220, torch230, and torch240.'}\n",
      "{'question': 'How do I install the unsloth library for a specific PyTorch and CUDA version?', 'answer': \"You can use pip to install the library with a command like `pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'`.\"}\n",
      "{'question': 'What CUDA versions are supported for PyTorch 2.4?', 'answer': 'The supported CUDA versions for torch 2.4 are cu118 and cu121 and cu124.'}\n",
      "{'question': 'How do I determine the correct installation command for unsloth based on my device type?', 'answer': 'For Ampere devices (A100, H100, RTX3090) and above, use cu118-ampere or cu121-ampere or cu124-ampere.'}\n",
      "{'question': 'Can you provide an example installation command for unsloth with torch 2.5 and CUDA 12.4?', 'answer': \"An example installation command is `pip install --upgrade pip` followed by `pip install 'unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git'`.\"}\n",
      "{'generated': [{'question': 'What is the optimal pip installation command for unsloth?', 'answer': 'The optimal pip installation command can be obtained by running `wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -` or manually in a Python REPL.'}, {'question': 'What are the specific CUDA versions supported by unsloth?', 'answer': 'Unsloth supports CUDA 12.1, 11.8, and 12.4.'}, {'question': 'Why is it necessary to have a minimum Torch version of 2.1.0 for unsloth?', 'answer': 'A minimum Torch version of 2.1.0 is required to ensure compatibility with unsloth.'}, {'question': 'What is the purpose of installing torch via pip before running the _auto_install.py script?', 'answer': 'Installing torch via pip is necessary because the _auto_install.py script requires it to function correctly.'}, {'question': 'How can one determine if their system has an Ampere GPU, which is supported by unsloth?', 'answer': 'You can check if your system has an Ampere GPU by running `torch.cuda.get_device_capability()[0] >= 8` in a Python REPL.'}]}\n",
      "{'question': 'What is the optimal pip installation command for unsloth?', 'answer': 'The optimal pip installation command can be obtained by running `wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -` or manually in a Python REPL.'}\n",
      "{'question': 'What are the specific CUDA versions supported by unsloth?', 'answer': 'Unsloth supports CUDA 12.1, 11.8, and 12.4.'}\n",
      "{'question': 'Why is it necessary to have a minimum Torch version of 2.1.0 for unsloth?', 'answer': 'A minimum Torch version of 2.1.0 is required to ensure compatibility with unsloth.'}\n",
      "{'question': 'What is the purpose of installing torch via pip before running the _auto_install.py script?', 'answer': 'Installing torch via pip is necessary because the _auto_install.py script requires it to function correctly.'}\n",
      "{'question': 'How can one determine if their system has an Ampere GPU, which is supported by unsloth?', 'answer': 'You can check if your system has an Ampere GPU by running `torch.cuda.get_device_capability()[0] >= 8` in a Python REPL.'}\n",
      "{'generated': [{'question': 'What are the conditions for using different versions of CUDA with PyTorch?', 'answer': 'The code checks if the version is less than or equal to v2.1.1, v2.1.2, v2.3.0, v2.4.0, v2.5.0, or v2.6.0 to use corresponding CUDA and PyTorch versions.'}, {'question': 'How does the code handle torch versions greater than 2.6.0?', 'answer': 'It raises a RuntimeError with the current Torch version being too new.'}]}\n",
      "{'question': 'What are the conditions for using different versions of CUDA with PyTorch?', 'answer': 'The code checks if the version is less than or equal to v2.1.1, v2.1.2, v2.3.0, v2.4.0, v2.5.0, or v2.6.0 to use corresponding CUDA and PyTorch versions.'}\n",
      "{'question': 'How does the code handle torch versions greater than 2.6.0?', 'answer': 'It raises a RuntimeError with the current Torch version being too new.'}\n",
      "{'generated': [{'question': 'What is the purpose of the unsloth command with a specific version installed?', 'answer': 'The unsloth command with a specific version installed is used to install a particular version of the unsloth library.'}, {'question': 'How do you install the modelscope library using pip?', 'answer': \"You can install the modelscope library by running 'pip install modelscope -U'.\"}, {'question': 'What is the purpose of setting the UNSLOTH_USE_MODELSCOPE environment variable to 1?', 'answer': 'Setting the UNSLOTH_USE_MODELSCOPE environment variable to 1 allows you to download models and datasets from ModelScope.'}, {'question': 'What is the syntax for installing a specific version of the unsloth library using pip?', 'answer': 'The syntax for installing a specific version of the unsloth library is \\'pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"\\'.'}, {'question': 'How do you download models and datasets from ModelScope using the unsloth_cli.py command?', 'answer': \"You can download models and datasets from ModelScope by running 'python unsloth_cli.py' with the UNSLOTH_USE_MODELSCOPE environment variable set to 1.\"}]}\n",
      "{'question': 'What is the purpose of the unsloth command with a specific version installed?', 'answer': 'The unsloth command with a specific version installed is used to install a particular version of the unsloth library.'}\n",
      "{'question': 'How do you install the modelscope library using pip?', 'answer': \"You can install the modelscope library by running 'pip install modelscope -U'.\"}\n",
      "{'question': 'What is the purpose of setting the UNSLOTH_USE_MODELSCOPE environment variable to 1?', 'answer': 'Setting the UNSLOTH_USE_MODELSCOPE environment variable to 1 allows you to download models and datasets from ModelScope.'}\n",
      "{'question': 'What is the syntax for installing a specific version of the unsloth library using pip?', 'answer': 'The syntax for installing a specific version of the unsloth library is \\'pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"\\'.'}\n",
      "{'question': 'How do you download models and datasets from ModelScope using the unsloth_cli.py command?', 'answer': \"You can download models and datasets from ModelScope by running 'python unsloth_cli.py' with the UNSLOTH_USE_MODELSCOPE environment variable set to 1.\"}\n",
      "{'generated': [{'question': 'What is the purpose of loading the LAION dataset?', 'answer': 'The LAION dataset serves as training data for fine-tuning a language model.'}, {'question': 'How are models loaded from Hugging Face datasets in this code snippet?', 'answer': 'Models are loaded using the `load_dataset` function from the Hugging Face library, which supports loading datasets from various sources.'}]}\n",
      "{'question': 'What is the purpose of loading the LAION dataset?', 'answer': 'The LAION dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'How are models loaded from Hugging Face datasets in this code snippet?', 'answer': 'Models are loaded using the `load_dataset` function from the Hugging Face library, which supports loading datasets from various sources.'}\n",
      "{'generated': [{'question': 'What are the specific model variants mentioned in this documentation?', 'answer': 'The models discussed include unsloth/Llama-3.2-3B-bnb-4bit, unsloth/Llama-3.2-3B-Instruct-bnb-4bit, and unsloth/Llama-3.3-70B-Instruct-bnb-4bit.'}, {'question': 'Who is the producer of this PDF document, and what software was used to create it?', 'answer': 'The producer is pdfTeX-1.40.25, and the document was created using LaTeX via pandoc.'}, {'question': 'What are some key features of the new Llama 3.3 70B model mentioned in this documentation?', 'answer': 'This section does not mention specific details about the new Llama 3.3 70B model, so no answer can be provided.'}, {'question': 'What is the total number of pages in this document?', 'answer': 'The total number of pages in this document is 10.'}, {'question': 'Can you describe the environment and software used to generate this PDF, according to its metadata?', 'answer': 'According to the metadata, this PDF was generated using pdfTeX-1.40.25 and LaTeX via pandoc, with TeX Live 2023/Debian.'}]}\n",
      "{'question': 'What are the specific model variants mentioned in this documentation?', 'answer': 'The models discussed include unsloth/Llama-3.2-3B-bnb-4bit, unsloth/Llama-3.2-3B-Instruct-bnb-4bit, and unsloth/Llama-3.3-70B-Instruct-bnb-4bit.'}\n",
      "{'question': 'Who is the producer of this PDF document, and what software was used to create it?', 'answer': 'The producer is pdfTeX-1.40.25, and the document was created using LaTeX via pandoc.'}\n",
      "{'question': 'What are some key features of the new Llama 3.3 70B model mentioned in this documentation?', 'answer': 'This section does not mention specific details about the new Llama 3.3 70B model, so no answer can be provided.'}\n",
      "{'question': 'What is the total number of pages in this document?', 'answer': 'The total number of pages in this document is 10.'}\n",
      "{'question': 'Can you describe the environment and software used to generate this PDF, according to its metadata?', 'answer': 'According to the metadata, this PDF was generated using pdfTeX-1.40.25 and LaTeX via pandoc, with TeX Live 2023/Debian.'}\n",
      "{'generated': [{'question': 'What is the primary purpose of this code snippet?', 'answer': 'This code snippet serves as an example for loading and fine-tuning a language model using FastModel from Hugging Face.'}, {'question': 'How does the code load and patch the model, and what is the significance of 4-bit quantization?', 'answer': \"The code loads the 'unsloth/gemma-3-4B-it' model, then patches it using FastLanguageModel.get_peft_model. The 4-bit quantization reduces memory usage.\"}, {'question': 'What are the key differences between loading in 4-bit and 8-bit for this model?', 'answer': 'Loading in 8-bit uses more memory but is slightly more accurate than loading in 4-bit, which reduces memory usage.'}, {'question': 'Can you explain the purpose of using gradient checkpointing in this code snippet?', 'answer': \"Gradient checkpointing is used to reduce memory usage and improve performance for very long context models. 'unsloth' uses 30% less VRAM compared to other methods.\"}, {'question': 'What are the benefits and trade-offs of using different levels of gradient checkpointing in this code?', 'answer': \"Gradient checkpointing reduces VRAM usage but may increase computation time or memory usage depending on the specific application. 'unsloth' is optimized for very long context models.\"}]}\n",
      "{'question': 'What is the primary purpose of this code snippet?', 'answer': 'This code snippet serves as an example for loading and fine-tuning a language model using FastModel from Hugging Face.'}\n",
      "{'question': 'How does the code load and patch the model, and what is the significance of 4-bit quantization?', 'answer': \"The code loads the 'unsloth/gemma-3-4B-it' model, then patches it using FastLanguageModel.get_peft_model. The 4-bit quantization reduces memory usage.\"}\n",
      "{'question': 'What are the key differences between loading in 4-bit and 8-bit for this model?', 'answer': 'Loading in 8-bit uses more memory but is slightly more accurate than loading in 4-bit, which reduces memory usage.'}\n",
      "{'question': 'Can you explain the purpose of using gradient checkpointing in this code snippet?', 'answer': \"Gradient checkpointing is used to reduce memory usage and improve performance for very long context models. 'unsloth' uses 30% less VRAM compared to other methods.\"}\n",
      "{'question': 'What are the benefits and trade-offs of using different levels of gradient checkpointing in this code?', 'answer': \"Gradient checkpointing reduces VRAM usage but may increase computation time or memory usage depending on the specific application. 'unsloth' is optimized for very long context models.\"}\n",
      "{'generated': [{'question': 'What is the purpose of setting max_seq_length in this code snippet?', 'answer': 'The max_seq_length parameter determines the maximum length of a sequence that can be processed by the model.'}, {'question': 'How does the use_rslora flag affect the training process?', 'answer': 'When set to False, rank stabilized LoRA (RSLoRA) is not used in the training process.'}]}\n",
      "{'question': 'What is the purpose of setting max_seq_length in this code snippet?', 'answer': 'The max_seq_length parameter determines the maximum length of a sequence that can be processed by the model.'}\n",
      "{'question': 'How does the use_rslora flag affect the training process?', 'answer': 'When set to False, rank stabilized LoRA (RSLoRA) is not used in the training process.'}\n",
      "{'generated': [{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}, {'question': 'What are some advanced tips provided in the Unsloth documentation?', 'answer': 'The tips include saving to GGUF, merging to 16-bit for vLLM, continued training from a saved LoRA adapter, adding an evaluation loop, and customized chat templates.'}, {'question': 'Which reinforcement learning algorithms are supported by Unsloth?', 'answer': 'Unsloth supports ReinforcementLearning including DPO, GRPO, PPO, Reward Modelling, and Online DPO.'}, {'question': 'Where can I find more information about the implementation of these algorithms in notebooks?', 'answer': \"You can find a list of RL notebooks on Unsloth's wiki page, which includes links to advanced Qwen3 GRPO notebook, ORPO notebook, DPO Zephyr notebook, KTO notebook, and SimPO notebook.\"}, {'question': 'How do I import necessary libraries for using Unsloth?', 'answer': \"You can import the required libraries by adding the following lines of code: `import os`, `os.environ['CUDA_VISIBLE_DEVICES'] = '0'`, `from unsloth import FastLanguageModel`, and `import torch`.\"}, {'question': 'What is the process for loading a model in 4-bit precision?', 'answer': 'You can load a model in 4-bit precision using the `load_in_4bit=True` argument when calling `FastLanguageModel.from_pretrained()`. Additionally, you need to set `max_seq_length=2048` and specify the model name as `unsloth/zephyr-sft-bnb-4bit`.'}]}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'What are some advanced tips provided in the Unsloth documentation?', 'answer': 'The tips include saving to GGUF, merging to 16-bit for vLLM, continued training from a saved LoRA adapter, adding an evaluation loop, and customized chat templates.'}\n",
      "{'question': 'Which reinforcement learning algorithms are supported by Unsloth?', 'answer': 'Unsloth supports ReinforcementLearning including DPO, GRPO, PPO, Reward Modelling, and Online DPO.'}\n",
      "{'question': 'Where can I find more information about the implementation of these algorithms in notebooks?', 'answer': \"You can find a list of RL notebooks on Unsloth's wiki page, which includes links to advanced Qwen3 GRPO notebook, ORPO notebook, DPO Zephyr notebook, KTO notebook, and SimPO notebook.\"}\n",
      "{'question': 'How do I import necessary libraries for using Unsloth?', 'answer': \"You can import the required libraries by adding the following lines of code: `import os`, `os.environ['CUDA_VISIBLE_DEVICES'] = '0'`, `from unsloth import FastLanguageModel`, and `import torch`.\"}\n",
      "{'question': 'What is the process for loading a model in 4-bit precision?', 'answer': 'You can load a model in 4-bit precision using the `load_in_4bit=True` argument when calling `FastLanguageModel.from_pretrained()`. Additionally, you need to set `max_seq_length=2048` and specify the model name as `unsloth/zephyr-sft-bnb-4bit`.'}\n",
      "{'generated': [{'question': 'What is the purpose of adding fast LoRA weights to the model?', 'answer': \"This step enables efficient and flexible adaptation of the model's parameters.\"}, {'question': \"How does the use of 'unsloth' for gradient checkpointing affect the model's performance and memory usage?\", 'answer': \"'Unsloth' is a strategy that uses 30% less VRAM, allowing for larger batch sizes and efficient computation.\"}, {'question': 'What are the target modules being patched in this code snippet?', 'answer': \"The 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', and 'down_proj' modules are targeted for patching.\"}, {'question': \"What is the significance of setting 'lora_dropout' to 0 in this code?\", 'answer': \"Setting 'lora_dropout' to 0 optimizes the model's performance by minimizing dropout probability.\"}, {'question': \"Why are certain modules ('q_proj', 'k_proj', etc.) specified as targets for patching and LoRA adaptation?\", 'answer': \"These modules are critical for the model's attention mechanisms, making them ideal candidates for efficient LoRA adaptation.\"}]}\n",
      "{'question': 'What is the purpose of adding fast LoRA weights to the model?', 'answer': \"This step enables efficient and flexible adaptation of the model's parameters.\"}\n",
      "{'question': \"How does the use of 'unsloth' for gradient checkpointing affect the model's performance and memory usage?\", 'answer': \"'Unsloth' is a strategy that uses 30% less VRAM, allowing for larger batch sizes and efficient computation.\"}\n",
      "{'question': 'What are the target modules being patched in this code snippet?', 'answer': \"The 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', and 'down_proj' modules are targeted for patching.\"}\n",
      "{'question': \"What is the significance of setting 'lora_dropout' to 0 in this code?\", 'answer': \"Setting 'lora_dropout' to 0 optimizes the model's performance by minimizing dropout probability.\"}\n",
      "{'question': \"Why are certain modules ('q_proj', 'k_proj', etc.) specified as targets for patching and LoRA adaptation?\", 'answer': \"These modules are critical for the model's attention mechanisms, making them ideal candidates for efficient LoRA adaptation.\"}\n",
      "{'generated': [{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}, {'question': 'How does the DPOConfig class influence the training process?', 'answer': 'The DPOConfig class defines various parameters such as per_device_train_batch_size, gradient_accumulation_steps, and warmup_ratio that affect the training process.'}]}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'How does the DPOConfig class influence the training process?', 'answer': 'The DPOConfig class defines various parameters such as per_device_train_batch_size, gradient_accumulation_steps, and warmup_ratio that affect the training process.'}\n",
      "{'generated': [{'question': 'What type of model was fine-tuned using the dataset?', 'answer': 'The QLoRA model was used for fine-tuning.'}, {'question': 'How many linear layers were modified during fine-tuning?', 'answer': 'All linear layers (Q, K, V, O, gate, up and down) were modified.'}, {'question': 'What was the rank of QLoRA used for fine-tuning?', 'answer': 'The rank used was 32.'}]}\n",
      "{'question': 'What type of model was fine-tuned using the dataset?', 'answer': 'The QLoRA model was used for fine-tuning.'}\n",
      "{'question': 'How many linear layers were modified during fine-tuning?', 'answer': 'All linear layers (Q, K, V, O, gate, up and down) were modified.'}\n",
      "{'question': 'What was the rank of QLoRA used for fine-tuning?', 'answer': 'The rank used was 32.'}\n",
      "{'generated': [{'question': 'What were the results of testing Llama 3.3 (70B) with a maximum context length on an A100 GPU?', 'answer': 'We tested Llama 3.3 (70B) Instruct on an 80GB A100 and did 4bit QLoRA on all linear layers.'}, {'question': 'What was the effect of using a batch size of 1 in the experiment with Llama 3.3 (70B)?', 'answer': 'We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.'}]}\n",
      "{'question': 'What were the results of testing Llama 3.3 (70B) with a maximum context length on an A100 GPU?', 'answer': 'We tested Llama 3.3 (70B) Instruct on an 80GB A100 and did 4bit QLoRA on all linear layers.'}\n",
      "{'question': 'What was the effect of using a batch size of 1 in the experiment with Llama 3.3 (70B)?', 'answer': 'We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.'}\n",
      "{'generated': [{'question': 'Who has contributed to Unsloth?', 'answer': 'Every single person who has used or contributed to Unsloth'}, {'question': 'What is the significance of Unsloth in the context of user contributions and usage?', 'answer': 'Unsloth acknowledges and appreciates the value of every individual who has used or contributed to it.'}]}\n",
      "{'question': 'Who has contributed to Unsloth?', 'answer': 'Every single person who has used or contributed to Unsloth'}\n",
      "{'question': 'What is the significance of Unsloth in the context of user contributions and usage?', 'answer': 'Unsloth acknowledges and appreciates the value of every individual who has used or contributed to it.'}\n",
      "Done writing unsloth_data.json to system\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List \n",
    "from pydantic import BaseModel\n",
    "import ollama\n",
    "\n",
    "def prompt_template(data: str, num_records: int = 5):\n",
    "\n",
    "    return f\"\"\"You are an expert data curator assisting a machine learning engineer in creating a high-quality instruction tuning dataset. Your task is to transform \n",
    "    the provided data chunk into diverse question and answer (Q&A) pairs that will be used to fine-tune a language model. \n",
    "\n",
    "    For each of the {num_records} entries, generate one or two well-structured questions that reflect different aspects of the information in the chunk. \n",
    "    Ensure a mix of longer and shorter questions, with shorter ones typically containing 1-2 sentences and longer ones spanning up to 3-4 sentences. Each \n",
    "    Q&A pair should be concise yet informative, capturing key insights from the data.\n",
    "\n",
    "    Structure your output in JSON format, where each object contains 'question' and 'answer' fields. The JSON structure should look like this:\n",
    "\n",
    "        \"question\": \"Your question here...\",\n",
    "        \"answer\": \"Your answer here...\"\n",
    "\n",
    "    Focus on creating clear, relevant, and varied questions that encourage the model to learn from diverse perspectives. Avoid any sensitive or biased \n",
    "    content, ensuring answers are accurate and neutral.\n",
    "\n",
    "    Example:\n",
    "    \n",
    "        \"question\": \"What is the primary purpose of this dataset?\",\n",
    "        \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
    "    \n",
    "\n",
    "    By following these guidelines, you'll contribute to a robust and effective dataset that enhances the model's performance.\"\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Explanation:**\n",
    "\n",
    "    - **Clarity and Specificity:** The revised prompt clearly defines the role of the assistant and the importance of the task, ensuring alignment with the \n",
    "    project goals.\n",
    "    - **Quality Standards:** It emphasizes the need for well-formulated Q&A pairs, specifying the structure and content of each question and answer.\n",
    "    - **Output Format:** An example JSON structure is provided to guide the format accurately.\n",
    "    - **Constraints and Biases:** A note on avoiding sensitive or biased content ensures ethical considerations are met.\n",
    "    - **Step-by-Step Guidance:** The prompt breaks down the task into manageable steps, making it easier for the assistant to follow.\n",
    "\n",
    "    This approach ensures that the generated data is both high-quality and meets the specific requirements of the machine learning project.\n",
    "    \n",
    "    Data\n",
    "    {data}\n",
    "    \"\"\"\n",
    "\n",
    "class Record(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class Response(BaseModel):\n",
    "    generated: List[Record]\n",
    "\n",
    "def llm_call(data: str, num_records: int = 5) -> dict:\n",
    "    response = ollama.generate(\n",
    "        model=\"llama3.1\",\n",
    "        prompt=prompt_template(data, num_records),\n",
    "        options={\n",
    "            \"num_predict\": 2000\n",
    "        },\n",
    "        format=Response.model_json_schema(),\n",
    "    )\n",
    "    return json.loads(response['response'])\n",
    "\n",
    "dataset = []\n",
    "for i, chunk in enumerate(split_docs):\n",
    "    data = llm_call(chunk)\n",
    "    print(data)\n",
    "    for pair in data['generated']:\n",
    "        print(pair)\n",
    "        dataset.append({\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer']\n",
    "            })\n",
    "tuning_data = 'unsloth_data.json'\n",
    "with open(tuning_data,'w') as f: \n",
    "    json.dump(dataset, f) \n",
    "\n",
    "print(f\"Done writing {tuning_data} to system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb0d03-f066-4850-b250-ba4bb5983635",
   "metadata": {},
   "source": [
    "Now that we have a set of training data we are ready to train!\n",
    "\n",
    "Before doing however we need to install unsloth to help speedup the performance the reduce the RAM required for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14f23ca-942c-4515-9616-f3e4d66f9e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.3 environment at: venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m85 packages\u001b[0m \u001b[2min 372ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m41 packages\u001b[0m \u001b[2min 43.07s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m61 packages\u001b[0m \u001b[2min 176ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.46.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcut-cross-entropy\u001b[0m\u001b[2m==25.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.34.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-transfer\u001b[0m\u001b[2m==0.1.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.6.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.6.80\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.11.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.7.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.85\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==20.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshtab\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.19.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtypeguard\u001b[0m\u001b[2m==4.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyro\u001b[0m\u001b[2m==0.9.26\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munsloth\u001b[0m\u001b[2m==2025.6.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munsloth-zoo\u001b[0m\u001b[2m==2025.6.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwheel\u001b[0m\u001b[2m==0.45.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.30\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.23.0\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.3 environment at: venv\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--torch-backend` setting is experimental and may change without warning. Pass `--preview` to disable this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m147 packages\u001b[0m \u001b[2min 804ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m87 packages\u001b[0m \u001b[2min 5.30s\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m21 packages\u001b[0m \u001b[2min 106ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m87 packages\u001b[0m \u001b[2min 230ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250706\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcloudpickle\u001b[0m\u001b[2m==3.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcupy-cuda12x\u001b[0m\u001b[2m==13.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1meinops\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.116.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastrlock\u001b[0m\u001b[2m==0.8.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.70.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.73.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.27.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgpack\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mninja\u001b[0m\u001b[2m==1.11.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.6\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.6.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.3.14\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.6.80\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.57\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.61\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.57\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.7.1.26\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.41\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.11.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.0.11\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.7.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.55\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.2.55\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.7.53\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.85\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.61\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.55\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.93.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.12.0.88\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.34.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.34.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.34.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.34.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.34.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.34.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.34.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.55b1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpy-cpuinfo\u001b[0m\u001b[2m==9.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mray\u001b[0m\u001b[2m==2.47.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.32.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==79.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.46.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.0+cu128\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.7.0+cu128\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.0+cu128\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.19\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install unsloth vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a035e5-63c0-44ba-b21e-aaa017deaaed",
   "metadata": {},
   "source": [
    "You will also need PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d2ee12-6410-4761-bf1e-94da31e388c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.22.0+cu128)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (79.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in ./venv/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in ./venv/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in ./venv/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in ./venv/lib/python3.12/site-packages (from torch) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in ./venv/lib/python3.12/site-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in ./venv/lib/python3.12/site-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in ./venv/lib/python3.12/site-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in ./venv/lib/python3.12/site-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in ./venv/lib/python3.12/site-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in ./venv/lib/python3.12/site-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in ./venv/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in ./venv/lib/python3.12/site-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in ./venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[1m\u001b[31merror:\u001b[0m unrecognized subcommand '\u001b[33minstall\u001b[0m'\n",
      "\n",
      "  \u001b[32mtip:\u001b[0m a similar subcommand exists: '\u001b[32muv pip install\u001b[0m'\n",
      "\n",
      "\u001b[1m\u001b[32mUsage:\u001b[0m \u001b[1m\u001b[36muv\u001b[0m \u001b[36m[OPTIONS]\u001b[0m \u001b[36m<COMMAND>\u001b[0m\n",
      "\n",
      "For more information, try '\u001b[1m\u001b[36m--help\u001b[0m'.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!uv install -U xformers\n",
    "!uv pip install flash-attn --no-build-isolation\n",
    "!uv pip install flashinfer-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded20d68-dc32-4c5e-8ab7-53192c8e5e24",
   "metadata": {},
   "source": [
    "Validate the the device is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dd6bed3-1334-427e-94c6-da123725702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 5070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", use_cuda)\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d1d7e-0406-4510-a20d-e2e3718c3bd4",
   "metadata": {},
   "source": [
    "How let's setup our tokenizer using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d21330a-792e-422e-aea7-4de988f4110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.12: Fast Llama patching. Transformers: 4.53.1. vLLM: 0.9.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070. Num GPUs = 1. Max memory: 11.94 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c2e435f6574c52be6e34b5fa69a25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "\"You are a helpful, honest and harmless assitant designed to help engineers. Think through each question logically and provide an answer. Don't make things up, if you're unable to answer a question advise the user that you're unable to answer as it is outside of your scope.\n",
      "\n",
      "### Input:\n",
      "What are the models that can be fine-tuned with the provided data?\n",
      "\n",
      "### Response:\n",
      "The data supports fine-tuning of Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral<|end_of_text|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None\n",
    ")\n",
    "\n",
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "\"You are a helpful, honest and harmless assitant designed to help engineers. Think through each question logically and provide an answer. Don't make things up, if you're unable to answer a question advise the user that you're unable to answer as it is outside of your scope.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"question\"]\n",
    "    outputs      = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = prompt_style.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", split='train', data_files='unsloth_data.json')\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset[\"text\"][0])\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0, \n",
    "    bias=\"none\", \n",
    "   \n",
    "    use_gradient_checkpointing=\"unsloth\", \n",
    "    random_state=3407,\n",
    "    use_rslora=False, \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81e60b-f0df-4d5d-8cfa-f00f62413574",
   "metadata": {},
   "source": [
    "Now that we have the tokenizer we can setup the prompt for fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a582658-e27b-4d5e-bdd6-de21c7521f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function _UnslothSFTTrainer._prepare_dataset.<locals>._tokenize at 0x7f5947efe3e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49b4e5acdfc49669ba16f8435c82682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    num_train_epochs = 10,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 25,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        report_to=\"none\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a9d66-1c90-4f75-9ac9-e13a21ceef9a",
   "metadata": {},
   "source": [
    "The next step is to map our dataset to the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e82d836-9a6e-4d79-b242-624ebafe160b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 117 | Num Epochs = 13 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 335,544,320 of 8,000,000,000 (4.19% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:24, Epoch 12/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.867800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.940200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.717700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.348300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.2667884164303541, metrics={'train_runtime': 268.0753, 'train_samples_per_second': 5.968, 'train_steps_per_second': 0.373, 'total_flos': 1.1018697183535104e+16, 'train_loss': 0.2667884164303541})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da8dd3-9e96-4a33-9d24-6214408d82bc",
   "metadata": {},
   "source": [
    "Save the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7eeae5b-10f4-4174-9d5e-3f77bd0d6ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama-3.1-8B-unsloth/tokenizer_config.json',\n",
       " 'Llama-3.1-8B-unsloth/special_tokens_map.json',\n",
       " 'Llama-3.1-8B-unsloth/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_local = \"Llama-3.1-8B-unsloth\"\n",
    "model.save_pretrained(new_model_local) # Local saving\n",
    "tokenizer.save_pretrained(new_model_local) # Local saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b6601-021c-477e-a5c5-1442b9905746",
   "metadata": {},
   "source": [
    "host in local ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa08fff2-85f6-4ec5-9ae1-0184ceb6b9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 0% ⠋ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 11% ⠙ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 16% ⠹ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 27% ⠸ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 38% ⠼ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 44% ⠴ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 55% ⠦ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 67% ⠧ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 72% ⠇ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 83% ⠏ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 94% ⠋ \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:10bf4fc984f53fc830904de79d5d88d2ed020951f0cfb9d7471d0b594de7d1af 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:43ebb808e1ff9c5db32e112d1213e0ce0b78162a6eb2436aeee361cb88ae847b 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter \u001b[K\n",
      "using existing layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 \u001b[K\n",
      "using existing layer sha256:948af2743fc78a328dcb3b0f5a31b3d75f415840fdb699e8b1235978392ecf85 \u001b[K\n",
      "using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177 \u001b[K\n",
      "creating new layer sha256:fb16a8714e883bd9a2bd854143751e9132e5199139a4844257bf7b934f53bd9b \u001b[K\n",
      "using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Brian-McGinn/Fine-Tuning-Tutorial/main/Modelfile\n",
    "!ollama create unsloth-trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7740ad47-9554-4205-9595-468485ad12f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                         ID              SIZE      MODIFIED       \n",
      "unsloth-trained:latest       45b921c735c8    5.6 GB    38 seconds ago    \n",
      "unsloth-trained-v2:latest    dc225e3f3634    5.6 GB    47 hours ago      \n",
      "tm1-trained:latest           22f030dbb978    5.6 GB    2 days ago        \n",
      "qwen2.5:14b                  7cdf5a0187d5    9.0 GB    2 days ago        \n",
      "llama3.1:latest              46e0c10c039e    4.9 GB    3 days ago        \n",
      "deepseek-r1:1.5b             e0979632db5a    1.1 GB    4 days ago        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Confirm ollama loaded the model correctly\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567bf4f-ceba-4dd7-90b7-1ea1c4172712",
   "metadata": {},
   "source": [
    "Now let's do some cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991744e-4cd1-4006-85e0-75b63fe85f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6fdb7-a98a-4242-977d-d606c9c8b214",
   "metadata": {},
   "source": [
    "This will require an ollama reboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9f6d2-7b5a-494a-ae3e-9342bf9ae69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm\n",
    " # ollama serve & "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf389d-2fb2-4603-b9b1-619e50a789e8",
   "metadata": {},
   "source": [
    "Call latest model from ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f65071-df25-4968-8eb1-786c950363fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the latest updates from Unsloth:\n",
      "\n",
      "### Model Updates:\n",
      "Unsloth has released a new model, Llama 3.2 (8B), which offers improved performance and capabilities.\n",
      "\n",
      "### Featured Articles:\n",
      "Check out these articles for in-depth information on topics like AI safety, ethics, and cutting-edge research:\n",
      "\n",
      "1. \"Philosophical Foundations of Deep Learning: A Review\" - This article explores the philosophical underpinnings of deep learning and its potential implications.\n",
      "2. \"The Ethics of Creating Life: A Discussion on Llama 3.2 (7B) and Beyond\" - This piece delves into the ethical considerations surrounding the development of advanced AI models like Llama 3.2 (7B).\n",
      "\n",
      "### Community News:\n",
      "Join the conversation by participating in discussions related to these topics:\n",
      "\n",
      "1. What are some potential applications of Llama 3.2 (8B) in industries like healthcare and finance?\n",
      "2. How can users contribute to the development of Unsloth's models through their own projects and research?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"unsloth-trained\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4e799-5b50-42b2-9cea-8f3d20e20134",
   "metadata": {},
   "source": [
    "Results inconsistent let's try rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "954620de-a87b-45c8-9e9a-0dc08f90d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: pypdf in ./venv/lib/python3.12/site-packages (5.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19c4f35-e9cf-4c7d-8a22-8f99364e2697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documentation, here's a comprehensive summary of Unsloth.ai news:\n",
      "\n",
      "**Recent Developments**\n",
      "\n",
      "* The Unsloth team has made significant progress in optimizing language models for long context finetuning workloads. Their solution, called Unsloth, allows users to fine-tune large language models with minimal computational resources.\n",
      "* Unsloth has been integrated with Hugging Face's Transformers Library (TRL) and supports a wide range of language models, including Llama-3.1, Meta-Llama-3.2, and Phi-3.5.\n",
      "\n",
      "**Performance Benchmarks**\n",
      "\n",
      "* The team has conducted extensive benchmarking tests using the Alpaca Dataset and reported significant improvements in performance compared to Hugging Face's implementation.\n",
      "* Unsloth achieves a 2x speedup and reduces VRAM usage by >75% while maintaining comparable results to Hugging Face's implementation.\n",
      "* The team has also released detailed benchmarks for Llama-3.3 (70B) on the Alpaca Dataset, demonstrating its ability to handle long context lengths with minimal memory requirements.\n",
      "\n",
      "**New Features**\n",
      "\n",
      "* Unsloth now supports 4-bit quantization, which reduces memory usage and allows users to fine-tune larger models.\n",
      "* The team has also introduced a new \"unsloth\" strategy for gradient checkpointing, which uses 30% less VRAM and enables the use of larger batch sizes.\n",
      "\n",
      "**Reinforcement Learning**\n",
      "\n",
      "* Unsloth supports Reinforcement Learning (RL) algorithms, including DPO, GRPO, PPO, Reward Modelling, and Online DPO.\n",
      "* The team has released several notebooks demonstrating the usage of these RL algorithms with Unsloth.\n",
      "\n",
      "**Model Patching and LoRA Weights**\n",
      "\n",
      "* Unsloth allows users to patch models using FastLanguageModel.get_peft_model() and add fast LoRA weights for improved performance.\n",
      "* The team has also introduced a new \"unsloth\" strategy for gradient checkpointing, which uses 30% less VRAM and enables the use of larger batch sizes.\n",
      "\n",
      "**Community Contributions**\n",
      "\n",
      "* The Unsloth team expresses gratitude to several contributors, including Erik, Etherl, and the Hugging Face team, for their help in developing and refining the library.\n",
      "* The team also acknowledges the contributions of users who have helped improve the library through bug reports and feature requests.\n",
      "\n",
      "Overall, Unsloth.ai has made significant progress in recent months, delivering improvements in performance, memory usage, and ease of use. The library continues to be an active project with ongoing development and community engagement.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pypdf  import PdfReader\n",
    "import os\n",
    "\n",
    "# Load and extract text from the PDF\n",
    "def load_pdf_content(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_content = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_content += page.extract_text() + \"\\n\"\n",
    "    return text_content\n",
    "\n",
    "# Load the unsloth documentation\n",
    "pdf_content = load_pdf_content(\"unsloth_documentation.pdf\")\n",
    "\n",
    "# Create the RAG-enhanced prompt\n",
    "rag_prompt = f\"\"\"Based on the following documentation about Unsloth.ai, please answer the user's question:\n",
    "\n",
    "Documentation:\n",
    "{pdf_content}\n",
    "\n",
    "User Question: Give me some Unsloth.ai News?\n",
    "\n",
    "Please provide a comprehensive answer based on the documentation provid d.\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
