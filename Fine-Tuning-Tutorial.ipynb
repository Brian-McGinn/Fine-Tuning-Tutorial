{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36078ad3-348c-4164-8483-955966a4f020",
   "metadata": {},
   "source": [
    "Initial query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8361ecb9-8a5c-446b-990d-9f70ebdc04ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1', 'created_at': '2025-07-06T23:54:24.498315152Z', 'message': {'role': 'assistant', 'content': 'There is no such thing as \"Unsloth.ai\" news. It appears to be a made-up or fictional term.\\n\\nIf you\\'re looking for information on AI, technology, or industry-related news, I\\'d be happy to provide updates and insights from reputable sources like:\\n\\n* Google\\'s AI research\\n* Meta AI (formerly Facebook AI)\\n* Microsoft AI\\n* OpenAI\\n* Research papers and publications in the field of artificial intelligence\\n\\nLet me know if there\\'s something specific you\\'re interested in, and I\\'ll do my best to help!'}, 'done_reason': 'stop', 'done': True, 'total_duration': 6440074865, 'load_duration': 5076463525, 'prompt_eval_count': 18, 'prompt_eval_duration': 208438417, 'eval_count': 112, 'eval_duration': 1153440250}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"}],\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, data=json.dumps(payload))\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600a5a9-705c-42a0-9141-40c1e7039346",
   "metadata": {},
   "source": [
    "Now let's try the same command with Python.\n",
    "\n",
    "First we need to install the Ollama library for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac3c5ce-6258-4930-9b3a-0956b39620f1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Collecting ollama\n",
      "  Using cached ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in ./venv/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Using cached ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, ollama\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.5.1 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbce5ce-0222-4cd8-a28a-9e63ac989e98",
   "metadata": {},
   "source": [
    "Next let's create a simple python query using the Ollama library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7550214-582d-47bf-8f6d-5b6ab26cbb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find any information on \"Unsloth.ai\". It's possible that it's a fictional company, or it may be a very new and unknown entity. Can you provide more context or clarify what Unsloth.ai is? I'll do my best to help.\n",
      "\n",
      "However, if you're interested in news related to AI, machine learning, or technology, I'd be happy to share some general updates or trends with you!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48630fa6-a7ed-4137-8e42-442237ab81e2",
   "metadata": {},
   "source": [
    "As you can see unsloth.ai causes some hallucinations with llama3.1 and the results can be quite humorous. We will want to fix this by fine tuning the model and give it some information about the Unsloth project. \n",
    "\n",
    "The first step in doing this is to convert the provided unsloth_documentation.pdf into chunks using the PyPDFLoader library from LangChain. To use LangChain we will need to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c5161b-b737-49f2-8d30-51c0ac9b81cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: langchain in ./venv/lib/python3.12/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain_community in ./venv/lib/python3.12/site-packages (0.3.27)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in ./venv/lib/python3.12/site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./venv/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./venv/lib/python3.12/site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./venv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.12/site-packages (from langchain_community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./venv/lib/python3.12/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./venv/lib/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./venv/lib/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./venv/lib/python3.12/site-packages (from langchain_community) (2.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in ./venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Downloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-5.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f75d4-0d62-4c7d-9f9e-52a151830892",
   "metadata": {},
   "source": [
    "With everything installed we can split our PDF into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b69cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Finetune Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral 2x faster with\n",
      "80% less VRAM!\n",
      "Finetune for Free\n",
      "Notebooks are beginner friendly. Read our guide. Add your dataset, click “Run\n",
      "All”, and export your finetuned model to GGUF, Ollama, vLLM or Hugging\n",
      "Face.\n",
      "Unsloth supports Free Notebooks Performance Memory use\n",
      "Gemma 3n (4B) Start for free 1.5x faster 50% less\n",
      "Qwen3 (14B) Start for free 2x faster 70% less\n",
      "Qwen3 (4B):\n",
      "GRPO\n",
      "Start for free 2x faster 80% less\n",
      "Gemma 3 (4B) Start for free 1.6x faster 60% less\n",
      "Llama 3.2 (3B) Start for free 2x faster 70% less\n",
      "Phi-4 (14B) Start for free 2x faster 70% less\n",
      "Llama 3.2 Vision\n",
      "(11B)\n",
      "Start for free 2x faster 50% less\n",
      "Llama 3.1 (8B) Start for free 2x faster 70% less\n",
      "Mistral v0.3 (7B) Start for free 2.2x faster 75% less\n",
      "Orpheus-TTS\n",
      "(3B)\n",
      "Start for free 1.5x faster 50% less\n",
      "• See all our notebooks for: Kaggle, GRPO,TTS & Vision\n",
      "• See all our models and all our notebooks\n",
      "• See detailed documentation for Unsloth here\n",
      "Quickstart' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX via pandoc', 'creationdate': '2025-07-03T16:37:52-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-03T16:37:52-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023/Debian) kpathsea version 6.3.5', 'source': 'unsloth_documentation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"unsloth_documentation.pdf\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, \n",
    "            chunk_overlap=50\n",
    "        )\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(split_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32283d35",
   "metadata": {},
   "source": [
    "Now that we have the PDF chunks we can use ollama to generate data for our fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f4b259d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated': [{'question': \"What are the benefits of using Unsloth's Notebooks for fine-tuning language models?\", 'answer': \"Unsloth's Notebooks offer a faster and more memory-efficient way to finetune language models, with some models being 2x faster and requiring up to 80% less VRAM.\"}, {'question': 'Which Unsloth Notebooks support free usage for beginners?', 'answer': \"Unsloth's Notebooks are beginner-friendly, and users can start using them for free, allowing them to finetune their models without any additional cost.\"}, {'question': 'What is the difference in performance between Gemma 3n and Qwen3 when it comes to fine-tuning language models?', 'answer': 'Gemma 3n offers 1.5x faster performance compared to Qwen3, while also requiring 50% less VRAM.'}, {'question': \"Can I export my finetuned model to various platforms using Unsloth's Notebooks?\", 'answer': \"Yes, users can export their finetuned models to GGUF, Ollama, vLLM, or Hugging Face using Unsloth's Notebooks.\"}, {'question': 'Where can I find detailed documentation for Unsloth and its Notebooks?', 'answer': 'Detailed documentation for Unsloth is available here, providing users with a comprehensive guide to using the platform and its features.'}]}\n",
      "{'question': \"What are the benefits of using Unsloth's Notebooks for fine-tuning language models?\", 'answer': \"Unsloth's Notebooks offer a faster and more memory-efficient way to finetune language models, with some models being 2x faster and requiring up to 80% less VRAM.\"}\n",
      "{'question': 'Which Unsloth Notebooks support free usage for beginners?', 'answer': \"Unsloth's Notebooks are beginner-friendly, and users can start using them for free, allowing them to finetune their models without any additional cost.\"}\n",
      "{'question': 'What is the difference in performance between Gemma 3n and Qwen3 when it comes to fine-tuning language models?', 'answer': 'Gemma 3n offers 1.5x faster performance compared to Qwen3, while also requiring 50% less VRAM.'}\n",
      "{'question': \"Can I export my finetuned model to various platforms using Unsloth's Notebooks?\", 'answer': \"Yes, users can export their finetuned models to GGUF, Ollama, vLLM, or Hugging Face using Unsloth's Notebooks.\"}\n",
      "{'question': 'Where can I find detailed documentation for Unsloth and its Notebooks?', 'answer': 'Detailed documentation for Unsloth is available here, providing users with a comprehensive guide to using the platform and its features.'}\n",
      "{'generated': [{'question': 'How do I install Unsloth on a Linux device?', 'answer': 'You can install Unsloth on a Linux device using pip with the command `pip install unsloth`.'}, {'question': 'What are some of the new features supported by Unsloth, and which specific models are included?', 'answer': 'Unsloth now supports Text-to-Speech (TTS), including sesame/csm-1b and STT openai/whisper-large-v3. Additionally, Qwen3 is supported, with Qwen3-30B-A3B fitting on 17.5GB VRAM.'}, {'question': 'Can you provide more information about the Dynamic 2.0 quants in Unsloth and their impact?', 'answer': \"Unsloth's Dynamic 2.0 quants have set new benchmarks for 5-shot MMLU & KL Divergence, indicating improved performance and efficiency.\"}, {'question': 'What are the supported Llama models in Unsloth, and how do they contribute to its capabilities?', 'answer': 'Unsloth now supports Llama 4 by Meta, including Scout & Maverick, which further enhance its language understanding and generation abilities.'}, {'question': 'How does Unsloth handle updates and new features, ensuring users have access to the latest advancements?', 'answer': \"The provided installation instructions for Windows can be found via a link within the documentation. Regular updates and new feature introductions are available as part of Unsloth's development and improvement process.\"}]}\n",
      "{'question': 'How do I install Unsloth on a Linux device?', 'answer': 'You can install Unsloth on a Linux device using pip with the command `pip install unsloth`.'}\n",
      "{'question': 'What are some of the new features supported by Unsloth, and which specific models are included?', 'answer': 'Unsloth now supports Text-to-Speech (TTS), including sesame/csm-1b and STT openai/whisper-large-v3. Additionally, Qwen3 is supported, with Qwen3-30B-A3B fitting on 17.5GB VRAM.'}\n",
      "{'question': 'Can you provide more information about the Dynamic 2.0 quants in Unsloth and their impact?', 'answer': \"Unsloth's Dynamic 2.0 quants have set new benchmarks for 5-shot MMLU & KL Divergence, indicating improved performance and efficiency.\"}\n",
      "{'question': 'What are the supported Llama models in Unsloth, and how do they contribute to its capabilities?', 'answer': 'Unsloth now supports Llama 4 by Meta, including Scout & Maverick, which further enhance its language understanding and generation abilities.'}\n",
      "{'question': 'How does Unsloth handle updates and new features, ensuring users have access to the latest advancements?', 'answer': \"The provided installation instructions for Windows can be found via a link within the documentation. Regular updates and new feature introductions are available as part of Unsloth's development and improvement process.\"}\n",
      "{'generated': []}\n",
      "{'generated': [{'question': 'What is the primary purpose of Unsloth in relation to transformer-style models?', 'answer': 'Unsloth enables support for full-finetuning, pretraining, 4b-bit, 16-bit and 8-bit training, as well as supporting all transformer-style models including TTS, STT, multimodal, diffusion, BERT and more.'}, {'question': 'How does Unsloth improve memory usage in comparison to native transformer models?', 'answer': 'We cut memory usage by a further 30% with Unsloth, allowing for support of 4x longer context windows compared to its native counterpart.'}, {'question': 'What is the significance of the gradient accumulation bug fix in Unsloth and transformers?', 'answer': 'The gradient accumulation bug was found and fixed; users are advised to update Unsloth and transformers accordingly to ensure optimal performance.'}, {'question': \"How does Unsloth's context support compare to that of Llama 3.1 (8B) and other models?\", 'answer': \"Unsloth enables 342K context, surpassing the native 128K support of Llama 3.1 (8B), and is 13x longer than HF+FA2's context.\"}, {'question': \"What programming languages are used in Unsloth's kernel implementation?\", 'answer': 'All kernels written in OpenAI’s Triton language, which includes a manual backprop engine for added efficiency.'}]}\n",
      "{'question': 'What is the primary purpose of Unsloth in relation to transformer-style models?', 'answer': 'Unsloth enables support for full-finetuning, pretraining, 4b-bit, 16-bit and 8-bit training, as well as supporting all transformer-style models including TTS, STT, multimodal, diffusion, BERT and more.'}\n",
      "{'question': 'How does Unsloth improve memory usage in comparison to native transformer models?', 'answer': 'We cut memory usage by a further 30% with Unsloth, allowing for support of 4x longer context windows compared to its native counterpart.'}\n",
      "{'question': 'What is the significance of the gradient accumulation bug fix in Unsloth and transformers?', 'answer': 'The gradient accumulation bug was found and fixed; users are advised to update Unsloth and transformers accordingly to ensure optimal performance.'}\n",
      "{'question': \"How does Unsloth's context support compare to that of Llama 3.1 (8B) and other models?\", 'answer': \"Unsloth enables 342K context, surpassing the native 128K support of Llama 3.1 (8B), and is 13x longer than HF+FA2's context.\"}\n",
      "{'question': \"What programming languages are used in Unsloth's kernel implementation?\", 'answer': 'All kernels written in OpenAI’s Triton language, which includes a manual backprop engine for added efficiency.'}\n",
      "{'generated': [{'question': 'What are the system requirements for running Unsloth?', 'answer': 'Unsloth supports NVIDIA GPUs with CUDA Capability 7.0 or higher, and works on both Linux and Windows.'}, {'question': 'Is there a specific version of Python required to use Unsloth?', 'answer': 'No, but you should avoid using Python 3.13 as it is not supported by Unsloth. Instead, use versions like 3.12, 3.11, or 3.10.'}, {'question': 'How do I install the necessary drivers for my NVIDIA GPU?', 'answer': \"You should download and install the latest version of your GPU's driver from the NVIDIA website.\"}, {'question': 'What are some additional steps required for setting up Unsloth on Windows?', 'answer': 'On Windows, you need to install Visual Studio with C++ metadata in addition to installing the NVIDIA Video Driver.'}, {'question': 'How do I update Unsloth and its dependencies using pip?', 'answer': \"You can update Unsloth by running 'pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo'.\"}]}\n",
      "{'question': 'What are the system requirements for running Unsloth?', 'answer': 'Unsloth supports NVIDIA GPUs with CUDA Capability 7.0 or higher, and works on both Linux and Windows.'}\n",
      "{'question': 'Is there a specific version of Python required to use Unsloth?', 'answer': 'No, but you should avoid using Python 3.13 as it is not supported by Unsloth. Instead, use versions like 3.12, 3.11, or 3.10.'}\n",
      "{'question': 'How do I install the necessary drivers for my NVIDIA GPU?', 'answer': \"You should download and install the latest version of your GPU's driver from the NVIDIA website.\"}\n",
      "{'question': 'What are some additional steps required for setting up Unsloth on Windows?', 'answer': 'On Windows, you need to install Visual Studio with C++ metadata in addition to installing the NVIDIA Video Driver.'}\n",
      "{'question': 'How do I update Unsloth and its dependencies using pip?', 'answer': \"You can update Unsloth by running 'pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo'.\"}\n",
      "{'generated': [{'question': 'What C++ options should be selected during the installation process in Visual Studio?', 'answer': 'Select all of the C++ options and also select options for Windows 10/11 SDK.'}, {'question': 'How can I ensure that I have installed the correct version of PyTorch compatible with my CUDA drivers?', 'answer': 'Make sure to select the correct version of PyTorch that is compatible with your CUDA drivers.'}, {'question': 'What are the prerequisites for running Unsloth directly on Windows?', 'answer': 'Install Triton from a specific Windows fork and ensure you have PyTorch >= 2.4 and CUDA 12 installed.'}, {'question': 'How can I troubleshoot a crashing issue in SFTTrainer when using Unsloth?', 'answer': 'Set dataset_num_proc=1 to avoid the crashing issue by adding this parameter to the trainer initialization: trainer = SFTTrainer(dataset_num_proc=1,...'}, {'question': 'What are the necessary steps to install CUDA Toolkit according to the instructions provided?', 'answer': 'Follow the instructions to install CUDA Toolkit.'}]}\n",
      "{'question': 'What C++ options should be selected during the installation process in Visual Studio?', 'answer': 'Select all of the C++ options and also select options for Windows 10/11 SDK.'}\n",
      "{'question': 'How can I ensure that I have installed the correct version of PyTorch compatible with my CUDA drivers?', 'answer': 'Make sure to select the correct version of PyTorch that is compatible with your CUDA drivers.'}\n",
      "{'question': 'What are the prerequisites for running Unsloth directly on Windows?', 'answer': 'Install Triton from a specific Windows fork and ensure you have PyTorch >= 2.4 and CUDA 12 installed.'}\n",
      "{'question': 'How can I troubleshoot a crashing issue in SFTTrainer when using Unsloth?', 'answer': 'Set dataset_num_proc=1 to avoid the crashing issue by adding this parameter to the trainer initialization: trainer = SFTTrainer(dataset_num_proc=1,...'}\n",
      "{'question': 'What are the necessary steps to install CUDA Toolkit according to the instructions provided?', 'answer': 'Follow the instructions to install CUDA Toolkit.'}\n",
      "{'generated': [{'question': 'What are some common installation errors that may occur during the setup of this project?', 'answer': 'Some common installation errors include weird errors during installations, which can be resolved by following advanced installation instructions or troubleshooting steps.'}, {'question': 'How do I confirm if CUDA is installed correctly?', 'answer': 'You can try using nvcc to confirm if CUDA is installed correctly. If it fails, you may need to install cudatoolkit or CUDA drivers.'}, {'question': 'What are the steps for installing xformers manually?', 'answer': 'To install xformers manually, you can try installing vllm and see if it succeeds. Then, check if xformers succeeded with python -m xformers.info. Alternatively, you can use flash-attn for Ampere GPUs.'}, {'question': 'How do I ensure compatibility between different versions of Python, CUDA, CUDNN, torch, triton, and xformers?', 'answer': 'You can refer to the PyTorch Compatibility Matrix to check the compatibility between different versions. Double-check that your versions are compatible with each other.'}, {'question': 'What is an alternative installation method using Conda?', 'answer': \"If you have Conda installed, you can use it as an alternative to Pip for installation. However, if not, it's recommended to use Pip instead.\"}]}\n",
      "{'question': 'What are some common installation errors that may occur during the setup of this project?', 'answer': 'Some common installation errors include weird errors during installations, which can be resolved by following advanced installation instructions or troubleshooting steps.'}\n",
      "{'question': 'How do I confirm if CUDA is installed correctly?', 'answer': 'You can try using nvcc to confirm if CUDA is installed correctly. If it fails, you may need to install cudatoolkit or CUDA drivers.'}\n",
      "{'question': 'What are the steps for installing xformers manually?', 'answer': 'To install xformers manually, you can try installing vllm and see if it succeeds. Then, check if xformers succeeded with python -m xformers.info. Alternatively, you can use flash-attn for Ampere GPUs.'}\n",
      "{'question': 'How do I ensure compatibility between different versions of Python, CUDA, CUDNN, torch, triton, and xformers?', 'answer': 'You can refer to the PyTorch Compatibility Matrix to check the compatibility between different versions. Double-check that your versions are compatible with each other.'}\n",
      "{'question': 'What is an alternative installation method using Conda?', 'answer': \"If you have Conda installed, you can use it as an alternative to Pip for installation. However, if not, it's recommended to use Pip instead.\"}\n",
      "{'generated': [{'question': 'What are the supported versions of PyTorch and CUDA for this project?', 'answer': 'This project supports PyTorch versions 3.10, 3.11, and 3.12, as well as CUDA 11.8 and 12.1.'}, {'question': 'How can you create a new environment with the required packages using Conda?', 'answer': \"You can create a new environment named 'unsloth_env' using the command `conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y`.\"}, {'question': 'What is the alternative method for installing packages if you already have Conda installed?', 'answer': 'You can use pip, but be aware of potential dependency issues. The pip command for this project would be different than the one used for PyTorch 4.'}, {'question': 'How do you activate a Conda environment in Linux after installation?', 'answer': 'After installing Conda in Linux, you can activate it using the command `conda init bash` or `conda init zsh`, depending on your shell.'}, {'question': 'What is the recommended approach for installing packages if you have existing Conda environments?', 'answer': \"It's best not to use pip if you already have Conda installed. Instead, refer to the instructions for advanced pip installation, taking note of potential dependency issues.\"}]}\n",
      "{'question': 'What are the supported versions of PyTorch and CUDA for this project?', 'answer': 'This project supports PyTorch versions 3.10, 3.11, and 3.12, as well as CUDA 11.8 and 12.1.'}\n",
      "{'question': 'How can you create a new environment with the required packages using Conda?', 'answer': \"You can create a new environment named 'unsloth_env' using the command `conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y`.\"}\n",
      "{'question': 'What is the alternative method for installing packages if you already have Conda installed?', 'answer': 'You can use pip, but be aware of potential dependency issues. The pip command for this project would be different than the one used for PyTorch 4.'}\n",
      "{'question': 'How do you activate a Conda environment in Linux after installation?', 'answer': 'After installing Conda in Linux, you can activate it using the command `conda init bash` or `conda init zsh`, depending on your shell.'}\n",
      "{'question': 'What is the recommended approach for installing packages if you have existing Conda environments?', 'answer': \"It's best not to use pip if you already have Conda installed. Instead, refer to the instructions for advanced pip installation, taking note of potential dependency issues.\"}\n",
      "{'generated': [{'question': 'What versions of PyTorch are supported by this dataset?', 'answer': 'The dataset supports torch211, torch212, torch220, torch230, and torch240.'}, {'question': 'How do I install the unsloth library for a specific PyTorch and CUDA version?', 'answer': \"You can use pip to install the library with a command like `pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'`.\"}, {'question': 'What CUDA versions are supported for PyTorch 2.4?', 'answer': 'The supported CUDA versions for torch 2.4 are cu118 and cu121 and cu124.'}, {'question': 'How do I determine the correct installation command for unsloth based on my device type?', 'answer': 'For Ampere devices (A100, H100, RTX3090) and above, use cu118-ampere or cu121-ampere or cu124-ampere.'}, {'question': 'Can you provide an example installation command for unsloth with torch 2.5 and CUDA 12.4?', 'answer': \"An example installation command is `pip install --upgrade pip` followed by `pip install 'unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git'`.\"}]}\n",
      "{'question': 'What versions of PyTorch are supported by this dataset?', 'answer': 'The dataset supports torch211, torch212, torch220, torch230, and torch240.'}\n",
      "{'question': 'How do I install the unsloth library for a specific PyTorch and CUDA version?', 'answer': \"You can use pip to install the library with a command like `pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'`.\"}\n",
      "{'question': 'What CUDA versions are supported for PyTorch 2.4?', 'answer': 'The supported CUDA versions for torch 2.4 are cu118 and cu121 and cu124.'}\n",
      "{'question': 'How do I determine the correct installation command for unsloth based on my device type?', 'answer': 'For Ampere devices (A100, H100, RTX3090) and above, use cu118-ampere or cu121-ampere or cu124-ampere.'}\n",
      "{'question': 'Can you provide an example installation command for unsloth with torch 2.5 and CUDA 12.4?', 'answer': \"An example installation command is `pip install --upgrade pip` followed by `pip install 'unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git'`.\"}\n",
      "{'generated': [{'question': 'What is the optimal pip installation command for unsloth?', 'answer': 'The optimal pip installation command can be obtained by running `wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -` or manually in a Python REPL.'}, {'question': 'What are the specific CUDA versions supported by unsloth?', 'answer': 'Unsloth supports CUDA 12.1, 11.8, and 12.4.'}, {'question': 'Why is it necessary to have a minimum Torch version of 2.1.0 for unsloth?', 'answer': 'A minimum Torch version of 2.1.0 is required to ensure compatibility with unsloth.'}, {'question': 'What is the purpose of installing torch via pip before running the _auto_install.py script?', 'answer': 'Installing torch via pip is necessary because the _auto_install.py script requires it to function correctly.'}, {'question': 'How can one determine if their system has an Ampere GPU, which is supported by unsloth?', 'answer': 'You can check if your system has an Ampere GPU by running `torch.cuda.get_device_capability()[0] >= 8` in a Python REPL.'}]}\n",
      "{'question': 'What is the optimal pip installation command for unsloth?', 'answer': 'The optimal pip installation command can be obtained by running `wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -` or manually in a Python REPL.'}\n",
      "{'question': 'What are the specific CUDA versions supported by unsloth?', 'answer': 'Unsloth supports CUDA 12.1, 11.8, and 12.4.'}\n",
      "{'question': 'Why is it necessary to have a minimum Torch version of 2.1.0 for unsloth?', 'answer': 'A minimum Torch version of 2.1.0 is required to ensure compatibility with unsloth.'}\n",
      "{'question': 'What is the purpose of installing torch via pip before running the _auto_install.py script?', 'answer': 'Installing torch via pip is necessary because the _auto_install.py script requires it to function correctly.'}\n",
      "{'question': 'How can one determine if their system has an Ampere GPU, which is supported by unsloth?', 'answer': 'You can check if your system has an Ampere GPU by running `torch.cuda.get_device_capability()[0] >= 8` in a Python REPL.'}\n",
      "{'generated': [{'question': 'What are the conditions for using different versions of CUDA with PyTorch?', 'answer': 'The code checks if the version is less than or equal to v2.1.1, v2.1.2, v2.3.0, v2.4.0, v2.5.0, or v2.6.0 to use corresponding CUDA and PyTorch versions.'}, {'question': 'How does the code handle torch versions greater than 2.6.0?', 'answer': 'It raises a RuntimeError with the current Torch version being too new.'}]}\n",
      "{'question': 'What are the conditions for using different versions of CUDA with PyTorch?', 'answer': 'The code checks if the version is less than or equal to v2.1.1, v2.1.2, v2.3.0, v2.4.0, v2.5.0, or v2.6.0 to use corresponding CUDA and PyTorch versions.'}\n",
      "{'question': 'How does the code handle torch versions greater than 2.6.0?', 'answer': 'It raises a RuntimeError with the current Torch version being too new.'}\n",
      "{'generated': [{'question': 'What is the purpose of the unsloth command with a specific version installed?', 'answer': 'The unsloth command with a specific version installed is used to install a particular version of the unsloth library.'}, {'question': 'How do you install the modelscope library using pip?', 'answer': \"You can install the modelscope library by running 'pip install modelscope -U'.\"}, {'question': 'What is the purpose of setting the UNSLOTH_USE_MODELSCOPE environment variable to 1?', 'answer': 'Setting the UNSLOTH_USE_MODELSCOPE environment variable to 1 allows you to download models and datasets from ModelScope.'}, {'question': 'What is the syntax for installing a specific version of the unsloth library using pip?', 'answer': 'The syntax for installing a specific version of the unsloth library is \\'pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"\\'.'}, {'question': 'How do you download models and datasets from ModelScope using the unsloth_cli.py command?', 'answer': \"You can download models and datasets from ModelScope by running 'python unsloth_cli.py' with the UNSLOTH_USE_MODELSCOPE environment variable set to 1.\"}]}\n",
      "{'question': 'What is the purpose of the unsloth command with a specific version installed?', 'answer': 'The unsloth command with a specific version installed is used to install a particular version of the unsloth library.'}\n",
      "{'question': 'How do you install the modelscope library using pip?', 'answer': \"You can install the modelscope library by running 'pip install modelscope -U'.\"}\n",
      "{'question': 'What is the purpose of setting the UNSLOTH_USE_MODELSCOPE environment variable to 1?', 'answer': 'Setting the UNSLOTH_USE_MODELSCOPE environment variable to 1 allows you to download models and datasets from ModelScope.'}\n",
      "{'question': 'What is the syntax for installing a specific version of the unsloth library using pip?', 'answer': 'The syntax for installing a specific version of the unsloth library is \\'pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"\\'.'}\n",
      "{'question': 'How do you download models and datasets from ModelScope using the unsloth_cli.py command?', 'answer': \"You can download models and datasets from ModelScope by running 'python unsloth_cli.py' with the UNSLOTH_USE_MODELSCOPE environment variable set to 1.\"}\n",
      "{'generated': [{'question': 'What is the purpose of loading the LAION dataset?', 'answer': 'The LAION dataset serves as training data for fine-tuning a language model.'}, {'question': 'How are models loaded from Hugging Face datasets in this code snippet?', 'answer': 'Models are loaded using the `load_dataset` function from the Hugging Face library, which supports loading datasets from various sources.'}]}\n",
      "{'question': 'What is the purpose of loading the LAION dataset?', 'answer': 'The LAION dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'How are models loaded from Hugging Face datasets in this code snippet?', 'answer': 'Models are loaded using the `load_dataset` function from the Hugging Face library, which supports loading datasets from various sources.'}\n",
      "{'generated': [{'question': 'What are the specific model variants mentioned in this documentation?', 'answer': 'The models discussed include unsloth/Llama-3.2-3B-bnb-4bit, unsloth/Llama-3.2-3B-Instruct-bnb-4bit, and unsloth/Llama-3.3-70B-Instruct-bnb-4bit.'}, {'question': 'Who is the producer of this PDF document, and what software was used to create it?', 'answer': 'The producer is pdfTeX-1.40.25, and the document was created using LaTeX via pandoc.'}, {'question': 'What are some key features of the new Llama 3.3 70B model mentioned in this documentation?', 'answer': 'This section does not mention specific details about the new Llama 3.3 70B model, so no answer can be provided.'}, {'question': 'What is the total number of pages in this document?', 'answer': 'The total number of pages in this document is 10.'}, {'question': 'Can you describe the environment and software used to generate this PDF, according to its metadata?', 'answer': 'According to the metadata, this PDF was generated using pdfTeX-1.40.25 and LaTeX via pandoc, with TeX Live 2023/Debian.'}]}\n",
      "{'question': 'What are the specific model variants mentioned in this documentation?', 'answer': 'The models discussed include unsloth/Llama-3.2-3B-bnb-4bit, unsloth/Llama-3.2-3B-Instruct-bnb-4bit, and unsloth/Llama-3.3-70B-Instruct-bnb-4bit.'}\n",
      "{'question': 'Who is the producer of this PDF document, and what software was used to create it?', 'answer': 'The producer is pdfTeX-1.40.25, and the document was created using LaTeX via pandoc.'}\n",
      "{'question': 'What are some key features of the new Llama 3.3 70B model mentioned in this documentation?', 'answer': 'This section does not mention specific details about the new Llama 3.3 70B model, so no answer can be provided.'}\n",
      "{'question': 'What is the total number of pages in this document?', 'answer': 'The total number of pages in this document is 10.'}\n",
      "{'question': 'Can you describe the environment and software used to generate this PDF, according to its metadata?', 'answer': 'According to the metadata, this PDF was generated using pdfTeX-1.40.25 and LaTeX via pandoc, with TeX Live 2023/Debian.'}\n",
      "{'generated': [{'question': 'What is the primary purpose of this code snippet?', 'answer': 'This code snippet serves as an example for loading and fine-tuning a language model using FastModel from Hugging Face.'}, {'question': 'How does the code load and patch the model, and what is the significance of 4-bit quantization?', 'answer': \"The code loads the 'unsloth/gemma-3-4B-it' model, then patches it using FastLanguageModel.get_peft_model. The 4-bit quantization reduces memory usage.\"}, {'question': 'What are the key differences between loading in 4-bit and 8-bit for this model?', 'answer': 'Loading in 8-bit uses more memory but is slightly more accurate than loading in 4-bit, which reduces memory usage.'}, {'question': 'Can you explain the purpose of using gradient checkpointing in this code snippet?', 'answer': \"Gradient checkpointing is used to reduce memory usage and improve performance for very long context models. 'unsloth' uses 30% less VRAM compared to other methods.\"}, {'question': 'What are the benefits and trade-offs of using different levels of gradient checkpointing in this code?', 'answer': \"Gradient checkpointing reduces VRAM usage but may increase computation time or memory usage depending on the specific application. 'unsloth' is optimized for very long context models.\"}]}\n",
      "{'question': 'What is the primary purpose of this code snippet?', 'answer': 'This code snippet serves as an example for loading and fine-tuning a language model using FastModel from Hugging Face.'}\n",
      "{'question': 'How does the code load and patch the model, and what is the significance of 4-bit quantization?', 'answer': \"The code loads the 'unsloth/gemma-3-4B-it' model, then patches it using FastLanguageModel.get_peft_model. The 4-bit quantization reduces memory usage.\"}\n",
      "{'question': 'What are the key differences between loading in 4-bit and 8-bit for this model?', 'answer': 'Loading in 8-bit uses more memory but is slightly more accurate than loading in 4-bit, which reduces memory usage.'}\n",
      "{'question': 'Can you explain the purpose of using gradient checkpointing in this code snippet?', 'answer': \"Gradient checkpointing is used to reduce memory usage and improve performance for very long context models. 'unsloth' uses 30% less VRAM compared to other methods.\"}\n",
      "{'question': 'What are the benefits and trade-offs of using different levels of gradient checkpointing in this code?', 'answer': \"Gradient checkpointing reduces VRAM usage but may increase computation time or memory usage depending on the specific application. 'unsloth' is optimized for very long context models.\"}\n",
      "{'generated': [{'question': 'What is the purpose of setting max_seq_length in this code snippet?', 'answer': 'The max_seq_length parameter determines the maximum length of a sequence that can be processed by the model.'}, {'question': 'How does the use_rslora flag affect the training process?', 'answer': 'When set to False, rank stabilized LoRA (RSLoRA) is not used in the training process.'}]}\n",
      "{'question': 'What is the purpose of setting max_seq_length in this code snippet?', 'answer': 'The max_seq_length parameter determines the maximum length of a sequence that can be processed by the model.'}\n",
      "{'question': 'How does the use_rslora flag affect the training process?', 'answer': 'When set to False, rank stabilized LoRA (RSLoRA) is not used in the training process.'}\n",
      "{'generated': [{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}, {'question': 'What are some advanced tips provided in the Unsloth documentation?', 'answer': 'The tips include saving to GGUF, merging to 16-bit for vLLM, continued training from a saved LoRA adapter, adding an evaluation loop, and customized chat templates.'}, {'question': 'Which reinforcement learning algorithms are supported by Unsloth?', 'answer': 'Unsloth supports ReinforcementLearning including DPO, GRPO, PPO, Reward Modelling, and Online DPO.'}, {'question': 'Where can I find more information about the implementation of these algorithms in notebooks?', 'answer': \"You can find a list of RL notebooks on Unsloth's wiki page, which includes links to advanced Qwen3 GRPO notebook, ORPO notebook, DPO Zephyr notebook, KTO notebook, and SimPO notebook.\"}, {'question': 'How do I import necessary libraries for using Unsloth?', 'answer': \"You can import the required libraries by adding the following lines of code: `import os`, `os.environ['CUDA_VISIBLE_DEVICES'] = '0'`, `from unsloth import FastLanguageModel`, and `import torch`.\"}, {'question': 'What is the process for loading a model in 4-bit precision?', 'answer': 'You can load a model in 4-bit precision using the `load_in_4bit=True` argument when calling `FastLanguageModel.from_pretrained()`. Additionally, you need to set `max_seq_length=2048` and specify the model name as `unsloth/zephyr-sft-bnb-4bit`.'}]}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'What are some advanced tips provided in the Unsloth documentation?', 'answer': 'The tips include saving to GGUF, merging to 16-bit for vLLM, continued training from a saved LoRA adapter, adding an evaluation loop, and customized chat templates.'}\n",
      "{'question': 'Which reinforcement learning algorithms are supported by Unsloth?', 'answer': 'Unsloth supports ReinforcementLearning including DPO, GRPO, PPO, Reward Modelling, and Online DPO.'}\n",
      "{'question': 'Where can I find more information about the implementation of these algorithms in notebooks?', 'answer': \"You can find a list of RL notebooks on Unsloth's wiki page, which includes links to advanced Qwen3 GRPO notebook, ORPO notebook, DPO Zephyr notebook, KTO notebook, and SimPO notebook.\"}\n",
      "{'question': 'How do I import necessary libraries for using Unsloth?', 'answer': \"You can import the required libraries by adding the following lines of code: `import os`, `os.environ['CUDA_VISIBLE_DEVICES'] = '0'`, `from unsloth import FastLanguageModel`, and `import torch`.\"}\n",
      "{'question': 'What is the process for loading a model in 4-bit precision?', 'answer': 'You can load a model in 4-bit precision using the `load_in_4bit=True` argument when calling `FastLanguageModel.from_pretrained()`. Additionally, you need to set `max_seq_length=2048` and specify the model name as `unsloth/zephyr-sft-bnb-4bit`.'}\n",
      "{'generated': [{'question': 'What is the purpose of adding fast LoRA weights to the model?', 'answer': \"This step enables efficient and flexible adaptation of the model's parameters.\"}, {'question': \"How does the use of 'unsloth' for gradient checkpointing affect the model's performance and memory usage?\", 'answer': \"'Unsloth' is a strategy that uses 30% less VRAM, allowing for larger batch sizes and efficient computation.\"}, {'question': 'What are the target modules being patched in this code snippet?', 'answer': \"The 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', and 'down_proj' modules are targeted for patching.\"}, {'question': \"What is the significance of setting 'lora_dropout' to 0 in this code?\", 'answer': \"Setting 'lora_dropout' to 0 optimizes the model's performance by minimizing dropout probability.\"}, {'question': \"Why are certain modules ('q_proj', 'k_proj', etc.) specified as targets for patching and LoRA adaptation?\", 'answer': \"These modules are critical for the model's attention mechanisms, making them ideal candidates for efficient LoRA adaptation.\"}]}\n",
      "{'question': 'What is the purpose of adding fast LoRA weights to the model?', 'answer': \"This step enables efficient and flexible adaptation of the model's parameters.\"}\n",
      "{'question': \"How does the use of 'unsloth' for gradient checkpointing affect the model's performance and memory usage?\", 'answer': \"'Unsloth' is a strategy that uses 30% less VRAM, allowing for larger batch sizes and efficient computation.\"}\n",
      "{'question': 'What are the target modules being patched in this code snippet?', 'answer': \"The 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', and 'down_proj' modules are targeted for patching.\"}\n",
      "{'question': \"What is the significance of setting 'lora_dropout' to 0 in this code?\", 'answer': \"Setting 'lora_dropout' to 0 optimizes the model's performance by minimizing dropout probability.\"}\n",
      "{'question': \"Why are certain modules ('q_proj', 'k_proj', etc.) specified as targets for patching and LoRA adaptation?\", 'answer': \"These modules are critical for the model's attention mechanisms, making them ideal candidates for efficient LoRA adaptation.\"}\n",
      "{'generated': [{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}, {'question': 'How does the DPOConfig class influence the training process?', 'answer': 'The DPOConfig class defines various parameters such as per_device_train_batch_size, gradient_accumulation_steps, and warmup_ratio that affect the training process.'}]}\n",
      "{'question': 'What is the primary purpose of this dataset?', 'answer': 'This dataset serves as training data for fine-tuning a language model.'}\n",
      "{'question': 'How does the DPOConfig class influence the training process?', 'answer': 'The DPOConfig class defines various parameters such as per_device_train_batch_size, gradient_accumulation_steps, and warmup_ratio that affect the training process.'}\n",
      "{'generated': [{'question': 'What type of model was fine-tuned using the dataset?', 'answer': 'The QLoRA model was used for fine-tuning.'}, {'question': 'How many linear layers were modified during fine-tuning?', 'answer': 'All linear layers (Q, K, V, O, gate, up and down) were modified.'}, {'question': 'What was the rank of QLoRA used for fine-tuning?', 'answer': 'The rank used was 32.'}]}\n",
      "{'question': 'What type of model was fine-tuned using the dataset?', 'answer': 'The QLoRA model was used for fine-tuning.'}\n",
      "{'question': 'How many linear layers were modified during fine-tuning?', 'answer': 'All linear layers (Q, K, V, O, gate, up and down) were modified.'}\n",
      "{'question': 'What was the rank of QLoRA used for fine-tuning?', 'answer': 'The rank used was 32.'}\n",
      "{'generated': [{'question': 'What were the results of testing Llama 3.3 (70B) with a maximum context length on an A100 GPU?', 'answer': 'We tested Llama 3.3 (70B) Instruct on an 80GB A100 and did 4bit QLoRA on all linear layers.'}, {'question': 'What was the effect of using a batch size of 1 in the experiment with Llama 3.3 (70B)?', 'answer': 'We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.'}]}\n",
      "{'question': 'What were the results of testing Llama 3.3 (70B) with a maximum context length on an A100 GPU?', 'answer': 'We tested Llama 3.3 (70B) Instruct on an 80GB A100 and did 4bit QLoRA on all linear layers.'}\n",
      "{'question': 'What was the effect of using a batch size of 1 in the experiment with Llama 3.3 (70B)?', 'answer': 'We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.'}\n",
      "{'generated': [{'question': 'Who has contributed to Unsloth?', 'answer': 'Every single person who has used or contributed to Unsloth'}, {'question': 'What is the significance of Unsloth in the context of user contributions and usage?', 'answer': 'Unsloth acknowledges and appreciates the value of every individual who has used or contributed to it.'}]}\n",
      "{'question': 'Who has contributed to Unsloth?', 'answer': 'Every single person who has used or contributed to Unsloth'}\n",
      "{'question': 'What is the significance of Unsloth in the context of user contributions and usage?', 'answer': 'Unsloth acknowledges and appreciates the value of every individual who has used or contributed to it.'}\n",
      "Done writing unsloth_data.json to system\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List \n",
    "from pydantic import BaseModel\n",
    "import ollama\n",
    "\n",
    "def prompt_template(data: str, num_records: int = 5):\n",
    "\n",
    "    return f\"\"\"You are an expert data curator assisting a machine learning engineer in creating a high-quality instruction tuning dataset. Your task is to transform \n",
    "    the provided data chunk into diverse question and answer (Q&A) pairs that will be used to fine-tune a language model. \n",
    "\n",
    "    For each of the {num_records} entries, generate one or two well-structured questions that reflect different aspects of the information in the chunk. \n",
    "    Ensure a mix of longer and shorter questions, with shorter ones typically containing 1-2 sentences and longer ones spanning up to 3-4 sentences. Each \n",
    "    Q&A pair should be concise yet informative, capturing key insights from the data.\n",
    "\n",
    "    Structure your output in JSON format, where each object contains 'question' and 'answer' fields. The JSON structure should look like this:\n",
    "\n",
    "        \"question\": \"Your question here...\",\n",
    "        \"answer\": \"Your answer here...\"\n",
    "\n",
    "    Focus on creating clear, relevant, and varied questions that encourage the model to learn from diverse perspectives. Avoid any sensitive or biased \n",
    "    content, ensuring answers are accurate and neutral.\n",
    "\n",
    "    Example:\n",
    "    \n",
    "        \"question\": \"What is the primary purpose of this dataset?\",\n",
    "        \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
    "    \n",
    "\n",
    "    By following these guidelines, you'll contribute to a robust and effective dataset that enhances the model's performance.\"\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Explanation:**\n",
    "\n",
    "    - **Clarity and Specificity:** The revised prompt clearly defines the role of the assistant and the importance of the task, ensuring alignment with the \n",
    "    project goals.\n",
    "    - **Quality Standards:** It emphasizes the need for well-formulated Q&A pairs, specifying the structure and content of each question and answer.\n",
    "    - **Output Format:** An example JSON structure is provided to guide the format accurately.\n",
    "    - **Constraints and Biases:** A note on avoiding sensitive or biased content ensures ethical considerations are met.\n",
    "    - **Step-by-Step Guidance:** The prompt breaks down the task into manageable steps, making it easier for the assistant to follow.\n",
    "\n",
    "    This approach ensures that the generated data is both high-quality and meets the specific requirements of the machine learning project.\n",
    "    \n",
    "    Data\n",
    "    {data}\n",
    "    \"\"\"\n",
    "\n",
    "class Record(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class Response(BaseModel):\n",
    "    generated: List[Record]\n",
    "\n",
    "def llm_call(data: str, num_records: int = 5) -> dict:\n",
    "    response = ollama.generate(\n",
    "        model=\"llama3.1\",\n",
    "        prompt=prompt_template(data, num_records),\n",
    "        options={\n",
    "            \"num_predict\": 2000\n",
    "        },\n",
    "        format=Response.model_json_schema(),\n",
    "    )\n",
    "    return json.loads(response['response'])\n",
    "\n",
    "dataset = []\n",
    "for i, chunk in enumerate(split_docs):\n",
    "    data = llm_call(chunk)\n",
    "    print(data)\n",
    "    for pair in data['generated']:\n",
    "        print(pair)\n",
    "        dataset.append({\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer']\n",
    "            })\n",
    "tuning_data = 'unsloth_data.json'\n",
    "with open(tuning_data,'w') as f: \n",
    "    json.dump(dataset, f) \n",
    "\n",
    "print(f\"Done writing {tuning_data} to system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb0d03-f066-4850-b250-ba4bb5983635",
   "metadata": {},
   "source": [
    "Now that we have a set of training data we are ready to train!\n",
    "\n",
    "Before doing however we need to install unsloth to help speedup the performance the reduce the RAM required for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f14f23ca-942c-4515-9616-f3e4d66f9e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Collecting unsloth\n",
      "  Using cached unsloth-2025.6.12-py3-none-any.whl.metadata (46 kB)\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Collecting unsloth_zoo>=2025.6.8 (from unsloth)\n",
      "  Using cached unsloth_zoo-2025.6.8-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from unsloth) (24.2)\n",
      "Collecting tyro (from unsloth)\n",
      "  Using cached tyro-0.9.26-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 (from unsloth)\n",
      "  Using cached transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets>=3.4.1 (from unsloth)\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth)\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tqdm (from unsloth)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.12/site-packages (from unsloth) (7.0.0)\n",
      "Collecting wheel>=0.42.0 (from unsloth)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from unsloth) (2.3.1)\n",
      "Collecting accelerate>=0.34.1 (from unsloth)\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
      "  Using cached trl-0.19.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\n",
      "  Using cached peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting huggingface_hub (from unsloth)\n",
      "  Using cached huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Using cached diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting regex (from vllm)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting cachetools (from vllm)\n",
      "  Using cached cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.12/site-packages (from vllm) (2.32.4)\n",
      "Collecting blake3 (from vllm)\n",
      "  Using cached blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting tokenizers>=0.21.1 (from vllm)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.12/site-packages (from vllm) (3.12.13)\n",
      "Collecting openai>=1.52.0 (from vllm)\n",
      "  Using cached openai-1.93.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in ./venv/lib/python3.12/site-packages (from vllm) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in ./venv/lib/python3.12/site-packages (from vllm) (0.22.1)\n",
      "Collecting pillow (from vllm)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
      "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
      "  Using cached llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines==0.1.11 (from vllm)\n",
      "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.19 (from vllm)\n",
      "  Using cached xgrammar-0.1.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in ./venv/lib/python3.12/site-packages (from vllm) (4.14.1)\n",
      "Collecting filelock>=3.16.1 (from vllm)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Using cached partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in ./venv/lib/python3.12/site-packages (from vllm) (27.0.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Using cached gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
      "  Using cached mistral_common-1.6.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.12/site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: six>=1.16.0 in ./venv/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Collecting setuptools<80,>=77.0.3 (from vllm)\n",
      "  Using cached setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.10.1 (from vllm)\n",
      "  Using cached compressed_tensors-0.10.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.18.0 (from vllm)\n",
      "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Using cached watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in ./venv/lib/python3.12/site-packages (from vllm) (3.3.0)\n",
      "Collecting scipy (from vllm)\n",
      "  Using cached scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting ninja (from vllm)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-api>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n",
      "  Using cached opentelemetry_semantic_conventions_ai-0.4.10-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Using cached numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached ray-2.47.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting torchaudio==2.7.0 (from vllm)\n",
      "  Using cached torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.18.0->vllm)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Using cached llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting numpy (from unsloth)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting interegular (from outlines==0.1.11->vllm)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: referencing in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in ./venv/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (4.24.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
      "  Downloading airportsdata-20250706-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
      "  Using cached outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sympy>=1.13.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fsspec (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=3.4.1->unsloth)\n",
      "  Using cached pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill (from depyf==0.18.0->vllm)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=3.4.1->unsloth)\n",
      "  Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets>=3.4.1->unsloth)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.4.1->unsloth)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub->unsloth)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.52.0->vllm)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm)\n",
      "  Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.26.0->vllm)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.63.2 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (0.4.1)\n",
      "Collecting click>=7.0 (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached msgpack-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2025.6.15)\n",
      "INFO: pip is looking at multiple versions of unsloth-zoo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ray[cgraph]!=2.44.*,>=2.43.0 (from vllm)\n",
      "  Using cached ray-2.47.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "  Using cached ray-2.46.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Using cached ray-2.45.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Using cached ray-2.43.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: aiosignal in ./venv/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist in ./venv/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.7.0)\n",
      "INFO: pip is looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of unsloth-zoo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.9.0.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Collecting compressed-tensors==0.9.4 (from vllm)\n",
      "  Using cached compressed_tensors-0.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.6.8->unsloth)\n",
      "  Using cached cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.9.0-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
      "Collecting xgrammar==0.1.18 (from vllm)\n",
      "  Using cached xgrammar-0.1.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools>=74.1.1 in ./venv/lib/python3.12/site-packages (from vllm) (80.9.0)\n",
      "Collecting compressed-tensors==0.9.3 (from vllm)\n",
      "  Using cached compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchaudio==2.6.0 (from vllm)\n",
      "  Using cached torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib_metadata (from vllm)\n",
      "  Using cached importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting zipp>=0.5 (from importlib_metadata->vllm)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp->vllm) (1.20.1)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached rich_toolkit-0.14.8-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.26.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=3.4.1->unsloth)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=3.4.1->unsloth)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Using cached unsloth-2025.6.12-py3-none-any.whl (294 kB)\n",
      "Using cached vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl (326.4 MB)\n",
      "Using cached compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n",
      "Using cached depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "Using cached outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "Using cached torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
      "Using cached torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "Using cached xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl (44.3 MB)\n",
      "Using cached xgrammar-0.1.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached ray-2.47.1-cp312-cp312-manylinux2014_x86_64.whl (68.9 MB)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached fastapi-0.115.14-py3-none-any.whl (95 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Using cached huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "Using cached llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "Using cached lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "Using cached mistral_common-1.6.3-py3-none-any.whl (6.5 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Using cached openai-1.93.0-py3-none-any.whl (755 kB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "Using cached opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "Using cached importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
      "Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "Using cached opentelemetry_semantic_conventions_ai-0.4.10-py3-none-any.whl (5.6 kB)\n",
      "Using cached peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "Using cached trl-0.19.0-py3-none-any.whl (375 kB)\n",
      "Using cached unsloth_zoo-2025.6.8-py3-none-any.whl (154 kB)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "Using cached blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "Using cached cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Using cached partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "Using cached tyro-0.9.26-py3-none-any.whl (128 kB)\n",
      "Using cached watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Using cached fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "Using cached msgpack-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading airportsdata-20250706-py3-none-any.whl (912 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl (105.3 MB)\n",
      "Using cached cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Using cached fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached rich_toolkit-0.14.8-py3-none-any.whl (24 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: triton, sentencepiece, pytz, py-cpuinfo, nvidia-cusparselt-cu12, mpmath, fastrlock, blake3, zipp, xxhash, wrapt, wheel, websockets, uvloop, tzdata, typeguard, tqdm, sympy, shtab, shellingham, safetensors, regex, python-multipart, pycountry, pyarrow, protobuf, pillow, partial-json-parser, opentelemetry-semantic-conventions-ai, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, msgspec, msgpack, mdurl, llvmlite, llguidance, lark, jiter, interegular, httptools, hf-xet, hf_transfer, grpcio, fsspec, filelock, einops, docstring-parser, dnspython, distro, diskcache, dill, cloudpickle, click, cachetools, astor, airportsdata, watchfiles, uvicorn, tiktoken, starlette, scipy, pandas, opentelemetry-proto, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, multiprocess, markdown-it-py, importlib_metadata, huggingface_hub, googleapis-common-protos, gguf, email-validator, depyf, deprecated, cupy-cuda12x, tokenizers, rich, prometheus-fastapi-instrumentator, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, openai, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, diffusers, tyro, typer, transformers, torch, rich-toolkit, ray, outlines_core, opentelemetry-semantic-conventions, mistral_common, datasets, xgrammar, xformers, torchvision, torchaudio, outlines, opentelemetry-sdk, fastapi-cli, cut_cross_entropy, compressed-tensors, bitsandbytes, accelerate, trl, peft, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, unsloth_zoo, opentelemetry-exporter-otlp, vllm, unsloth\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.1\n",
      "    Uninstalling numpy-2.3.1:\n",
      "      Successfully uninstalled numpy-2.3.1\n",
      "Successfully installed accelerate-1.8.1 airportsdata-20250706 astor-0.8.1 bitsandbytes-0.46.1 blake3-1.0.5 cachetools-6.1.0 click-8.2.1 cloudpickle-3.1.1 compressed-tensors-0.9.3 cupy-cuda12x-13.4.1 cut_cross_entropy-25.1.1 datasets-3.6.0 deprecated-1.2.18 depyf-0.18.0 diffusers-0.34.0 dill-0.3.8 diskcache-5.6.3 distro-1.9.0 dnspython-2.7.0 docstring-parser-0.16 einops-0.8.1 email-validator-2.2.0 fastapi-0.115.14 fastapi-cli-0.0.7 fastrlock-0.8.3 filelock-3.18.0 fsspec-2025.3.0 gguf-0.17.1 googleapis-common-protos-1.70.0 grpcio-1.73.1 hf-xet-1.1.5 hf_transfer-0.1.9 httptools-0.6.4 huggingface_hub-0.33.2 importlib_metadata-8.0.0 interegular-0.3.3 jiter-0.10.0 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.10.11 markdown-it-py-3.0.0 mdurl-0.1.2 mistral_common-1.6.3 mpmath-1.3.0 msgpack-1.1.1 msgspec-0.19.0 multiprocess-0.70.16 networkx-3.5 ninja-1.11.1.4 numba-0.61.2 numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.93.0 opencv-python-headless-4.11.0.86 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.10 outlines-0.1.11 outlines_core-0.1.26 pandas-2.3.0 partial-json-parser-0.2.1.1.post6 peft-0.16.0 pillow-11.3.0 prometheus-fastapi-instrumentator-7.1.0 protobuf-3.20.3 py-cpuinfo-9.0.0 pyarrow-20.0.0 pycountry-24.6.1 python-multipart-0.0.20 pytz-2025.2 ray-2.47.1 regex-2024.11.6 rich-14.0.0 rich-toolkit-0.14.8 safetensors-0.5.3 scipy-1.16.0 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.2 starlette-0.46.2 sympy-1.13.1 tiktoken-0.9.0 tokenizers-0.21.2 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 tqdm-4.67.1 transformers-4.53.1 triton-3.2.0 trl-0.19.0 typeguard-4.4.4 typer-0.16.0 tyro-0.9.26 tzdata-2025.2 unsloth-2025.6.12 unsloth_zoo-2025.6.8 uvicorn-0.35.0 uvloop-0.21.0 vllm-0.8.5.post1 watchfiles-1.1.0 websockets-15.0.1 wheel-0.45.1 wrapt-1.17.2 xformers-0.0.29.post2 xgrammar-0.1.18 xxhash-3.5.0 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a035e5-63c0-44ba-b21e-aaa017deaaed",
   "metadata": {},
   "source": [
    "You will also need PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6d2ee12-6410-4761-bf1e-94da31e388c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./venv/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./venv/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded20d68-dc32-4c5e-8ab7-53192c8e5e24",
   "metadata": {},
   "source": [
    "Validate the the device is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dd6bed3-1334-427e-94c6-da123725702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 5070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", use_cuda)\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d1d7e-0406-4510-a20d-e2e3718c3bd4",
   "metadata": {},
   "source": [
    "How let's setup our tokenizer using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d21330a-792e-422e-aea7-4de988f4110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.12: Fast Llama patching. Transformers: 4.53.1. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070. Num GPUs = 1. Max memory: 11.94 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 12.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      3\u001b[39m max_seq_length = \u001b[32m2048\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munsloth/Meta-Llama-3.1-8B-bnb-4bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m prompt_style = \u001b[33m\"\"\"\u001b[39m\u001b[33mBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\u001b[39m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33m### Instruction:\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33m### Response:\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     22\u001b[39m EOS_TOKEN = tokenizer.eos_token \u001b[38;5;66;03m# Must add EOS_TOKEN\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/unsloth/models/loader.py:393\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    417\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/unsloth/models/llama.py:1912\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, **kwargs)\u001b[39m\n\u001b[32m   1899\u001b[39m     model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m   1900\u001b[39m         model_name,\n\u001b[32m   1901\u001b[39m         device_map              = device_map,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1909\u001b[39m         **kwargs,\n\u001b[32m   1910\u001b[39m     )\n\u001b[32m   1911\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[32m-> \u001b[39m\u001b[32m1912\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meager\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1923\u001b[39m     model.fast_generate = model.generate\n\u001b[32m   1924\u001b[39m     model.fast_generate_batches = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4766\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4758\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._autoset_attn_implementation(\n\u001b[32m   4759\u001b[39m         config,\n\u001b[32m   4760\u001b[39m         torch_dtype=torch_dtype,\n\u001b[32m   4761\u001b[39m         device_map=device_map,\n\u001b[32m   4762\u001b[39m     )\n\u001b[32m   4764\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4765\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4766\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4768\u001b[39m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[32m   4769\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:483\u001b[39m, in \u001b[36mLlamaForCausalLM.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m    482\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    485\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:358\u001b[39m, in \u001b[36mLlamaModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28mself\u001b[39m.layers = nn.ModuleList(\n\u001b[32m    355\u001b[39m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.num_hidden_layers)]\n\u001b[32m    356\u001b[39m )\n\u001b[32m    357\u001b[39m \u001b[38;5;28mself\u001b[39m.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[38;5;28mself\u001b[39m.rotary_emb = \u001b[43mLlamaRotaryEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/unsloth/models/llama.py:1337\u001b[39m, in \u001b[36mLlamaRotaryEmbedding.__init__\u001b[39m\u001b[34m(self, dim, max_position_embeddings, base, device, config)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28mself\u001b[39m.current_rope_size = \u001b[38;5;28mmin\u001b[39m(\u001b[32m4\u001b[39m * \u001b[32m8192\u001b[39m, \u001b[38;5;28mself\u001b[39m.max_position_embeddings)\n\u001b[32m   1336\u001b[39m \u001b[38;5;66;03m# Build here to make `torch.jit.trace` work.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1337\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_cos_sin_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_rope_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_default_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bmcginn/fine-tuning/venv/lib/python3.12/site-packages/unsloth/models/llama.py:1352\u001b[39m, in \u001b[36mLlamaRotaryEmbedding._set_cos_sin_cache\u001b[39m\u001b[34m(self, seq_len, device, dtype)\u001b[39m\n\u001b[32m   1350\u001b[39m \u001b[38;5;66;03m# Different from paper, but it uses a different permutation in order to obtain the same calculation\u001b[39;00m\n\u001b[32m   1351\u001b[39m emb = torch.cat((freqs, freqs), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1352\u001b[39m \u001b[38;5;28mself\u001b[39m.register_buffer(\u001b[33m\"\u001b[39m\u001b[33mcos_cached\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43memb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, persistent=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1353\u001b[39m \u001b[38;5;28mself\u001b[39m.register_buffer(\u001b[33m\"\u001b[39m\u001b[33msin_cached\u001b[39m\u001b[33m\"\u001b[39m, emb.sin().to(dtype=dtype, device=device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m), persistent=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None\n",
    ")\n",
    "\n",
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "\"You are a helpful, honest and harmless assitant designed to help engineers. Think through each question logically and provide an answer. Don't make things up, if you're unable to answer a question advise the user that you're unable to answer as it is outside of your scope.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"question\"]\n",
    "    outputs      = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = prompt_style.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"data\", split='train')\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset[\"text\"][0])\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0, \n",
    "    bias=\"none\", \n",
    "   \n",
    "    use_gradient_checkpointing=\"unsloth\", \n",
    "    random_state=3407,\n",
    "    use_rslora=False, \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81e60b-f0df-4d5d-8cfa-f00f62413574",
   "metadata": {},
   "source": [
    "Now that we have the tokenizer we can setup the prompt for fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a582658-e27b-4d5e-bdd6-de21c7521f3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bfloat16_supported\n\u001b[32m      5\u001b[39m trainer = SFTTrainer(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     model = \u001b[43mmodel\u001b[49m,\n\u001b[32m      7\u001b[39m     tokenizer = tokenizer,\n\u001b[32m      8\u001b[39m     train_dataset = dataset,\n\u001b[32m      9\u001b[39m     max_seq_length = max_seq_length,\n\u001b[32m     10\u001b[39m     dataset_num_proc = \u001b[32m2\u001b[39m,\n\u001b[32m     11\u001b[39m     num_train_epochs = \u001b[32m10\u001b[39m,\n\u001b[32m     12\u001b[39m     args = TrainingArguments(\n\u001b[32m     13\u001b[39m         per_device_train_batch_size = \u001b[32m4\u001b[39m,\n\u001b[32m     14\u001b[39m         gradient_accumulation_steps = \u001b[32m4\u001b[39m,\n\u001b[32m     15\u001b[39m         warmup_steps = \u001b[32m5\u001b[39m,\n\u001b[32m     16\u001b[39m         max_steps = \u001b[32m100\u001b[39m,\n\u001b[32m     17\u001b[39m         learning_rate = \u001b[32m2e-4\u001b[39m,\n\u001b[32m     18\u001b[39m         fp16 = \u001b[38;5;129;01mnot\u001b[39;00m is_bfloat16_supported(),\n\u001b[32m     19\u001b[39m         bf16 = is_bfloat16_supported(),\n\u001b[32m     20\u001b[39m         logging_steps = \u001b[32m1\u001b[39m,\n\u001b[32m     21\u001b[39m         optim = \u001b[33m\"\u001b[39m\u001b[33madamw_8bit\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m         weight_decay = \u001b[32m0.01\u001b[39m,\n\u001b[32m     23\u001b[39m         lr_scheduler_type = \u001b[33m\"\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m         seed = \u001b[32m3407\u001b[39m,\n\u001b[32m     25\u001b[39m         output_dir = \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     ),\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    num_train_epochs = 10,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 100,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a9d66-1c90-4f75-9ac9-e13a21ceef9a",
   "metadata": {},
   "source": [
    "The next step is to map our dataset to the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e82d836-9a6e-4d79-b242-624ebafe160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 117 | Num Epochs = 25 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 335,544,320 of 8,000,000,000 (4.19% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 09:05, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.472800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549.5646 seconds used for training.\n",
      "9.16 minutes used for training.\n",
      "Peak reserved memory = 10.236 GB.\n",
      "Peak reserved memory for training = 3.576 GB.\n",
      "Peak reserved memory % of max memory = 85.729 %.\n",
      "Peak reserved memory for training % of max memory = 29.95 %.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da8dd3-9e96-4a33-9d24-6214408d82bc",
   "metadata": {},
   "source": [
    "Save the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7eeae5b-10f4-4174-9d5e-3f77bd0d6ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama-3.1-8B-unsloth/tokenizer_config.json',\n",
       " 'Llama-3.1-8B-unsloth/special_tokens_map.json',\n",
       " 'Llama-3.1-8B-unsloth/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_local = \"Llama-3.1-8B-unsloth\"\n",
    "model.save_pretrained(new_model_local) # Local saving\n",
    "tokenizer.save_pretrained(new_model_local) # Local saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b6601-021c-477e-a5c5-1442b9905746",
   "metadata": {},
   "source": [
    "host in local ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa08fff2-85f6-4ec5-9ae1-0184ceb6b9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 0% ⠋ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 5% ⠙ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 16% ⠹ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 27% ⠸ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 32% ⠼ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 42% ⠴ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 53% ⠦ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 58% ⠧ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 69% ⠇ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 79% ⠏ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 84% ⠋ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 94% ⠙ \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:72cc873cb9a73ac730a3fd3680d4c2380d9ad7cb402bbafc8c05e0d441276675 100% \u001b[K\n",
      "copying file sha256:36f0351f1afd665ae72623dc7e0d7f51272123f992e93ddda1cb8b625c062778 100% \u001b[K\n",
      "copying file sha256:52716f60c3ad328509fa37cdded9a2f1196ecae463f5480f5d38c66a25e7a7dc 100% \u001b[K\n",
      "copying file sha256:32f404d626cf7b1b6eea36c241ae7cbd6ec29c777a8701ea48c0fe9ebb94c9b1 100% \u001b[K\n",
      "copying file sha256:889397283200148c6aabc316131bb48d62f4b547e4e53da6e13b31851f6a01a1 100% \u001b[K\n",
      "converting adapter \u001b[K\n",
      "using existing layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 \u001b[K\n",
      "using existing layer sha256:948af2743fc78a328dcb3b0f5a31b3d75f415840fdb699e8b1235978392ecf85 \u001b[K\n",
      "using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177 \u001b[K\n",
      "creating new layer sha256:d7708cc5dde5e99feed15efe6f923f5737f34d9bff5d63c579e27e1e641437a3 \u001b[K\n",
      "using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama create unsloth-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf389d-2fb2-4603-b9b1-619e50a789e8",
   "metadata": {},
   "source": [
    "Call latest model from ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95f65071-df25-4968-8eb1-786c950363fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not aware of any recent news or updates from Unsloth.ai. As a conversational AI, I don't have direct access to the web and may not be up-to-date on the latest developments.\n",
      "\n",
      "However, I can suggest some possible ways to find recent news or updates from Unsloth.ai:\n",
      "\n",
      "1. Check their website: You can visit Unsloth.ai's website and check for any recent blog posts, announcements, or updates.\n",
      "2. Social media: Unsloth.ai may have a social media presence on platforms like Twitter, LinkedIn, or Facebook. You can try searching for them on these platforms to see if they've posted any recent updates.\n",
      "3. News articles: You can try searching online for news articles related to Unsloth.ai or their products.\n",
      "\n",
      "If you're looking for specific information, I'd be happy to help with that!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"unsloth-trained\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me some Unsloth.ai News?\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4e799-5b50-42b2-9cea-8f3d20e20134",
   "metadata": {},
   "source": [
    "Results inconsistent let's try rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c19c4f35-e9cf-4c7d-8a22-8f99364e2697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documentation, here are some key points and news related to Unsloth:\n",
      "\n",
      "**News and Updates**\n",
      "\n",
      "* Unsloth was tested using the Alpaca Dataset with a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down). The results showed that Unsloth achieves a significant VRAM reduction of >75% compared to Hugging Face + FA2.\n",
      "* Benchmarking of Unsloth was also conducted by Hugging Face. They tested Llama 3.1 (8B) and Llama 3.3 (70B) with QLoRA on all linear layers and reported that Unsloth achieves a longer context length than Hugging Face + FA2.\n",
      "\n",
      "**Recent Developments**\n",
      "\n",
      "* Unsloth has added support for 4-bit quantization, which reduces memory usage while maintaining accuracy.\n",
      "* The team has also implemented full finetuning capabilities, allowing users to fine-tune models from scratch.\n",
      "* A new LoRA module called \"unsloth\" has been introduced, which uses 30% less VRAM and fits 2x larger batch sizes.\n",
      "\n",
      "**Recent Contributions**\n",
      "\n",
      "* The llama.cpp library allows users to save models with Unsloth.\n",
      "* The Hugging Face team and their TRL library have contributed significantly to Unsloth's development.\n",
      "* Erik helped add Apple's ML Cross Entropy in Unsloth, while Etherl added support for TTS, diffusion, and BERT models.\n",
      "\n",
      "**Reinforcement Learning (RL) Support**\n",
      "\n",
      "* Unsloth supports various RL algorithms, including DPO, GRPO, PPO, Reward Modeling, and Online DPO.\n",
      "* The team has also published several notebooks on GitHub, showcasing the usage of these RL algorithms with Unsloth.\n",
      "\n",
      "**Performance Benchmarking**\n",
      "\n",
      "* Unsloth has been tested using the Alpaca Dataset, and the results showed a significant VRAM reduction compared to Hugging Face + FA2.\n",
      "* Context length benchmarks have been conducted using Llama 3.1 (8B) and Llama 3.3 (70B), demonstrating that Unsloth achieves longer context lengths than Hugging Face + FA2.\n",
      "\n",
      "**Citation**\n",
      "\n",
      "If you're planning to use or reference Unsloth in your research, please cite the repository as follows:\n",
      "\n",
      "@software{unsloth,\n",
      "author = {Daniel Han, Michael Han and Unsloth team},\n",
      "title = {Unsloth},\n",
      "url = {http://github.com/unslothai/unsloth},\n",
      "year = {2023}\n",
      "}\n",
      "\n",
      "Overall, Unsloth continues to evolve with new features and improvements, making it an exciting development in the NLP community.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "\n",
    "# Load and extract text from the PDF\n",
    "def load_pdf_content(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_content = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_content += page.extract_text() + \"\\n\"\n",
    "    return text_content\n",
    "\n",
    "# Load the unsloth documentation\n",
    "pdf_content = load_pdf_content(\"unsloth_documentation.pdf\")\n",
    "\n",
    "# Create the RAG-enhanced prompt\n",
    "rag_prompt = f\"\"\"Based on the following documentation about Unsloth.ai, please answer the user's question:\n",
    "\n",
    "Documentation:\n",
    "{pdf_content}\n",
    "\n",
    "User Question: Give me some Unsloth.ai News?\n",
    "\n",
    "Please provide a comprehensive answer based on the documentation provid d.\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
